//===- AIELegalizerInfo.cpp -----------------------------------------------===//
//
// This file is licensed under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
// (c) Copyright 2023-2024 Advanced Micro Devices, Inc. or its affiliates
//
//===----------------------------------------------------------------------===//
/// \file
/// This file implements the targeting of the Machinelegalizer class for AIE.
/// \todo This should be generated by TableGen.
//===----------------------------------------------------------------------===//

#include "AIELegalizerInfo.h"
#include "AIE2TargetMachine.h"
#include "AIEBaseISelLowering.h"
#include "AIEBaseRegisterInfo.h"
#include "AIEMachineFunctionInfo.h"
#include "AIETargetMachine.h"
#include "MCTargetDesc/AIEMCTargetDesc.h"
#include "llvm/Analysis/VectorUtils.h"
#include "llvm/CodeGen/GlobalISel/CallLowering.h"
#include "llvm/CodeGen/GlobalISel/GenericMachineInstrs.h"
#include "llvm/CodeGen/GlobalISel/LegalizerHelper.h"
#include "llvm/CodeGen/GlobalISel/LegalizerInfo.h"
#include "llvm/CodeGen/GlobalISel/LostDebugLocObserver.h"
#include "llvm/CodeGen/GlobalISel/MachineIRBuilder.h"
#include "llvm/CodeGen/LowLevelType.h"
#include "llvm/CodeGen/MachineInstrBuilder.h"
#include "llvm/CodeGen/MachineMemOperand.h"
#include "llvm/CodeGen/MachineOperand.h"
#include "llvm/CodeGen/MachineRegisterInfo.h"
#include "llvm/CodeGen/Register.h"
#include "llvm/CodeGen/RuntimeLibcalls.h"
#include "llvm/CodeGen/SelectionDAGNodes.h"
#include "llvm/CodeGen/TargetOpcodes.h"
#include "llvm/IR/IntrinsicsAIE.h"
#include "llvm/IR/IntrinsicsAIE2.h"
#include "llvm/Support/Alignment.h"
#include <initializer_list>

using namespace llvm;
using namespace LegalityPredicates;

static LegalityPredicate isLegalBitCastType(unsigned TypeIdx) {
  return [=](const LegalityQuery &Query) {
    LLT Ty = Query.Types[TypeIdx];
    if (Ty.isScalar())
      return Ty == LLT::scalar(32) || Ty == LLT::scalar(64);
    const int EltSize = Ty.isVector() ? Ty.getElementType().getSizeInBits() : 0;
    return EltSize == 8 || EltSize == 16 || EltSize == 32 || EltSize == 64;
  };
}

static LegalizeMutation bitcastAccToVectorType(unsigned TypeIdx) {
  return [=](const LegalityQuery &Query) {
    LLT OrigTy = Query.Types[TypeIdx];
    assert(OrigTy.getElementType() == LLT::scalar(64) &&
           "Expected an accumulator type");
    unsigned Size = OrigTy.getSizeInBits();
    assert(Size % 32 == 0);
    return std::pair(TypeIdx, LLT::fixed_vector(Size / 32, 32));
  };
}

static LegalizeMutation bitcastToVectorElement32(const unsigned TypeIdx) {
  return [=](const LegalityQuery &Query) {
    const LLT Ty = Query.Types[TypeIdx];
    unsigned Size = Ty.getSizeInBits();
    assert(Size % 32 == 0);
    return std::pair(
        TypeIdx, LLT::scalarOrVector(ElementCount::getFixed(Size / 32), 32));
  };
}

static LegalityPredicate
isValidVectorMergeUnmergeOp(const unsigned BigVectorId,
                            const unsigned SmallVectorId) {
  return [=](const LegalityQuery &Query) {
    const LLT Big = Query.Types[BigVectorId];
    const LLT Small = Query.Types[SmallVectorId];
    return Big.isVector() && Small.isVector() &&
           Big.getElementType() == Small.getElementType() &&
           Small.getNumElements() * 2 == Big.getNumElements();
  };
}

static LegalityPredicate isValidVectorAIE2(const unsigned TypeIdx) {
  return [=](const LegalityQuery &Query) {
    const LLT DstTy = Query.Types[TypeIdx];
    const unsigned DstSize = DstTy.getSizeInBits();
    return DstTy.isVector() && (DstSize == 32 || DstSize > 64);
  };
}

LegalityPredicate
negatePredicate(const std::function<bool(const LegalityQuery &)> &Func) {
  return [=](const LegalityQuery &Query) { return !Func(Query); };
}

AIELegalizerInfo::AIELegalizerInfo(const AIEBaseSubtarget &ST) {
  using namespace TargetOpcode;
  const LLT S8 = LLT::scalar(8);
  const LLT S16 = LLT::scalar(16);
  const LLT S20 = LLT::scalar(20);
  const LLT S32 = LLT::scalar(32);
  const LLT S64 = LLT::scalar(64);
  const LLT P0 = LLT::pointer(0, 20);

  // 32-bit vectors
  const LLT V4S8 = LLT::fixed_vector(4, 8);
  const LLT V2S16 = LLT::fixed_vector(2, 16);

  // 64-bit vectors
  const LLT V2S32 = LLT::fixed_vector(2, 32);

  // 128-bit vectors
  const LLT V16S8 = LLT::fixed_vector(16, 8);
  const LLT V8S16 = LLT::fixed_vector(8, 16);
  const LLT V4S32 = LLT::fixed_vector(4, 32);

  // 256-bit vectors
  const LLT V8S32 = LLT::fixed_vector(8, 32);
  const LLT V16S16 = LLT::fixed_vector(16, 16);
  const LLT V32S8 = LLT::fixed_vector(32, 8);

  // 256-bit accumulators
  const LLT ACC256 = LLT::fixed_vector(4, 64);

  // 512-bit vectors
  const LLT V16S32 = LLT::fixed_vector(16, 32);
  const LLT V32S16 = LLT::fixed_vector(32, 16);
  const LLT V64S8 = LLT::fixed_vector(64, 8);

  // 512-bit accumulators
  const LLT ACC512 = LLT::fixed_vector(8, 64);

  // 1024-bit vectors
  const LLT V32S32 = LLT::fixed_vector(32, 32);
  const LLT V64S16 = LLT::fixed_vector(64, 16);
  const LLT V128S8 = LLT::fixed_vector(128, 8);

  // 1024-bit accumulators
  const LLT ACC1024 = LLT::fixed_vector(16, 64);

  const LLT S128 = LLT::scalar(128);

  static const std::initializer_list<LLT> AIE2VectorTypes = {
      /* Begin 32-bit types*/
      V4S8, V2S16,
      /* Begin 256-bit types */
      V8S32, V16S16, V32S8,
      /* Begin 512-bit types */
      V16S32, V32S16, V64S8,
      /* Begin 1024-bit types */
      V32S32, V64S16, V128S8};

  // Accumulator types are 32-bit vectors that pretend to 64-bit vectors of
  // half the size.
  static const std::initializer_list<LLT> AIE2AccumulatorTypes = {
      /* Begin 256-bit types */
      ACC256,
      /* Begin 512-bit types */
      ACC512,
      /* Begin 1024-bit types */
      ACC1024};

  auto &IMPLICIT = getActionDefinitionsBuilder({G_IMPLICIT_DEF, G_FREEZE})
                       .legalFor({S20, S32, P0, S128});
  if (ST.isAIE2()) {
    IMPLICIT.legalFor(AIE2VectorTypes);
    IMPLICIT.legalFor(AIE2AccumulatorTypes);
  }

  IMPLICIT.widenScalarToNextPow2(0).clampScalar(0, S32, S32);

  getActionDefinitionsBuilder(G_CONSTANT)
      .legalFor({S20, S32, P0})
      .widenScalarToNextPow2(0)
      .clampScalar(0, S32, S32);

  // FIXME: AIE1 actually supports float. But since AIE2 is using the same
  // legalizer, we will cast both type to int now.
  getActionDefinitionsBuilder(G_FCONSTANT).customFor({S16, S32, S64});

  getActionDefinitionsBuilder(G_ICMP)
      .legalFor({{S32, S32}, {S32, P0}})
      .clampScalar(0, S32, S32)
      .clampScalar(1, S32, S32);

  if (ST.isAIE2()) {
    getActionDefinitionsBuilder(G_FCMP)
        .customFor({S32, S32})
        .clampScalar(0, S32, S32)
        .clampScalar(1, S32, S32);

    getActionDefinitionsBuilder(G_FPTRUNC)
        .libcallFor({{S32, S64}})
        .customFor({{S16, S32}});

    getActionDefinitionsBuilder({G_SITOFP, G_UITOFP})
        .libcallForCartesianProduct({S32, S64})
        .clampScalar(1, S32, S64)
        .widenScalarToNextPow2(1)
        .clampScalar(0, S32, S64);

    getActionDefinitionsBuilder(G_FPEXT)
        .libcallFor({{S64, S32}})
        .customFor({{S32, S16}});

    getActionDefinitionsBuilder({G_FPTOSI, G_FPTOUI})
        .libcallForCartesianProduct({S32, S64})
        .clampScalar(0, S32, S64)
        .widenScalarToNextPow2(0)
        .clampScalar(1, S32, S64);

    getActionDefinitionsBuilder(G_FABS).customFor({S32, S64});

    getActionDefinitionsBuilder({G_FADD, G_FSUB})
        .legalFor({V16S32})
        .customFor({S16})
        .libcallFor({S32, S64});

    getActionDefinitionsBuilder({G_FMUL, G_FDIV, G_FREM})
        .clampScalar(0, S32, S64)
        .libcallFor({S32, S64});
  }

  if (!ST.isAIE2())
    getActionDefinitionsBuilder({G_FMUL, G_FDIV, G_FADD, G_FSUB, G_FREM})
        .libcallFor({S32, S64});

  // Since the only integers smaller than 32 bits we produce are S20 (from
  // G_PTRTOINT), the only legal extension is S20 -> S32.
  // Extensions to types larger than 64 bits have to be broken down into
  // multiple parts.
  getActionDefinitionsBuilder({G_ANYEXT, G_SEXT, G_ZEXT})
      .legalFor({{S32, S20}})
      .clampScalar(0, S32, S32);
  // FIXME: (s|z|any)ext s20 to s64 is broken.

  auto &VANDOR = getActionDefinitionsBuilder({G_AND, G_OR})
                     .legalFor({S32})
                     .widenScalarToNextPow2(0)
                     .clampScalar(0, S32, S32);

  if (ST.isAIE2())
    VANDOR.legalFor(AIE2VectorTypes);

  getActionDefinitionsBuilder(G_XOR)
      .legalFor({S32})
      .widenScalarToNextPow2(0)
      .clampScalar(0, S32, S32);

  getActionDefinitionsBuilder(G_SEXT_INREG)
      .custom()
      .legalForTypeWithAnyImm({S32});

  getActionDefinitionsBuilder({G_ASHR, G_LSHR, G_SHL})
      .legalFor({{S32, S32}})
      .widenScalarToNextPow2(0)
      .clampScalar(0, S32, S32)
      .clampScalar(1, S32, S32);

  getActionDefinitionsBuilder(G_TRUNC).alwaysLegal();

  auto &SELECT = getActionDefinitionsBuilder(G_SELECT)
                     .legalFor({{S32, S32}, {P0, S32}})
                     .widenScalarToNextPow2(0)
                     .clampScalar(0, S32, S32)
                     .clampScalar(1, S32, S32);

  if (ST.isAIE2()) {
    SELECT.legalFor(AIE2VectorTypes);

    // We support G_SELECT only on the vector register bank
    // Mapping the G_SELECT operands to the vector register bank
    // during register bank selection introduces the proper cross-bank
    // copies. However, we cannot write ISEL patterns expressing accumulator
    // types on vector register banks, which requires to duplicate the vector
    // type patterns in C++. Introducing bitcasts during legalization allows to
    // re-use the existing code for register bank selection and ISEL patterns.
    SELECT.bitcastIf(typeInSet(0, AIE2AccumulatorTypes),
                     bitcastAccToVectorType(0));
  }
  auto &ADDSUB = getActionDefinitionsBuilder({G_ADD, G_SUB})
                     .legalFor({S32})
                     .widenScalarToNextPow2(0)
                     .clampScalar(0, S32, S32);

  if (ST.isAIE2())
    ADDSUB.legalFor({V16S32, V32S16, V64S8});

  // FIXME: G_SADDE/G_SSUBE doesn't support lowering. To support this properly,
  // the action needs to be implemented
  // FIXME: AIE2 has ADC and SBC operations to read the carry.
  getActionDefinitionsBuilder({G_UADDO, G_USUBO, G_UADDE, G_USUBE, G_SADDO,
                               G_SSUBO, G_SADDE, G_SSUBE, G_UADDSAT, G_USUBSAT,
                               G_SADDSAT, G_SSUBSAT})
      .lower();

  getActionDefinitionsBuilder(G_MUL)
      .legalFor({S32})
      .widenScalarToNextPow2(0)
      .minScalar(0, S32)
      .libcallFor({S64});

  // FIXME: G_SMULO, G_UMULO support
  getActionDefinitionsBuilder({G_UMULH, G_SMULH}).lower();

  getActionDefinitionsBuilder({G_SDIV, G_UDIV, G_SREM, G_UREM})
      .libcallFor({S32, S64})
      .widenScalarToNextPow2(0)
      .clampScalar(0, S32, S64);

  getActionDefinitionsBuilder({G_SDIVREM, G_UDIVREM})
      .lowerFor({S32, S64})
      .widenScalarToNextPow2(0)
      .clampScalar(0, S32, S64);

  getActionDefinitionsBuilder(G_ABS)
      .legalFor({S32})
      .lowerFor({S64})
      .widenScalarToNextPow2(0)
      .clampScalar(0, S32, S32);

  // The CLZ instruction implements CTLZ, which also covers CTLZ_ZERO_UNDEF
  getActionDefinitionsBuilder(G_CTLZ_ZERO_UNDEF)
      .lowerFor({{S32, S32}})
      .clampScalar(0, S32, S32)
      .clampScalar(1, S32, S32);

  getActionDefinitionsBuilder(G_CTLZ)
      .legalFor({{S32, S32}})
      .clampScalar(0, S32, S32)
      .clampScalar(1, S32, S32);

  getActionDefinitionsBuilder({G_FSHL, G_FSHR}).lower();

  getActionDefinitionsBuilder({G_MEMCPY, G_MEMSET, G_MEMMOVE})
      .customIf([=](const LegalityQuery &Query) {
        const LLT SizeArg = Query.Types[2];
        return SizeArg == S20;
      })
      .libcall();

  getActionDefinitionsBuilder(G_DYN_STACKALLOC).custom();
  getActionDefinitionsBuilder({G_STACKSAVE, G_STACKRESTORE}).lower();

  getActionDefinitionsBuilder({G_SMIN, G_SMAX, G_UMIN, G_UMAX})
      .widenScalarToNextPow2(0, 32)
      .lower();

  getActionDefinitionsBuilder({G_FRAME_INDEX, G_GLOBAL_VALUE}).legalFor({P0});

  getActionDefinitionsBuilder(G_INTTOPTR)
      .legalFor({{P0, S20}})
      .widenScalarToNextPow2(1)
      .clampScalar(1, S20, S20);

  getActionDefinitionsBuilder(G_PTRTOINT)
      .legalFor({{S20, P0}})
      .widenScalarToNextPow2(0)
      .clampScalar(0, S20, S20);

  // We support pointer arithmetic on both GPRs (32-bits) and pointer regs
  // (20-bits, where the scalar addend resides in a MOD register). To allow
  // specifying alternative register bank mappings, we need to truncate the RHS
  // operand to 20-bits, thus we only allow s20 types for the scalar addend
  getActionDefinitionsBuilder(G_PTR_ADD)
      .legalFor({{P0, S20}})
      .widenScalarToNextPow2(1)
      .clampScalar(1, S20, S20);

  getActionDefinitionsBuilder({G_LOAD, G_STORE})
      .legalForTypesWithMemDesc({
          {S32, P0, S8, 8},         {S32, P0, S16, 16},
          {S20, P0, S20, 32},       {S32, P0, S32, 32},
          {P0, P0, S20, 32},        {V16S8, P0, V16S8, 16},
          {V8S16, P0, V8S16, 16},   {V4S32, P0, V4S32, 16},
          {V8S32, P0, V8S32, 32},   {V16S16, P0, V16S16, 32},
          {V32S8, P0, V32S8, 32},   {V16S32, P0, V16S32, 32},
          {V32S16, P0, V32S16, 32}, {V64S8, P0, V64S8, 32},
          {V32S32, P0, V32S32, 32}, {V64S16, P0, V64S16, 32},
          {V128S8, P0, V128S8, 32}, {ACC256, P0, ACC256, 32},
          {ACC512, P0, ACC512, 32}, {ACC1024, P0, ACC1024, 32},
          {S128, P0, S128, 16},
      })
      .widenScalarToNextPow2(0)
      .lowerIfMemSizeNotPow2()
      .clampScalar(0, S32, S32)
      .lower();

  // FIXME: Storing a pointer to an un-aligned address isn't supported.
  getActionDefinitionsBuilder({G_ZEXTLOAD, G_SEXTLOAD})
      .legalForTypesWithMemDesc({{S32, P0, S8, 8}, {S32, P0, S16, 16}})
      .widenScalarToNextPow2(0)
      .lowerIfMemSizeNotPow2()
      .clampScalar(0, S32, S32)
      .lower();

  if (ST.isAIE2()) {
    getActionDefinitionsBuilder(G_EXTRACT_VECTOR_ELT)
        .unsupportedIf([=](const LegalityQuery &Query) {
          const LLT &EltTy = Query.Types[1].getElementType();
          return Query.Types[0] != EltTy;
        })
        // If it is 32-bit, the LLVM can perform some bitshifts to legalize it
        .bitcastIf(
            [=](const LegalityQuery &Query) {
              const LLT &VecTy = Query.Types[1];
              return VecTy.getSizeInBits() == 32;
            },
            bitcastToVectorElement32(1))
        // Extraction is supported for the native types of 32-, 256-, 512- and
        // 1024-bit
        .customIf(typeInSet(1, {V4S8, V2S16, V2S32, V8S32, V16S32, V32S32,
                                V16S16, V32S8, V32S16, V64S8, V64S16, V128S8}))
        // For 16-bits, we want to increase the number of elements to 4. Since
        // our architecture doesn't always support all intermediate sizes, we do
        // it as a special case so that we can use them minimum clamp for the
        // smallest vector register.
        .moreElementsIf(
            [=](const LegalityQuery &Query) {
              return Query.Types[1].getScalarSizeInBits() == 8 &&
                     Query.Types[1].getNumElements() == 2;
            },
            [=](const LegalityQuery &Query) {
              return std::make_pair(1, LLT::fixed_vector(4, S8));
            })
        // Increase the input vectors if they don't fit in the smallest vector
        // register
        .clampMinNumElements(1, S8, 32)
        .clampMinNumElements(1, S16, 16)
        .clampMinNumElements(1, S32, 8);

    getActionDefinitionsBuilder(G_INSERT_VECTOR_ELT)
        .clampScalar(2, S32, S32) // Clamp the idx to 32 bit since VINSERT
                                  // relies on eR29 only for idx.
        .customIf(typeInSet(0, {V2S32, V8S32, V16S16, V32S8, V16S32, V32S16,
                                V64S8, V32S32, V64S16, V128S8}));
  }

  // Control-flow
  getActionDefinitionsBuilder(G_BRCOND).legalFor({S32}).clampScalar(0, S32,
                                                                    S32);

  auto &PHI = getActionDefinitionsBuilder(G_PHI).legalFor({S20, S32, P0});

  if (ST.isAIE2()) {
    PHI.legalFor(AIE2VectorTypes);
    PHI.legalFor(AIE2AccumulatorTypes);
  }

  PHI.widenScalarToNextPow2(0).clampScalar(0, S32, S32);

  // Bitcast - vector source and vector destination - For AIEV2
  if (ST.isAIE2()) {
    const LegalityPredicate IsNotValidDestinationVector =
        negatePredicate(isValidVectorAIE2(0));

    getActionDefinitionsBuilder(G_BITCAST).legalIf(
        LegalityPredicates::all(isLegalBitCastType(0), isLegalBitCastType(1)));

    getActionDefinitionsBuilder(G_MERGE_VALUES).legalFor({{S64, S32}});
    getActionDefinitionsBuilder(G_UNMERGE_VALUES)
        .legalFor({{S32, S64}, {S32, V2S32}})
        .customIf([=](const LegalityQuery &Query) {
          const LLT &DstTy = Query.Types[0];
          const LLT &SrcTy = Query.Types[1];

          return SrcTy.isVector() && DstTy.isScalar() &&
                 DstTy == SrcTy.getElementType();
        })
        .unsupportedIf(IsNotValidDestinationVector)
        .legalIf(isValidVectorMergeUnmergeOp(1, 0));

    getActionDefinitionsBuilder(G_CONCAT_VECTORS)
        .unsupportedIf(IsNotValidDestinationVector)
        .legalIf(isValidVectorMergeUnmergeOp(0, 1));

    getActionDefinitionsBuilder(G_BUILD_VECTOR)
        // Legacy legalization for bitcasts
        .legalFor({{V2S32, S32}})
        .unsupportedIf(IsNotValidDestinationVector)
        // We clamp the high values and not the low ones, sice the former
        // splits the values but the latter keeps the same G_BUILD_VECTOR in
        // the output instructions which causes an infinite loop since it
        // can't reach our custom legalization code.
        .clampMaxNumElements(0, S8, 64)
        .clampMaxNumElements(0, S16, 32)
        .clampMaxNumElements(0, S32, 16)
        .custom();
  }

  getActionDefinitionsBuilder(G_JUMP_TABLE).custom();

  getActionDefinitionsBuilder(G_BRJT).custom();

  getActionDefinitionsBuilder(G_BRINDIRECT).legalFor({P0});

  // Variadic functions
  getActionDefinitionsBuilder(G_VASTART).custom();
  getActionDefinitionsBuilder(G_VAARG).custom();

  getLegacyLegalizerInfo().computeTables();
  verify(*ST.getInstrInfo());
}

bool AIELegalizerInfo::legalizeCustom(LegalizerHelper &Helper, MachineInstr &MI,
                                      LostDebugLocObserver &LocObserver) const {
  switch (MI.getOpcode()) {
  default:
    break;
  case TargetOpcode::G_VASTART:
    return legalizeG_VASTART(Helper, MI);
  case TargetOpcode::G_VAARG:
    return legalizeG_VAARG(Helper, MI);
  case TargetOpcode::G_MEMSET:
  case TargetOpcode::G_MEMCPY:
  case TargetOpcode::G_MEMMOVE:
    return legalizeMemCalls(Helper, MI, LocObserver);
  case TargetOpcode::G_BRJT:
    return legalizeG_BRJT(Helper, MI);
  case TargetOpcode::G_FCONSTANT:
    return legalizeG_FCONSTANT(Helper, MI);
  case TargetOpcode::G_JUMP_TABLE:
    return legalizeG_JUMP_TABLE(Helper, MI);
  case TargetOpcode::G_DYN_STACKALLOC:
    return legalizeG_DYN_STACKALLOC(Helper, MI);
  case TargetOpcode::G_EXTRACT_VECTOR_ELT:
    return legalizeG_EXTRACT_VECTOR_ELT(Helper, MI);
  case TargetOpcode::G_INSERT_VECTOR_ELT:
    return legalizeG_INSERT_VECTOR_ELT(Helper, MI);
  case TargetOpcode::G_FCMP:
    return legalizeG_FCMP(Helper, MI, LocObserver);
  case TargetOpcode::G_FPTRUNC:
    return legalizeG_FPTRUNC(Helper, MI);
  case TargetOpcode::G_FPEXT:
    return legalizeG_FPEXT(Helper, MI);
  case TargetOpcode::G_FABS:
    return legalizeG_FABS(Helper, MI);
  case TargetOpcode::G_FADD:
  case TargetOpcode::G_FSUB:
    return legalizeG_FADDSUB(Helper, MI);
  case TargetOpcode::G_BUILD_VECTOR:
    return legalizeG_BUILD_VECTOR(Helper, MI);
  case TargetOpcode::G_UNMERGE_VALUES:
    return legalizeG_UNMERGE_VALUES(Helper, MI);
  case TargetOpcode::G_SEXT_INREG:
    return legalizeG_SEXT_INREG(Helper, MI);
  }

  llvm_unreachable("Un-expected custom legalization");
}

bool AIELegalizerInfo::pack32BitVector(LegalizerHelper &Helper,
                                       MachineInstr &MI,
                                       Register SourceReg) const {
  MachineIRBuilder &MIRBuilder = Helper.MIRBuilder;
  MachineRegisterInfo &MRI = *MIRBuilder.getMRI();

  const LLT SourceRegTy = MRI.getType(SourceReg);
  const Register DstReg = MI.getOperand(0).getReg();
  assert(SourceRegTy.getSizeInBits() == 32 &&
         "cannot pack vectors larger or smaller than 32-bit");

  const LLT S32 = LLT::scalar(32);
  unsigned Offset = 0;
  Register DstCastReg = MRI.createGenericVirtualRegister(S32);

  // Skip the destination operand since that is where we are writing to.
  MachineOperand *Operand = MI.operands_begin() + 1,
                 *OperandEnd = MI.operands_end();
  MIRBuilder.buildConstant(DstCastReg, 0);

  const LLT RegTy = MRI.getType(DstReg);
  while (Operand != OperandEnd) {
    Register DestinationOperand = Operand->getReg();

    if (RegTy.getScalarSizeInBits() != 32) {
      const Register TmpReg32 = MRI.createGenericVirtualRegister(S32);
      MIRBuilder.buildInstr(AIE2::G_ZEXT, {TmpReg32}, {DestinationOperand});
      DestinationOperand = TmpReg32;
    }

    // Avoid a useless shift for the first element, since it doesn't get
    // optimized out in O0.
    const Register AccumulatorReg = MRI.createGenericVirtualRegister(S32);
    if (Offset != 0) {
      const MachineInstrBuilder ShiftConstant =
          MIRBuilder.buildConstant(S32, Offset);
      const MachineInstrBuilder Masked =
          MIRBuilder.buildShl(S32, DestinationOperand, ShiftConstant);
      MIRBuilder.buildOr(AccumulatorReg, DstCastReg, Masked);
    } else {
      MIRBuilder.buildOr(AccumulatorReg, DstCastReg, DestinationOperand);
    }

    DstCastReg = AccumulatorReg;
    Offset += RegTy.getScalarSizeInBits();
    ++Operand;
  }

  MIRBuilder.buildBitcast(DstReg, DstCastReg);
  MI.eraseFromParent();
  return true;
}

bool AIELegalizerInfo::unpack32BitVector(LegalizerHelper &Helper,
                                         MachineInstr &MI,
                                         Register SourceReg) const {
  MachineIRBuilder &MIRBuilder = Helper.MIRBuilder;
  MachineRegisterInfo &MRI = *MIRBuilder.getMRI();

  const LLT SourceRegTy = MRI.getType(SourceReg);
  assert(SourceRegTy.getSizeInBits() == 32 &&
         "cannot unpack vectors larger or smaller than 32-bit");

  const LLT S32 = LLT::scalar(32);
  unsigned Offset = 0;
  Register DstCastReg = MRI.createGenericVirtualRegister(S32);

  MachineOperand *Operand = MI.operands_begin(),
                 *OperandEnd = MI.operands_end() - 1;
  const LLT RegTy = MRI.getType(Operand->getReg());
  MIRBuilder.buildBitcast(DstCastReg, SourceReg);
  while (Operand != OperandEnd) {
    Register DestinationOperand = Operand->getReg();
    // Avoid a useless shift for the first element, since it doesn't get
    // optimized out in O0.
    if (Offset != 0) {
      const MachineInstrBuilder ShiftConstant =
          MIRBuilder.buildConstant(S32, Offset);
      const MachineInstrBuilder Masked =
          MIRBuilder.buildLShr(S32, DstCastReg, ShiftConstant);
      MIRBuilder.buildTrunc(DestinationOperand, Masked);

    } else {
      MIRBuilder.buildTrunc(DestinationOperand, DstCastReg);
    }

    Offset += RegTy.getScalarSizeInBits();
    ++Operand;
  }

  MI.eraseFromParent();
  return true;
}

bool AIELegalizerInfo::legalizeG_BUILD_VECTOR(LegalizerHelper &Helper,
                                              MachineInstr &MI) const {
  MachineIRBuilder &MIRBuilder = Helper.MIRBuilder;
  MachineRegisterInfo &MRI = *MIRBuilder.getMRI();

  const Register DstReg = MI.getOperand(0).getReg();
  LLT DstVecTy = MRI.getType(DstReg);
  assert(DstVecTy.isVector());
  unsigned DstVecSize = DstVecTy.getSizeInBits();
  const LLT DstVecEltTy = DstVecTy.getElementType();
  const unsigned EltSize = DstVecEltTy.getScalarSizeInBits();
  assert((EltSize == 8 || EltSize == 16 || EltSize == 32) &&
         "non-existent integer size");
  assert(DstVecSize == 32 || (DstVecSize > 64 && DstVecSize <= 1024 &&
                              "non-native vectors are not supported"));
  assert(DstVecSize < 1024 && "vadd takes a 512-bit argument");

  // If our vector is 32-bit we can store it as packed integer vector
  if (DstVecSize == 32)
    return pack32BitVector(Helper, MI, DstReg);

  // We are using an undef since we are building over multiple instructions
  const TypeSize VecEltTySize = DstVecEltTy.getSizeInBits();
  const LLT VecTy = LLT::fixed_vector(512 / VecEltTySize, DstVecEltTy);
  Register Src = MRI.createGenericVirtualRegister(VecTy);
  MIRBuilder.buildUndef(Src);

  MachineOperand *OperandBegin = MI.operands_begin(),
                 *Operand = MI.operands_end() - 1;
  while (Operand != OperandBegin) {
    Register Reg = Operand->getReg();
    Register Dst = MRI.createGenericVirtualRegister(VecTy);

    if (DstVecSize == 512 && Operand == OperandBegin + 1)
      Dst = DstReg;

    // vpush takes 32-bit operands so we sign extend the input variable. This is
    // required here since we don't have 16 or 32-bit registers.
    if (DstVecEltTy.getSizeInBits() != 32) {
      const Register TmpReg32 =
          MRI.createGenericVirtualRegister(LLT::scalar(32));
      MIRBuilder.buildInstr(AIE2::G_ANYEXT, {TmpReg32}, {Reg});
      Reg = TmpReg32;
    }

    MIRBuilder.buildInstr(AIE2::G_AIE_ADD_VECTOR_ELT_LEFT, {Dst}, {Src, Reg});
    Src = Dst;
    --Operand;
  }

  // For >512, the G_CONCAT_VECTOR is used instead which is added by the
  // automatic rules.
  // TODO: replace this with G_EXTRACT_SUBVECTOR when it lands into our tree.
  //    https://github.com/llvm/llvm-project/pull/84538
  if (DstVecSize == 256) {
    const Register UnusedSubReg = MRI.createGenericVirtualRegister(DstVecTy);
    MIRBuilder.buildUnmerge({DstReg, UnusedSubReg}, Src);
  } else if (DstVecSize == 128) {
    MIRBuilder.buildInstr(AIE2::G_AIE_UNPAD_VECTOR, {DstReg}, {Src});
  }

  MI.eraseFromParent();
  return true;
}

bool AIELegalizerInfo::legalizeG_UNMERGE_VALUES(LegalizerHelper &Helper,
                                                MachineInstr &MI) const {
  MachineIRBuilder &MIRBuilder = Helper.MIRBuilder;
  MachineRegisterInfo &MRI = *MIRBuilder.getMRI();

  const Register FirstReg = MI.getOperand(0).getReg();
  const Register LastReg = MI.getOperand(MI.getNumOperands() - 1).getReg();
  const LLT FirstTy = MRI.getType(FirstReg);
  const LLT LastTy = MRI.getType(LastReg);
  assert(LastTy.isVector() &&
         (FirstTy.getScalarSizeInBits() * (MI.getNumOperands() - 1)) ==
             LastTy.getSizeInBits() &&
         "This operation is only supported for vectors");

  if (LastTy.getSizeInBits() == 32)
    return unpack32BitVector(Helper, MI, LastReg);

  // Pad vectors of 128-bit vectors to 256-bit
  Register TargetReg = LastReg;
  if (LastTy.getSizeInBits() == 128) {
    const LLT NewRegTy =
        LLT::fixed_vector(LastTy.getNumElements() * 2, LastTy.getScalarType());
    const Register NewReg = MRI.createGenericVirtualRegister(NewRegTy);
    MIRBuilder.buildInstr(AIE2::G_AIE_PAD_VECTOR_UNDEF, {NewReg}, {LastReg});
    TargetReg = NewReg;
  }

  const unsigned NumOperands = MI.getNumOperands() - 1;
  for (unsigned Index = 0; Index < NumOperands; ++Index) {
    const Register Current = MI.getOperand(Index).getReg();
    const LLT CurrentTy = MRI.getType(Current);
    assert(CurrentTy.isScalar() &&
           "this operation is only supported for scalar types");

    // We build the constant ourselves since the default behaviour
    // of the builtin is to create 64-bit constants.
    const MachineInstrBuilder CurrentIndex =
        MIRBuilder.buildConstant(LLT::scalar(32), Index);
    MIRBuilder.buildExtractVectorElement(Current, TargetReg, CurrentIndex);
  }

  MI.eraseFromParent();
  return true;
}

bool AIELegalizerInfo::legalizeG_SEXT_INREG(LegalizerHelper &Helper,
                                            MachineInstr &MI) const {

  MachineIRBuilder &MIRBuilder = Helper.MIRBuilder;
  MachineRegisterInfo &MRI = *MIRBuilder.getMRI();

  const Register DestReg = MI.getOperand(0).getReg();
  const LLT DestRegTy = MRI.getType(DestReg);
  const LLT S32 = LLT::scalar(32);

  const int64_t Imm = MI.getOperand(2).getImm();
  if ((Imm != 8 && Imm != 16) || DestRegTy != S32)
    Helper.lowerSextInreg(MI);

  return true;
}

bool AIELegalizerInfo::legalizeG_VASTART(LegalizerHelper &Helper,
                                         MachineInstr &MI) const {
  MachineIRBuilder &MIRBuilder = Helper.MIRBuilder;
  MachineFunction &MF = MIRBuilder.getMF();
  auto *FuncInfo = MF.getInfo<AIEMachineFunctionInfo>();
  Register ListPtr = MI.getOperand(0).getReg();
  LLT PtrTy = MIRBuilder.getMRI()->getType(ListPtr);
  const Align PtrAlign = AIEBaseTargetLowering::getStackArgumentAlignment();

  Register VAList =
      MIRBuilder.buildFrameIndex(PtrTy, FuncInfo->getVarArgsFrameIndex())
          .getReg(0);
  MIRBuilder.buildStore(VAList, ListPtr,
                        *MF.getMachineMemOperand(MachinePointerInfo(),
                                                 MachineMemOperand::MOStore,
                                                 PtrTy, PtrAlign));

  MI.eraseFromParent();
  return true;
}

bool AIELegalizerInfo::legalizeG_VAARG(LegalizerHelper &Helper,
                                       MachineInstr &MI) const {
  MachineIRBuilder &MIRBuilder = Helper.MIRBuilder;
  MachineFunction &MF = MIRBuilder.getMF();
  MachineRegisterInfo &MRI = *MIRBuilder.getMRI();

  Align Alignment(MI.getOperand(2).getImm());
  const Align PtrAlign = AIEBaseTargetLowering::getStackArgumentAlignment();
  assert((Alignment <= PtrAlign) &&
         "Vaarg alignment is greater than the one of stack slots.");

  // Retrieve the vararg list pointer.
  Register ListPtr = MI.getOperand(1).getReg();
  LLT PtrTy = MRI.getType(ListPtr);
  auto VAList = MIRBuilder.buildLoad(
      PtrTy, ListPtr,
      *MF.getMachineMemOperand(MachinePointerInfo(), MachineMemOperand::MOLoad,
                               PtrTy, PtrAlign));

  // Compute the size of the current vararg slot. That is smallest multiple of
  // PtrAlign which can fit the vararg type.
  Register Dst = MI.getOperand(0).getReg();
  LLT ValTy = MRI.getType(Dst);
  unsigned ArgSize = alignTo(ValTy.getSizeInBytes(), PtrAlign);

  // Compute the address of the current VAARG by subtracting its size
  // from the previous VAARG address.
  LLT IntTy = LLT::scalar(32);
  auto Offset = MIRBuilder.buildConstant(IntTy, -ArgSize);
  auto NewVAList = MIRBuilder.buildPtrAdd(PtrTy, VAList.getReg(0), Offset);

  // Actually load the vararg and feed it into Dst
  MIRBuilder.buildLoad(
      Dst, NewVAList,
      *MF.getMachineMemOperand(MachinePointerInfo(), MachineMemOperand::MOLoad,
                               ValTy, std::max(Alignment, PtrAlign)));

  // Then store the new vararg list pointer so it can be used for next G_VARARG.
  MIRBuilder.buildStore(NewVAList, ListPtr,
                        *MF.getMachineMemOperand(MachinePointerInfo(),
                                                 MachineMemOperand::MOStore,
                                                 PtrTy, PtrAlign));

  MI.eraseFromParent();
  return true;
}

bool AIELegalizerInfo::legalizeMemCalls(
    LegalizerHelper &Helper, MachineInstr &MI,
    LostDebugLocObserver &LocObserver) const {
  MachineIRBuilder &MIRBuilder = Helper.MIRBuilder;
  LLVMContext &Ctx = MIRBuilder.getMF().getFunction().getContext();
  MachineRegisterInfo &MRI = *MIRBuilder.getMRI();
  Register ResultReg = MRI.createGenericVirtualRegister(LLT::pointer(0, 20));

  RTLIB::Libcall LibEntry = RTLIB::UNKNOWN_LIBCALL;
  PointerType *VoidPtrTy = PointerType::get(Ctx, 0);
  IntegerType *IntTy = Type::getInt32Ty(Ctx);
  LLT S32 = LLT::scalar(32);

  SmallVector<CallLowering::ArgInfo, 3> Args;
  Args.emplace_back(MI.getOperand(0).getReg(), VoidPtrTy, 0);
  MachineInstrBuilder ZextSize = MIRBuilder.buildZExt(S32, MI.getOperand(2));

  switch (MI.getOpcode()) {
  case TargetOpcode::G_MEMSET: {
    LibEntry = RTLIB::MEMSET;
    Register CharReg = MI.getOperand(1).getReg();
    LLT CharLLT = MRI.getType(CharReg);
    if (CharLLT != S32) {
      MachineInstrBuilder ZextChar =
          MIRBuilder.buildZExt(S32, MI.getOperand(1));
      CharReg = ZextChar->getOperand(0).getReg();
    }
    Args.emplace_back(CharReg, IntTy, 0);
    Args.emplace_back(ZextSize->getOperand(0).getReg(), IntTy, 0);
    break;
  }
  case TargetOpcode::G_MEMCPY:
    LibEntry = RTLIB::MEMCPY;
    Args.emplace_back(MI.getOperand(1).getReg(), VoidPtrTy, 0);
    Args.emplace_back(ZextSize->getOperand(0).getReg(), IntTy, 0);
    break;
  case TargetOpcode::G_MEMMOVE:
    LibEntry = RTLIB::MEMMOVE;
    Args.emplace_back(MI.getOperand(1).getReg(), VoidPtrTy, 0);
    Args.emplace_back(ZextSize->getOperand(0).getReg(), IntTy, 0);
    break;
  default:
    return false;
  }

  auto Status = createLibcall(MIRBuilder, LibEntry, {ResultReg, VoidPtrTy, 0},
                              Args, LocObserver);
  if (Status != LegalizerHelper::Legalized) {
    return false;
  }

  MI.eraseFromParent();
  return true;
}

bool AIELegalizerInfo::legalizeG_BRJT(LegalizerHelper &Helper,
                                      MachineInstr &MI) const {
  MachineIRBuilder &MIRBuilder = Helper.MIRBuilder;
  MachineFunction &MF = MIRBuilder.getMF();
  LLT S32 = LLT::scalar(32);
  LLT P0 = LLT::pointer(0, 20);
  unsigned EntrySize = MF.getJumpTableInfo()->getEntrySize(MF.getDataLayout());

  auto CopyIndexTo32 = MIRBuilder.buildZExt(S32, MI.getOperand(2));
  auto ConstantShift = MIRBuilder.buildConstant(S32, Log2_32(EntrySize));
  auto LShift = MIRBuilder.buildShl(S32, CopyIndexTo32->getOperand(0),
                                    ConstantShift->getOperand(0));
  auto PtrAdd =
      MIRBuilder.buildPtrAdd(P0, MI.getOperand(0), LShift->getOperand(0));
  auto *MMO = MF.getMachineMemOperand(
      MachinePointerInfo(), MachineMemOperand::MOLoad, P0,
      Align(MF.getJumpTableInfo()->getEntryAlignment(MF.getDataLayout())));
  auto LoadAddress = MIRBuilder.buildLoad(P0, PtrAdd->getOperand(0), *MMO);
  MIRBuilder.buildBrIndirect(LoadAddress->getOperand(0).getReg());

  MI.eraseFromParent();
  return true;
}

bool AIELegalizerInfo::legalizeG_FCONSTANT(LegalizerHelper &Helper,
                                           MachineInstr &MI) const {
  MachineIRBuilder &MIRBuilder = Helper.MIRBuilder;
  LLVMContext &Ctx = MIRBuilder.getMF().getFunction().getContext();

  // Convert to integer constants, while preserving the binary representation.
  auto AsInteger = MI.getOperand(1).getFPImm()->getValueAPF().bitcastToAPInt();
  MIRBuilder.buildConstant(MI.getOperand(0), *ConstantInt::get(Ctx, AsInteger));

  MI.eraseFromParent();
  return true;
}

bool AIELegalizerInfo::legalizeG_JUMP_TABLE(LegalizerHelper &Helper,
                                            MachineInstr &MI) const {
  Helper.Observer.changingInstr(MI);
  MachineIRBuilder &MIRBuilder = Helper.MIRBuilder;
  MI.setDesc(MIRBuilder.getTII().get(TargetOpcode::G_GLOBAL_VALUE));
  Helper.Observer.changedInstr(MI);
  return true;
}

bool AIELegalizerInfo::legalizeG_DYN_STACKALLOC(LegalizerHelper &Helper,
                                                MachineInstr &MI) const {
  MachineIRBuilder &MIRBuilder = Helper.MIRBuilder;
  const LLT P0 = LLT::pointer(0, 20);
  Register Dst = MI.getOperand(0).getReg();
  auto Size = MI.getOperand(1);
  Register SPReg =
      Helper.getTargetLowering().getStackPointerRegisterToSaveRestore();

  auto SPTmp = MIRBuilder.buildCopy(P0, SPReg);
  MIRBuilder.buildCopy(Dst, SPTmp);
  SPTmp = MIRBuilder.buildPtrAdd(P0, SPTmp, Size);
  MIRBuilder.buildCopy(SPReg, SPTmp);

  MI.removeFromParent();
  return true;
}

//%2:_(s8) = G_EXTRACT_VECTOR_ELT %0, %1
//==>
//%3:_(s32) = G_AIE_SEXT_EXTRACT_VECTOR_ELT %0, %1
//%4:_(s32) = G_ASSERT_SEXT %3, Elt_Size
//%2:_(s8) = G_TRUNC %4
bool AIELegalizerInfo::legalizeG_EXTRACT_VECTOR_ELT(LegalizerHelper &Helper,
                                                    MachineInstr &MI) const {
  MachineIRBuilder &MIRBuilder = Helper.MIRBuilder;
  MachineRegisterInfo &MRI = *MIRBuilder.getMRI();
  const Register DstReg = MI.getOperand(0).getReg();
  const Register SrcVecReg = MI.getOperand(1).getReg();
  const Register IdxReg = MI.getOperand(2).getReg();
  const LLT SrcVecTy = MRI.getType(SrcVecReg);
  const unsigned SrcVecSize = SrcVecTy.getSizeInBits();
  const LLT SrcVecEltTy = SrcVecTy.getElementType();
  assert(SrcVecEltTy == MRI.getType(DstReg));

  const LLT S32 = LLT::scalar(32);
  switch (SrcVecSize) {
  case 64: {
    assert(SrcVecTy == LLT::fixed_vector(2, 32) && "Unexpected 64bit vector!");
    const Register Reg0 = MRI.createGenericVirtualRegister(S32);
    const Register Reg1 = MRI.createGenericVirtualRegister(S32);
    MIRBuilder.buildUnmerge({Reg0, Reg1}, SrcVecReg);

    auto IdxVal = getIConstantVRegValWithLookThrough(IdxReg, MRI);
    if (!IdxVal)
      MIRBuilder.buildSelect(DstReg, IdxReg, Reg1, Reg0);
    else {
      const unsigned LaneIdx = IdxVal->Value.getZExtValue();
      if (LaneIdx)
        MIRBuilder.buildCopy(DstReg, Reg1);
      else
        MIRBuilder.buildCopy(DstReg, Reg0);
    }
    break;
  }
  case 256:
  case 512:
  case 1024: {
    const LLT S8 = LLT::scalar(8);
    const LLT S16 = LLT::scalar(16);
    bool IsS32 = SrcVecEltTy == S32;
    assert((SrcVecEltTy == S8 || SrcVecEltTy == S16 || IsS32) &&
           "Unexpected vector element type for extract vector elt!");
    if (!IsS32) {
      const Register ExtEltDstReg = MRI.createGenericVirtualRegister(S32);
      const Register ExtDstReg = MRI.createGenericVirtualRegister(S32);
      MIRBuilder.buildInstr(AIE2::G_AIE_SEXT_EXTRACT_VECTOR_ELT, {ExtEltDstReg},
                            {SrcVecReg, IdxReg});
      MIRBuilder.buildAssertInstr(TargetOpcode::G_ASSERT_SEXT, ExtDstReg,
                                  ExtEltDstReg, SrcVecEltTy.getSizeInBits());
      MIRBuilder.buildTrunc(DstReg, ExtDstReg);
    } else {
      MIRBuilder.buildInstr(AIE2::G_AIE_SEXT_EXTRACT_VECTOR_ELT, {DstReg},
                            {SrcVecReg, IdxReg});
    }
    break;
  }
  default:
    llvm_unreachable("Unexpected vector size for extract vector elt!");
  }
  MI.removeFromParent();
  return true;
}

bool AIELegalizerInfo::legalizeG_INSERT_VECTOR_ELT(LegalizerHelper &Helper,
                                                   MachineInstr &MI) const {
  MachineIRBuilder &MIRBuilder = Helper.MIRBuilder;
  MachineRegisterInfo &MRI = *MIRBuilder.getMRI();
  const Register DstVecReg = MI.getOperand(0).getReg();
  const Register SrcVecReg = MI.getOperand(1).getReg();
  const Register ValReg = MI.getOperand(2).getReg();
  const Register IdxReg = MI.getOperand(3).getReg();
  const LLT DstVecTy = MRI.getType(DstVecReg);
  const unsigned DstVecSize = DstVecTy.getSizeInBits();
  const LLT S32 = LLT::scalar(32);

  switch (DstVecSize) {
  case 64: {
    if (DstVecTy != LLT::fixed_vector(2, 32)) {
      llvm_unreachable("Unexpected 64-bit vector type!");
    }
    std::array<Register, 2> Regs = {MRI.createGenericVirtualRegister(S32),
                                    MRI.createGenericVirtualRegister(S32)};
    MIRBuilder.buildUnmerge(Regs, SrcVecReg);
    auto IdxVal = getIConstantVRegValWithLookThrough(IdxReg, MRI);
    if (IdxVal) {
      unsigned Idx = IdxVal->Value.getZExtValue();
      assert(Idx < Regs.size() && "Expected idx to be 0 or 1.");
      Regs[Idx] = ValReg;
      MIRBuilder.buildBuildVector(DstVecReg, Regs);
    } else {
      std::array<Register, 2> TmpSelDsts = {
          MRI.createGenericVirtualRegister(S32),
          MRI.createGenericVirtualRegister(S32)};
      MIRBuilder.buildSelect(TmpSelDsts[0], IdxReg, Regs[0], ValReg);
      MIRBuilder.buildSelect(TmpSelDsts[1], IdxReg, ValReg, Regs[1]);
      MIRBuilder.buildBuildVector(DstVecReg, TmpSelDsts);
    }
    break;
  }
  case 256:
  case 512:
  case 1024: {
    const LLT ValTy = MRI.getType(ValReg);
    if (ValTy == LLT::scalar(64)) {
      llvm_unreachable("Unexpected scalar value type for insert vec elt!");
    }
    Register NewValReg;
    if (ValTy == LLT::scalar(8) || ValTy == LLT::scalar(16)) {
      NewValReg = MRI.createGenericVirtualRegister(S32);
      MIRBuilder.buildAnyExt(NewValReg, ValReg);
    } else {
      NewValReg = ValReg;
    }
    MIRBuilder.buildInstr(AIE2::G_AIE_INSERT_VECTOR_ELT, {DstVecReg},
                          {SrcVecReg, NewValReg, IdxReg});
    break;
  }
  default:
    llvm_unreachable("Unexpected vector size for insert vector elt!");
  }
  MI.removeFromParent();
  return true;
}

static RTLIB::Libcall getFCmpLibCall(CmpInst::Predicate Predicate,
                                     CmpInst::Predicate &IPredicate) {
  switch (Predicate) {
  default:
    llvm_unreachable("Unsupported FCmp predicate");
  case CmpInst::FCMP_OEQ:
    IPredicate = CmpInst::ICMP_EQ;
    return RTLIB::OEQ_F32;
  case CmpInst::FCMP_OGE:
    IPredicate = CmpInst::ICMP_SGE;
    return RTLIB::OGE_F32;
  case CmpInst::FCMP_OGT:
    IPredicate = CmpInst::ICMP_SGT;
    return RTLIB::OGT_F32;
  case CmpInst::FCMP_OLE:
    IPredicate = CmpInst::ICMP_SLE;
    return RTLIB::OLE_F32;
  case CmpInst::FCMP_OLT:
    IPredicate = CmpInst::ICMP_SLT;
    return RTLIB::OLT_F32;
  case CmpInst::FCMP_ORD:
    IPredicate = CmpInst::ICMP_EQ;
    return RTLIB::UO_F32;
  /* Unordered comparisons are built from
   * the complement of the ordered ones */
  case CmpInst::FCMP_UGE:
    IPredicate = CmpInst::ICMP_SGE;
    return RTLIB::OLT_F32;
  case CmpInst::FCMP_UGT:
    IPredicate = CmpInst::ICMP_SGT;
    return RTLIB::OLE_F32;
  case CmpInst::FCMP_ULE:
    IPredicate = CmpInst::ICMP_SLE;
    return RTLIB::OGT_F32;
  case CmpInst::FCMP_ULT:
    IPredicate = CmpInst::ICMP_SLT;
    return RTLIB::OGE_F32;
  case CmpInst::FCMP_UNE:
    IPredicate = CmpInst::ICMP_NE;
    return RTLIB::UNE_F32;
  case CmpInst::FCMP_UNO:
    IPredicate = CmpInst::ICMP_NE;
    return RTLIB::UO_F32;
  }
}

bool AIELegalizerInfo::legalizeG_FCMP(LegalizerHelper &Helper, MachineInstr &MI,
                                      LostDebugLocObserver &LocObserver) const {
  MachineIRBuilder &MIRBuilder = Helper.MIRBuilder;
  LLVMContext &Ctx = MIRBuilder.getMF().getFunction().getContext();
  MachineRegisterInfo &MRI = *MIRBuilder.getMRI();
  assert(MRI.getType(MI.getOperand(2).getReg()) ==
             MRI.getType(MI.getOperand(3).getReg()) &&
         "Mismatched operands for G_FCMP");

  auto DstReg = MI.getOperand(0).getReg();
  auto FPredicate =
      static_cast<CmpInst::Predicate>(MI.getOperand(1).getPredicate());
  assert(CmpInst::isFPPredicate(FPredicate) && "Unsupported FCmp predicate");
  RTLIB::Libcall Libcall = RTLIB::UNKNOWN_LIBCALL;
  CmpInst::Predicate IPredicate = CmpInst::BAD_ICMP_PREDICATE;
  SmallVector<std::pair<RTLIB::Libcall, CmpInst::Predicate>, 2> Libcalls;
  switch (FPredicate) {
  case CmpInst::FCMP_TRUE:
  case CmpInst::FCMP_FALSE: {
    MIRBuilder.buildConstant(DstReg, FPredicate == CmpInst::FCMP_TRUE ? 1 : 0);
    MI.eraseFromParent();
    return true;
  }
  /* Compose ordered and unequal operands as follows:
   * a != b ==> a > b || a < b */
  case CmpInst::FCMP_ONE:
    Libcalls.push_back(std::make_pair(RTLIB::OGT_F32, CmpInst::ICMP_SGT));
    Libcalls.push_back(std::make_pair(RTLIB::OLT_F32, CmpInst::ICMP_SLT));
    break;
  /* Compose unordered or equal operands as follows:
   * (unord(a, b) or a == b) ==> a == b || a != b */
  case CmpInst::FCMP_UEQ:
    Libcalls.push_back(std::make_pair(RTLIB::OEQ_F32, CmpInst::ICMP_EQ));
    Libcalls.push_back(std::make_pair(RTLIB::UO_F32, CmpInst::ICMP_NE));
    break;
  default:
    Libcall = getFCmpLibCall(FPredicate, IPredicate);
    Libcalls.push_back(std::make_pair(Libcall, IPredicate));
    break;
  }
  auto *ArgTy = Type::getFloatTy(Ctx);
  auto *RetTy = Type::getInt32Ty(Ctx);

  SmallVector<Register, 2> Results;
  for (auto &[LibEntry, Predicate] : Libcalls) {
    auto LibcallResult = MRI.createGenericVirtualRegister(LLT::scalar(32));
    auto Status = createLibcall(MIRBuilder, LibEntry, {LibcallResult, RetTy, 0},
                                {{MI.getOperand(2).getReg(), ArgTy, 0},
                                 {MI.getOperand(3).getReg(), ArgTy, 0}},
                                LocObserver);

    if (Status != LegalizerHelper::Legalized)
      return false;

    auto NewDstReg =
        Libcalls.size() == 1
            ? DstReg
            : MRI.createGenericVirtualRegister(MRI.getType(DstReg));

    CmpInst::Predicate IDestPred = Predicate;
    // Compare against 0. Example, a ole b is transformed to ole(a, b) <= 0
    assert(CmpInst::isIntPredicate(IDestPred) && "Expected Int Predicate");
    auto Zero = MIRBuilder.buildConstant(LLT::scalar(32), 0);
    MIRBuilder.buildICmp(IDestPred, NewDstReg, LibcallResult, Zero);
    Results.push_back(NewDstReg);
  }
  // OR the results when we have two libcalls
  if (Results.size() != 1) {
    assert(Results.size() == 2 && "Unexpected Number of Results");
    MIRBuilder.buildOr(DstReg, Results[0], Results[1]);
  }
  MI.eraseFromParent();
  return true;
}

bool AIELegalizerInfo::legalizeG_FPTRUNC(LegalizerHelper &Helper,
                                         MachineInstr &MI) const {
  MachineIRBuilder &MIRBuilder = Helper.MIRBuilder;
  MachineRegisterInfo &MRI = *MIRBuilder.getMRI();

  Register DstReg = MI.getOperand(0).getReg();
  Register SrcReg = MI.getOperand(1).getReg();
  LLT DstTy = MRI.getType(DstReg);
  LLT SrcTy = MRI.getType(SrcReg);

  // We only handle single precision to bfloat16 conversion
  if (DstTy != LLT::scalar(16) || SrcTy != LLT::scalar(32))
    return false;

  LLT ACC512 = LLT::fixed_vector(8, 64);
  LLT V16S32 = LLT::fixed_vector(16, 32);
  LLT V16S16 = LLT::fixed_vector(16, 16);
  Register Vec512Reg = MRI.createGenericVirtualRegister(V16S32);
  Register Vec512Undef = MRI.createGenericVirtualRegister(V16S32);
  Register IdxReg = MRI.createGenericVirtualRegister(LLT::scalar(32));
  MIRBuilder.buildUndef(Vec512Undef);
  MIRBuilder.buildConstant(IdxReg, 0);
  MIRBuilder.buildInstr(AIE2::G_INSERT_VECTOR_ELT, {Vec512Reg},
                        {Vec512Undef, SrcReg, IdxReg});

  Register Acc512Reg = MRI.createGenericVirtualRegister(ACC512);
  MIRBuilder.buildBitcast(Acc512Reg, Vec512Reg);

  Register Vec256Reg = MRI.createGenericVirtualRegister(V16S16);
  MIRBuilder
      .buildIntrinsic(Intrinsic::aie2_v16accfloat_to_v16bf16, Vec256Reg, true, false)
      .addUse(Acc512Reg);

  MIRBuilder.buildInstr(TargetOpcode::G_EXTRACT_VECTOR_ELT, {DstReg},
                        {Vec256Reg, IdxReg});
  MI.eraseFromParent();
  return true;
}

bool AIELegalizerInfo::legalizeG_FPEXT(LegalizerHelper &Helper,
                                       MachineInstr &MI) const {
  MachineIRBuilder &MIRBuilder = Helper.MIRBuilder;
  MachineRegisterInfo &MRI = *MIRBuilder.getMRI();

  LLT S32 = LLT::scalar(32);

  Register DstReg = MI.getOperand(0).getReg();
  Register SrcReg = MI.getOperand(1).getReg();
  LLT DstTy = MRI.getType(DstReg);
  LLT SrcTy = MRI.getType(SrcReg);

  // We only handle bfloat16 to single precision conversion
  if (DstTy != LLT::scalar(32) || SrcTy != LLT::scalar(16))
    return false;

  Register AnyExt = MIRBuilder.buildAnyExt(S32, SrcReg).getReg(0);
  Register Cst = MIRBuilder.buildConstant(S32, 16).getReg(0);
  MIRBuilder.buildShl(DstReg, AnyExt, Cst);

  MI.eraseFromParent();
  return true;
}

// Legalized by masking sign bit of both double and float
bool AIELegalizerInfo::legalizeG_FABS(LegalizerHelper &Helper,
                                      MachineInstr &MI) const {
  MachineIRBuilder &MIRBuilder = Helper.MIRBuilder;
  MachineRegisterInfo &MRI = *MIRBuilder.getMRI();

  Register DstReg = MI.getOperand(0).getReg();
  Register SrcReg = MI.getOperand(1).getReg();
  LLT SrcTy = MRI.getType(SrcReg);
  if (SrcTy == LLT::scalar(64)) {
    Register SrcLSB = MRI.createGenericVirtualRegister(LLT::scalar(32));
    Register SrcMSB = MRI.createGenericVirtualRegister(LLT::scalar(32));
    Register AndDst = MRI.createGenericVirtualRegister(LLT::scalar(32));

    MIRBuilder.buildInstr(TargetOpcode::G_UNMERGE_VALUES, {SrcLSB, SrcMSB},
                          {SrcReg});
    auto Ones = MIRBuilder.buildConstant(LLT::scalar(32), 0x7fffffff);
    MIRBuilder.buildAnd(AndDst, SrcMSB, Ones);
    MIRBuilder.buildInstr(TargetOpcode::G_MERGE_VALUES, {DstReg},
                          {SrcLSB, AndDst});
  } else if (SrcTy == LLT::scalar(32)) {
    auto Ones = MIRBuilder.buildConstant(LLT::scalar(32), 0x7fffffff);
    MIRBuilder.buildAnd(DstReg, SrcReg, Ones);
  }

  MI.eraseFromParent();
  return true;
}

bool AIELegalizerInfo::legalizeG_FADDSUB(LegalizerHelper &Helper,
                                         MachineInstr &MI) const {
  MachineIRBuilder &MIRBuilder = Helper.MIRBuilder;
  MachineRegisterInfo &MRI = *MIRBuilder.getMRI();

  Register DstReg = MI.getOperand(0).getReg();
  Register SrcReg1 = MI.getOperand(1).getReg();
  Register SrcReg2 = MI.getOperand(2).getReg();

  assert(MRI.getType(DstReg) == LLT::scalar(16) &&
         "Expected bfloat16 type in custom legalization.");

  const LLT S32 = LLT::scalar(32);
  const LLT V16BF16 = LLT::fixed_vector(16, 16);
  const LLT V16FP32 = LLT::fixed_vector(16, 32);
  const LLT ACC512 = LLT::fixed_vector(8, 64);

  Register NewSrcReg1 = MIRBuilder.buildFPExt(S32, SrcReg1).getReg(0);
  Register NewSrcReg2 = MIRBuilder.buildFPExt(S32, SrcReg2).getReg(0);
  Register IdxReg = MIRBuilder.buildConstant(S32, 0).getReg(0);
  Register Src1Vec = MIRBuilder.buildUndef(V16FP32).getReg(0);
  Register Src2Vec = MIRBuilder.buildUndef(V16FP32).getReg(0);

  Register NewSrc1 = MIRBuilder
                         .buildInstr(AIE2::G_AIE_INSERT_VECTOR_ELT, {V16FP32},
                                     {Src1Vec, NewSrcReg1, IdxReg})
                         .getReg(0);
  Register NewSrc2 = MIRBuilder
                         .buildInstr(AIE2::G_AIE_INSERT_VECTOR_ELT, {V16FP32},
                                     {Src2Vec, NewSrcReg2, IdxReg})
                         .getReg(0);

  Register FPOp;
  if (MI.getOpcode() == TargetOpcode::G_FADD)
    FPOp = MIRBuilder.buildFAdd(V16FP32, NewSrc1, NewSrc2).getReg(0);
  else
    FPOp = MIRBuilder.buildFSub(V16FP32, NewSrc1, NewSrc2).getReg(0);

  Register FPRes = MIRBuilder.buildBitcast(ACC512, FPOp).getReg(0);
  Register Conv = MIRBuilder
                      .buildIntrinsic(Intrinsic::aie2_v16accfloat_to_v16bf16,
                                      {V16BF16}, true, false)
                      .addUse(FPRes)
                      .getReg(0);

  const Register ExtEltDstReg = MRI.createGenericVirtualRegister(S32);
  const Register ExtDstReg = MRI.createGenericVirtualRegister(S32);
  MIRBuilder.buildInstr(AIE2::G_AIE_SEXT_EXTRACT_VECTOR_ELT, {ExtEltDstReg},
                        {Conv, IdxReg});
  MIRBuilder.buildAssertInstr(TargetOpcode::G_ASSERT_SEXT, ExtDstReg,
                              ExtEltDstReg, 16);
  MIRBuilder.buildTrunc(DstReg, ExtDstReg);

  MI.eraseFromParent();
  return true;
}

bool AIELegalizerInfo::legalizeIntrinsic(LegalizerHelper &Helper,
                                         MachineInstr &MI) const {

  // The loop_decrement is a bit of an exception in legalization since it
  // is an architecture-neutral intrinsic to implement hardware loops, not a
  // dedicated AIE intrinsic. As such it carries a boolean, which should be
  // legalized to a 32 bit integer type.
  switch (cast<GIntrinsic>(MI).getIntrinsicID()) {
  case Intrinsic::loop_decrement: {
    assert(MI.getOpcode() == TargetOpcode::G_INTRINSIC_W_SIDE_EFFECTS);

    MachineIRBuilder &MIRBuilder = Helper.MIRBuilder;
    // Insert after our instruction
    MIRBuilder.setInsertPt(*MI.getParent(), ++MI.getIterator());

    Register OrigDst = MI.getOperand(0).getReg();
    Register NewDst =
        MIRBuilder.getMRI()->createGenericVirtualRegister(LLT::scalar(32));
    // NOTE: we don't inform the observer about this change as we do not want to
    // revisit this instruction
    MI.getOperand(0).setReg(NewDst);
    Register ZExtValueReg =
        MIRBuilder.buildAssertZExt(LLT::scalar(32), NewDst, 1).getReg(0);
    MIRBuilder.buildTrunc(OrigDst, ZExtValueReg);
    return true;
  }
  }

  return true;
}
