//===- AIELegalizerInfo.cpp -----------------------------------------------===//
//
// This file is licensed under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
// (c) Copyright 2023-2024 Advanced Micro Devices, Inc. or its affiliates
//
//===----------------------------------------------------------------------===//
/// \file
/// This file implements the targeting of the Machinelegalizer class for AIE.
/// \todo This should be generated by TableGen.
//===----------------------------------------------------------------------===//

#include "AIELegalizerInfo.h"
#include "AIE2TargetMachine.h"
#include "AIEBaseISelLowering.h"
#include "AIEBaseRegisterInfo.h"
#include "AIEMachineFunctionInfo.h"
#include "AIETargetMachine.h"
#include "MCTargetDesc/AIEMCTargetDesc.h"
#include "llvm/CodeGen/GlobalISel/CallLowering.h"
#include "llvm/CodeGen/GlobalISel/LegalizerHelper.h"
#include "llvm/CodeGen/GlobalISel/LegalizerInfo.h"
#include "llvm/CodeGen/GlobalISel/LostDebugLocObserver.h"
#include "llvm/CodeGen/GlobalISel/MachineIRBuilder.h"
#include "llvm/CodeGen/LowLevelType.h"
#include "llvm/CodeGen/MachineMemOperand.h"
#include "llvm/CodeGen/RuntimeLibcalls.h"
#include "llvm/CodeGen/TargetOpcodes.h"
#include "llvm/IR/IntrinsicsAIE.h"
#include "llvm/IR/IntrinsicsAIE2.h"
#include "llvm/Support/Alignment.h"

using namespace llvm;
using namespace LegalityPredicates;

static LegalityPredicate isLegalBitCastType(unsigned TypeIdx) {
  return [=](const LegalityQuery &Query) {
    LLT Ty = Query.Types[TypeIdx];
    if (Ty.isScalar())
      return Ty == LLT::scalar(32) || Ty == LLT::scalar(64);
    const int EltSize = Ty.isVector() ? Ty.getElementType().getSizeInBits() : 0;
    return EltSize == 8 || EltSize == 16 || EltSize == 32 || EltSize == 64;
  };
}

static LegalizeMutation bitcastAccToVectorType(unsigned TypeIdx) {
  return [=](const LegalityQuery &Query) {
    LLT OrigTy = Query.Types[TypeIdx];
    assert(OrigTy.getElementType() == LLT::scalar(64) &&
           "Expected an accumulator type");
    unsigned Size = OrigTy.getSizeInBits();
    assert(Size % 32 == 0);
    return std::pair(TypeIdx, LLT::fixed_vector(Size / 32, 32));
  };
}

AIELegalizerInfo::AIELegalizerInfo(const AIEBaseSubtarget &ST) {
  using namespace TargetOpcode;
  const LLT S8 = LLT::scalar(8);
  const LLT S16 = LLT::scalar(16);
  const LLT S20 = LLT::scalar(20);
  const LLT S32 = LLT::scalar(32);
  const LLT S64 = LLT::scalar(64);
  const LLT P0 = LLT::pointer(0, 20);

  // 64-bit vectors
  const LLT V2S32 = LLT::fixed_vector(2, 32);

  // 128-bit vectors
  const LLT V16S8 = LLT::fixed_vector(16, 8);
  const LLT V8S16 = LLT::fixed_vector(8, 16);
  const LLT V4S32 = LLT::fixed_vector(4, 32);

  // 256-bit vectors
  const LLT V8S32 = LLT::fixed_vector(8, 32);
  const LLT V16S16 = LLT::fixed_vector(16, 16);
  const LLT V32S8 = LLT::fixed_vector(32, 8);

  // 256-bit accumulators
  const LLT ACC256 = LLT::fixed_vector(4, 64);

  // 512-bit vectors
  const LLT V16S32 = LLT::fixed_vector(16, 32);
  const LLT V32S16 = LLT::fixed_vector(32, 16);
  const LLT V64S8 = LLT::fixed_vector(64, 8);

  // 512-bit accumulators
  const LLT ACC512 = LLT::fixed_vector(8, 64);

  // 1024-bit vectors
  const LLT V32S32 = LLT::fixed_vector(32, 32);
  const LLT V64S16 = LLT::fixed_vector(64, 16);
  const LLT V128S8 = LLT::fixed_vector(128, 8);

  // 1024-bit accumulators
  const LLT ACC1024 = LLT::fixed_vector(16, 64);

  static const std::initializer_list<LLT> AIE2VectorTypes = {
      /* Begin 256-bit types */
      V8S32, V16S16, V32S8,
      /* Begin 512-bit types */
      V16S32, V32S16, V64S8,
      /* Begin 1024-bit types */
      V32S32, V64S16, V128S8};

  static const std::initializer_list<LLT> AIE2AccumulatorTypes = {
      /* Begin 256-bit types */
      ACC256,
      /* Begin 512-bit types */
      ACC512,
      /* Begin 1024-bit types */
      ACC1024};

  auto &IMPLICIT = getActionDefinitionsBuilder({G_IMPLICIT_DEF, G_FREEZE})
                       .legalFor({S20, S32, P0});
  if (ST.isAIE2()) {
    IMPLICIT.legalFor(AIE2VectorTypes);
    IMPLICIT.legalFor(AIE2AccumulatorTypes);
  }

  IMPLICIT.widenScalarToNextPow2(0).clampScalar(0, S32, S32);

  getActionDefinitionsBuilder(G_CONSTANT)
      .legalFor({S20, S32, P0})
      .widenScalarToNextPow2(0)
      .clampScalar(0, S32, S32);

  // FIXME: AIE1 actually supports float. But since AIE2 is using the same
  // legalizer, we will cast both type to int now.
  getActionDefinitionsBuilder(G_FCONSTANT).customFor({S16, S32, S64});

  getActionDefinitionsBuilder(G_ICMP)
      .legalFor({{S32, S32}, {S32, P0}})
      .clampScalar(0, S32, S32)
      .clampScalar(1, S32, S32);

  if (ST.isAIE2()) {
    getActionDefinitionsBuilder(G_FCMP)
        .customFor({S32, S32})
        .clampScalar(0, S32, S32)
        .clampScalar(1, S32, S32);

    getActionDefinitionsBuilder(G_FPTRUNC)
        .libcallFor({{S32, S64}})
        .customFor({{S16, S32}});

    getActionDefinitionsBuilder({G_SITOFP, G_UITOFP})
        .libcallForCartesianProduct({S32, S64})
        .clampScalar(1, S32, S64)
        .widenScalarToNextPow2(1)
        .clampScalar(0, S32, S64);

    getActionDefinitionsBuilder(G_FPEXT)
        .libcallFor({{S64, S32}})
        .customFor({{S32, S16}});

    getActionDefinitionsBuilder({G_FPTOSI, G_FPTOUI})
        .libcallForCartesianProduct({S32, S64})
        .clampScalar(0, S32, S64)
        .widenScalarToNextPow2(0)
        .clampScalar(1, S32, S64);
  }

  // Since the only integers smaller than 32 bits we produce are S20 (from
  // G_PTRTOINT), the only legal extension is S20 -> S32.
  // Extensions to types larger than 64 bits have to be broken down into
  // multiple parts.
  getActionDefinitionsBuilder({G_SEXT, G_ZEXT})
      .legalFor({{S32, S20}})
      .clampScalar(0, S32, S32);
  // FIXME: (s|z|any)ext s20 to s64 is broken.

  auto &ANYEXT = getActionDefinitionsBuilder(G_ANYEXT)
                     .legalFor({{S32, S20}})
                     .clampScalar(0, S32, S32);

  if (ST.isAIE2())
    ANYEXT.legalFor({{S32, S16}});

  auto &VANDOR = getActionDefinitionsBuilder({G_AND, G_OR})
                     .legalFor({S32})
                     .widenScalarToNextPow2(0)
                     .clampScalar(0, S32, S32);

  if (ST.isAIE2())
    VANDOR.legalFor(AIE2VectorTypes);

  getActionDefinitionsBuilder(G_XOR)
      .legalFor({S32})
      .widenScalarToNextPow2(0)
      .clampScalar(0, S32, S32);

  getActionDefinitionsBuilder(G_SEXT_INREG)
      .legalForTypeWithAnyImm({S32})
      .lower();

  getActionDefinitionsBuilder({G_ASHR, G_LSHR, G_SHL})
      .legalFor({{S32, S32}})
      .widenScalarToNextPow2(0)
      .clampScalar(0, S32, S32)
      .clampScalar(1, S32, S32);

  getActionDefinitionsBuilder(G_TRUNC).alwaysLegal();

  getActionDefinitionsBuilder({G_FMUL, G_FDIV, G_FADD, G_FSUB, G_FREM})
      .libcallFor({S32, S64});

  auto &SELECT = getActionDefinitionsBuilder(G_SELECT)
                     .legalFor({{S32, S32}, {P0, S32}})
                     .widenScalarToNextPow2(0)
                     .clampScalar(0, S32, S32)
                     .clampScalar(1, S32, S32);

  if (ST.isAIE2()) {
    SELECT.legalFor(AIE2VectorTypes);

    // We support G_SELECT only on the vector register bank
    // Mapping the G_SELECT operands to the vector register bank
    // during register bank selection introduces the proper cross-bank
    // copies. However, we cannot write ISEL patterns expressing accumulator
    // types on vector register banks, which requires to duplicate the vector
    // type patterns in C++. Introducing bitcasts during legalization allows to
    // re-use the existing code for register bank selection and ISEL patterns.
    SELECT.bitcastIf(typeInSet(0, AIE2AccumulatorTypes),
                     bitcastAccToVectorType(0));
  }
  auto &ADDSUB = getActionDefinitionsBuilder({G_ADD, G_SUB})
                     .legalFor({S32})
                     .widenScalarToNextPow2(0)
                     .clampScalar(0, S32, S32);

  if (ST.isAIE2())
    ADDSUB.legalFor({V16S32, V32S16, V64S8});

  // FIXME: G_SADDE/G_SSUBE doesn't support lowering. To support this properly,
  // the action needs to be implemented
  // FIXME: AIE2 has ADC and SBC operations to read the carry.
  getActionDefinitionsBuilder(
      {G_UADDO, G_USUBO, G_UADDE, G_USUBE, G_SADDO, G_SSUBO, G_SADDE, G_SSUBE})
      .lower();

  getActionDefinitionsBuilder(G_MUL)
      .legalFor({S32})
      .widenScalarToNextPow2(0)
      .minScalar(0, S32)
      .libcallFor({S64});

  // FIXME: G_SMULO, G_UMULO support
  getActionDefinitionsBuilder({G_UMULH, G_SMULH}).lower();

  getActionDefinitionsBuilder({G_SDIV, G_UDIV, G_SREM, G_UREM})
      .libcallFor({S32, S64})
      .widenScalarToNextPow2(0)
      .clampScalar(0, S32, S64);

  getActionDefinitionsBuilder({G_SDIVREM, G_UDIVREM})
      .lowerFor({S32, S64})
      .widenScalarToNextPow2(0)
      .clampScalar(0, S32, S64);

  getActionDefinitionsBuilder(G_ABS)
      .legalFor({S32})
      .lowerFor({S64})
      .widenScalarToNextPow2(0)
      .clampScalar(0, S32, S32);

  // The CLZ instruction implements CTLZ, which also covers CTLZ_ZERO_UNDEF
  getActionDefinitionsBuilder(G_CTLZ_ZERO_UNDEF)
      .lowerFor({{S32, S32}})
      .clampScalar(0, S32, S32)
      .clampScalar(1, S32, S32);

  getActionDefinitionsBuilder(G_CTLZ)
      .legalFor({{S32, S32}})
      .clampScalar(0, S32, S32)
      .clampScalar(1, S32, S32);

  getActionDefinitionsBuilder({G_FSHL, G_FSHR}).lower();

  getActionDefinitionsBuilder({G_MEMCPY, G_MEMSET, G_MEMMOVE})
      .customIf([=](const LegalityQuery &Query) {
        const LLT SizeArg = Query.Types[2];
        return SizeArg == S20;
      })
      .libcall();

  getActionDefinitionsBuilder(G_DYN_STACKALLOC).custom();

  getActionDefinitionsBuilder({G_SMIN, G_SMAX, G_UMIN, G_UMAX})
      .widenScalarToNextPow2(0, 32)
      .lower();

  getActionDefinitionsBuilder({G_FRAME_INDEX, G_GLOBAL_VALUE}).legalFor({P0});

  getActionDefinitionsBuilder(G_INTTOPTR)
      .legalFor({{P0, S20}})
      .widenScalarToNextPow2(1)
      .clampScalar(1, S20, S20);

  getActionDefinitionsBuilder(G_PTRTOINT)
      .legalFor({{S20, P0}})
      .widenScalarToNextPow2(0)
      .clampScalar(0, S20, S20);

  // We support pointer arithmetic on both GPRs (32-bits) and pointer regs
  // (20-bits, where the scalar addend resides in a MOD register). To allow
  // specifying alternative register bank mappings, we need to truncate the RHS
  // operand to 20-bits, thus we only allow s20 types for the scalar addend
  getActionDefinitionsBuilder(G_PTR_ADD)
      .legalFor({{P0, S20}})
      .widenScalarToNextPow2(1)
      .clampScalar(1, S20, S20);

  getActionDefinitionsBuilder({G_LOAD, G_STORE})
      .legalForTypesWithMemDesc({
          {S32, P0, S8, 8},         {S32, P0, S16, 16},
          {S20, P0, S20, 32},       {S32, P0, S32, 32},
          {P0, P0, S20, 32},        {V16S8, P0, V16S8, 16},
          {V8S16, P0, V8S16, 16},   {V4S32, P0, V4S32, 16},
          {V8S32, P0, V8S32, 32},   {V16S16, P0, V16S16, 32},
          {V32S8, P0, V32S8, 32},   {V16S32, P0, V16S32, 32},
          {V32S16, P0, V32S16, 32}, {V64S8, P0, V64S8, 32},
          {V32S32, P0, V32S32, 32}, {V64S16, P0, V64S16, 32},
          {V128S8, P0, V128S8, 32}, {ACC256, P0, ACC256, 32},
          {ACC512, P0, ACC512, 32}, {ACC1024, P0, ACC1024, 32},
      })
      .widenScalarToNextPow2(0)
      .lowerIfMemSizeNotPow2()
      .clampScalar(0, S32, S32)
      .lower();

  // FIXME: Storing a pointer to an un-aligned address isn't supported.
  getActionDefinitionsBuilder({G_ZEXTLOAD, G_SEXTLOAD})
      .legalForTypesWithMemDesc({{S32, P0, S8, 8}, {S32, P0, S16, 16}})
      .widenScalarToNextPow2(0)
      .lowerIfMemSizeNotPow2()
      .clampScalar(0, S32, S32)
      .lower();

  if (ST.isAIE2()) {
    getActionDefinitionsBuilder(G_EXTRACT_VECTOR_ELT)
        .unsupportedIf([=](const LegalityQuery &Query) {
          const LLT &EltTy = Query.Types[1].getElementType();
          return Query.Types[0] != EltTy;
        })
        .customIf(typeInSet(1, {V2S32, V8S32, V16S32, V32S32, V16S16, V32S8,
                                V32S16, V64S8, V64S16, V128S8}));

    getActionDefinitionsBuilder(G_INSERT_VECTOR_ELT)
        .clampScalar(2, S32, S32) // Clamp the idx to 32 bit since VINSERT
                                  // relies on eR29 only for idx.
        .customIf(typeInSet(0, {V2S32, V8S32, V16S16, V32S8, V16S32, V32S16,
                                V64S8, V32S32, V64S16, V128S8}));
  }

  // Control-flow
  getActionDefinitionsBuilder(G_BRCOND).legalFor({S32}).clampScalar(0, S32,
                                                                    S32);

  auto &PHI = getActionDefinitionsBuilder(G_PHI).legalFor({S20, S32, P0});

  if (ST.isAIE2()) {
    PHI.legalFor(AIE2VectorTypes);
    PHI.legalFor(AIE2AccumulatorTypes);
  }

  PHI.widenScalarToNextPow2(0).clampScalar(0, S32, S32);

  // Bitcast - vector source and vector destination - For AIEV2
  if (ST.isAIE2()) {
    getActionDefinitionsBuilder(G_BITCAST).legalIf(
        LegalityPredicates::all(isLegalBitCastType(0), isLegalBitCastType(1)));

    getActionDefinitionsBuilder(G_MERGE_VALUES).legalFor({{S64, S32}});
    getActionDefinitionsBuilder(G_BUILD_VECTOR).legalFor({{V2S32, S32}});
    getActionDefinitionsBuilder(G_UNMERGE_VALUES)
        .legalFor({{S32, S64}, {S32, V2S32}});
  }

  getActionDefinitionsBuilder(G_JUMP_TABLE).custom();

  getActionDefinitionsBuilder(G_BRJT).custom();

  getActionDefinitionsBuilder(G_BRINDIRECT).legalFor({P0});

  // Variadic functions
  getActionDefinitionsBuilder(G_VASTART).custom();
  getActionDefinitionsBuilder(G_VAARG).custom();

  getLegacyLegalizerInfo().computeTables();
  verify(*ST.getInstrInfo());
}

bool AIELegalizerInfo::legalizeCustom(LegalizerHelper &Helper,
                                      MachineInstr &MI) const {
  switch (MI.getOpcode()) {
  default:
    break;
  case TargetOpcode::G_VASTART:
    return legalizeG_VASTART(Helper, MI);
  case TargetOpcode::G_VAARG:
    return legalizeG_VAARG(Helper, MI);
  case TargetOpcode::G_MEMSET:
  case TargetOpcode::G_MEMCPY:
  case TargetOpcode::G_MEMMOVE:
    return legalizeMemCalls(Helper, MI);
  case TargetOpcode::G_BRJT:
    return legalizeG_BRJT(Helper, MI);
  case TargetOpcode::G_FCONSTANT:
    return legalizeG_FCONSTANT(Helper, MI);
  case TargetOpcode::G_JUMP_TABLE:
    return legalizeG_JUMP_TABLE(Helper, MI);
  case TargetOpcode::G_DYN_STACKALLOC:
    return legalizeG_DYN_STACKALLOC(Helper, MI);
  case TargetOpcode::G_EXTRACT_VECTOR_ELT:
    return legalizeG_EXTRACT_VECTOR_ELT(Helper, MI);
  case TargetOpcode::G_INSERT_VECTOR_ELT:
    return legalizeG_INSERT_VECTOR_ELT(Helper, MI);
  case TargetOpcode::G_FCMP:
    return legalizeG_FCMP(Helper, MI);
  case TargetOpcode::G_FPTRUNC:
    return legalizeG_FPTRUNC(Helper, MI);
  case TargetOpcode::G_FPEXT:
    return legalizeG_FPEXT(Helper, MI);
  }

  llvm_unreachable("Un-expected custom legalization");
}

bool AIELegalizerInfo::legalizeG_VASTART(LegalizerHelper &Helper,
                                         MachineInstr &MI) const {
  MachineIRBuilder &MIRBuilder = Helper.MIRBuilder;
  MachineFunction &MF = MIRBuilder.getMF();
  auto *FuncInfo = MF.getInfo<AIEMachineFunctionInfo>();
  Register ListPtr = MI.getOperand(0).getReg();
  LLT PtrTy = MIRBuilder.getMRI()->getType(ListPtr);
  const Align PtrAlign = AIEBaseTargetLowering::getStackArgumentAlignment();

  Register VAList =
      MIRBuilder.buildFrameIndex(PtrTy, FuncInfo->getVarArgsFrameIndex())
          .getReg(0);
  MIRBuilder.buildStore(VAList, ListPtr,
                        *MF.getMachineMemOperand(MachinePointerInfo(),
                                                 MachineMemOperand::MOStore,
                                                 PtrTy, PtrAlign));

  MI.eraseFromParent();
  return true;
}

bool AIELegalizerInfo::legalizeG_VAARG(LegalizerHelper &Helper,
                                       MachineInstr &MI) const {
  MachineIRBuilder &MIRBuilder = Helper.MIRBuilder;
  MachineFunction &MF = MIRBuilder.getMF();
  MachineRegisterInfo &MRI = *MIRBuilder.getMRI();

  Align Alignment(MI.getOperand(2).getImm());
  const Align PtrAlign = AIEBaseTargetLowering::getStackArgumentAlignment();
  assert((Alignment <= PtrAlign) &&
         "Vaarg alignment is greater than the one of stack slots.");

  // Retrieve the vararg list pointer.
  Register ListPtr = MI.getOperand(1).getReg();
  LLT PtrTy = MRI.getType(ListPtr);
  auto VAList = MIRBuilder.buildLoad(
      PtrTy, ListPtr,
      *MF.getMachineMemOperand(MachinePointerInfo(), MachineMemOperand::MOLoad,
                               PtrTy, PtrAlign));

  // Compute the size of the current vararg slot. That is smallest multiple of
  // PtrAlign which can fit the vararg type.
  Register Dst = MI.getOperand(0).getReg();
  LLT ValTy = MRI.getType(Dst);
  unsigned ArgSize = alignTo(ValTy.getSizeInBytes(), PtrAlign);

  // Compute the address of the current VAARG by subtracting its size
  // from the previous VAARG address.
  LLT IntTy = LLT::scalar(32);
  auto Offset = MIRBuilder.buildConstant(IntTy, -ArgSize);
  auto NewVAList = MIRBuilder.buildPtrAdd(PtrTy, VAList.getReg(0), Offset);

  // Actually load the vararg and feed it into Dst
  MIRBuilder.buildLoad(
      Dst, NewVAList,
      *MF.getMachineMemOperand(MachinePointerInfo(), MachineMemOperand::MOLoad,
                               ValTy, std::max(Alignment, PtrAlign)));

  // Then store the new vararg list pointer so it can be used for next G_VARARG.
  MIRBuilder.buildStore(NewVAList, ListPtr,
                        *MF.getMachineMemOperand(MachinePointerInfo(),
                                                 MachineMemOperand::MOStore,
                                                 PtrTy, PtrAlign));

  MI.eraseFromParent();
  return true;
}

bool AIELegalizerInfo::legalizeMemCalls(LegalizerHelper &Helper,
                                        MachineInstr &MI) const {
  MachineIRBuilder &MIRBuilder = Helper.MIRBuilder;
  LLVMContext &Ctx = MIRBuilder.getMF().getFunction().getContext();
  MachineRegisterInfo &MRI = *MIRBuilder.getMRI();
  Register ResultReg = MRI.createGenericVirtualRegister(LLT::pointer(0, 20));

  RTLIB::Libcall LibEntry = RTLIB::UNKNOWN_LIBCALL;
  PointerType *VoidPtrTy = PointerType::get(Ctx, 0);
  IntegerType *IntTy = Type::getInt32Ty(Ctx);
  LLT S32 = LLT::scalar(32);

  SmallVector<CallLowering::ArgInfo, 3> Args;
  Args.emplace_back(MI.getOperand(0).getReg(), VoidPtrTy, 0);
  MachineInstrBuilder ZextSize = MIRBuilder.buildZExt(S32, MI.getOperand(2));

  switch (MI.getOpcode()) {
  case TargetOpcode::G_MEMSET: {
    LibEntry = RTLIB::MEMSET;
    Register CharReg = MI.getOperand(1).getReg();
    LLT CharLLT = MRI.getType(CharReg);
    if (CharLLT != S32) {
      MachineInstrBuilder ZextChar =
          MIRBuilder.buildZExt(S32, MI.getOperand(1));
      CharReg = ZextChar->getOperand(0).getReg();
    }
    Args.emplace_back(CharReg, IntTy, 0);
    Args.emplace_back(ZextSize->getOperand(0).getReg(), IntTy, 0);
    break;
  }
  case TargetOpcode::G_MEMCPY:
    LibEntry = RTLIB::MEMCPY;
    Args.emplace_back(MI.getOperand(1).getReg(), VoidPtrTy, 0);
    Args.emplace_back(ZextSize->getOperand(0).getReg(), IntTy, 0);
    break;
  case TargetOpcode::G_MEMMOVE:
    LibEntry = RTLIB::MEMMOVE;
    Args.emplace_back(MI.getOperand(1).getReg(), VoidPtrTy, 0);
    Args.emplace_back(ZextSize->getOperand(0).getReg(), IntTy, 0);
    break;
  default:
    return false;
  }

  auto Status =
      createLibcall(MIRBuilder, LibEntry, {ResultReg, VoidPtrTy, 0}, Args);
  if (Status != LegalizerHelper::Legalized) {
    return false;
  }

  MI.eraseFromParent();
  return true;
}

bool AIELegalizerInfo::legalizeG_BRJT(LegalizerHelper &Helper,
                                      MachineInstr &MI) const {
  MachineIRBuilder &MIRBuilder = Helper.MIRBuilder;
  MachineFunction &MF = MIRBuilder.getMF();
  LLT S32 = LLT::scalar(32);
  LLT P0 = LLT::pointer(0, 20);
  unsigned EntrySize = MF.getJumpTableInfo()->getEntrySize(MF.getDataLayout());

  auto CopyIndexTo32 = MIRBuilder.buildZExt(S32, MI.getOperand(2));
  auto ConstantShift = MIRBuilder.buildConstant(S32, Log2_32(EntrySize));
  auto LShift = MIRBuilder.buildShl(S32, CopyIndexTo32->getOperand(0),
                                    ConstantShift->getOperand(0));
  auto PtrAdd =
      MIRBuilder.buildPtrAdd(P0, MI.getOperand(0), LShift->getOperand(0));
  auto *MMO = MF.getMachineMemOperand(
      MachinePointerInfo(), MachineMemOperand::MOLoad, P0,
      Align(MF.getJumpTableInfo()->getEntryAlignment(MF.getDataLayout())));
  auto LoadAddress = MIRBuilder.buildLoad(P0, PtrAdd->getOperand(0), *MMO);
  MIRBuilder.buildBrIndirect(LoadAddress->getOperand(0).getReg());

  MI.eraseFromParent();
  return true;
}

bool AIELegalizerInfo::legalizeG_FCONSTANT(LegalizerHelper &Helper,
                                           MachineInstr &MI) const {
  MachineIRBuilder &MIRBuilder = Helper.MIRBuilder;
  LLVMContext &Ctx = MIRBuilder.getMF().getFunction().getContext();

  // Convert to integer constants, while preserving the binary representation.
  auto AsInteger = MI.getOperand(1).getFPImm()->getValueAPF().bitcastToAPInt();
  MIRBuilder.buildConstant(MI.getOperand(0), *ConstantInt::get(Ctx, AsInteger));

  MI.eraseFromParent();
  return true;
}

bool AIELegalizerInfo::legalizeG_JUMP_TABLE(LegalizerHelper &Helper,
                                            MachineInstr &MI) const {
  Helper.Observer.changingInstr(MI);
  MachineIRBuilder &MIRBuilder = Helper.MIRBuilder;
  MI.setDesc(MIRBuilder.getTII().get(TargetOpcode::G_GLOBAL_VALUE));
  Helper.Observer.changedInstr(MI);
  return true;
}

bool AIELegalizerInfo::legalizeG_DYN_STACKALLOC(LegalizerHelper &Helper,
                                                MachineInstr &MI) const {
  MachineIRBuilder &MIRBuilder = Helper.MIRBuilder;
  const LLT P0 = LLT::pointer(0, 20);
  Register Dst = MI.getOperand(0).getReg();
  auto Size = MI.getOperand(1);
  Register SPReg =
      Helper.getTargetLowering().getStackPointerRegisterToSaveRestore();

  auto SPTmp = MIRBuilder.buildCopy(P0, SPReg);
  MIRBuilder.buildCopy(Dst, SPTmp);
  SPTmp = MIRBuilder.buildPtrAdd(P0, SPTmp, Size);
  MIRBuilder.buildCopy(SPReg, SPTmp);

  MI.removeFromParent();
  return true;
}

//%2:_(s8) = G_EXTRACT_VECTOR_ELT %0, %1
//==>
//%3:_(s32) = G_AIE_SEXT_EXTRACT_VECTOR_ELT %0, %1
//%4:_(s32) = G_ASSERT_SEXT %3, Elt_Size
//%2:_(s8) = G_TRUNC %4
bool AIELegalizerInfo::legalizeG_EXTRACT_VECTOR_ELT(LegalizerHelper &Helper,
                                                    MachineInstr &MI) const {
  MachineIRBuilder &MIRBuilder = Helper.MIRBuilder;
  MachineRegisterInfo &MRI = *MIRBuilder.getMRI();
  const Register DstReg = MI.getOperand(0).getReg();
  const Register SrcVecReg = MI.getOperand(1).getReg();
  const Register IdxReg = MI.getOperand(2).getReg();
  const LLT SrcVecTy = MRI.getType(SrcVecReg);
  const unsigned SrcVecSize = SrcVecTy.getSizeInBits();
  const LLT SrcVecEltTy = SrcVecTy.getElementType();
  assert(SrcVecEltTy == MRI.getType(DstReg));

  const LLT S32 = LLT::scalar(32);
  switch (SrcVecSize) {
  case 64: {
    assert(SrcVecTy == LLT::fixed_vector(2, 32) && "Unexpected 64bit vector!");
    const Register Reg0 = MRI.createGenericVirtualRegister(S32);
    const Register Reg1 = MRI.createGenericVirtualRegister(S32);
    MIRBuilder.buildUnmerge({Reg0, Reg1}, SrcVecReg);

    auto IdxVal = getIConstantVRegValWithLookThrough(IdxReg, MRI);
    if (!IdxVal)
      MIRBuilder.buildSelect(DstReg, IdxReg, Reg1, Reg0);
    else {
      const unsigned LaneIdx = IdxVal->Value.getZExtValue();
      if (LaneIdx)
        MIRBuilder.buildCopy(DstReg, Reg1);
      else
        MIRBuilder.buildCopy(DstReg, Reg0);
    }
    break;
  }
  case 256:
  case 512:
  case 1024: {
    const LLT S8 = LLT::scalar(8);
    const LLT S16 = LLT::scalar(16);
    bool IsS32 = SrcVecEltTy == S32;
    assert((SrcVecEltTy == S8 || SrcVecEltTy == S16 || IsS32) &&
           "Unexpected vector element type for extract vector elt!");
    if (!IsS32) {
      const Register ExtEltDstReg = MRI.createGenericVirtualRegister(S32);
      const Register ExtDstReg = MRI.createGenericVirtualRegister(S32);
      MIRBuilder.buildInstr(AIE2::G_AIE_SEXT_EXTRACT_VECTOR_ELT, {ExtEltDstReg},
                            {SrcVecReg, IdxReg});
      MIRBuilder.buildAssertInstr(TargetOpcode::G_ASSERT_SEXT, ExtDstReg,
                                  ExtEltDstReg, SrcVecEltTy.getSizeInBits());
      MIRBuilder.buildTrunc(DstReg, ExtDstReg);
    } else {
      MIRBuilder.buildInstr(AIE2::G_AIE_SEXT_EXTRACT_VECTOR_ELT, {DstReg},
                            {SrcVecReg, IdxReg});
    }
    break;
  }
  default:
    llvm_unreachable("Unexpected vector size for extract vector elt!");
  }
  MI.removeFromParent();
  return true;
}

bool AIELegalizerInfo::legalizeG_INSERT_VECTOR_ELT(LegalizerHelper &Helper,
                                                   MachineInstr &MI) const {
  MachineIRBuilder &MIRBuilder = Helper.MIRBuilder;
  MachineRegisterInfo &MRI = *MIRBuilder.getMRI();
  const Register DstVecReg = MI.getOperand(0).getReg();
  const Register SrcVecReg = MI.getOperand(1).getReg();
  const Register ValReg = MI.getOperand(2).getReg();
  const Register IdxReg = MI.getOperand(3).getReg();
  const LLT DstVecTy = MRI.getType(DstVecReg);
  const unsigned DstVecSize = DstVecTy.getSizeInBits();
  const LLT S32 = LLT::scalar(32);

  switch (DstVecSize) {
  case 64: {
    if (DstVecTy != LLT::fixed_vector(2, 32)) {
      llvm_unreachable("Unexpected 64-bit vector type!");
    }
    std::array<Register, 2> Regs = {MRI.createGenericVirtualRegister(S32),
                                    MRI.createGenericVirtualRegister(S32)};
    MIRBuilder.buildUnmerge(Regs, SrcVecReg);
    auto IdxVal = getIConstantVRegValWithLookThrough(IdxReg, MRI);
    if (IdxVal) {
      unsigned Idx = IdxVal->Value.getZExtValue();
      assert(Idx < Regs.size() && "Expected idx to be 0 or 1.");
      Regs[Idx] = ValReg;
      MIRBuilder.buildBuildVector(DstVecReg, Regs);
    } else {
      std::array<Register, 2> TmpSelDsts = {
          MRI.createGenericVirtualRegister(S32),
          MRI.createGenericVirtualRegister(S32)};
      MIRBuilder.buildSelect(TmpSelDsts[0], IdxReg, Regs[0], ValReg);
      MIRBuilder.buildSelect(TmpSelDsts[1], IdxReg, ValReg, Regs[1]);
      MIRBuilder.buildBuildVector(DstVecReg, TmpSelDsts);
    }
    break;
  }
  case 256:
  case 512:
  case 1024: {
    const LLT ValTy = MRI.getType(ValReg);
    if (ValTy == LLT::scalar(64)) {
      llvm_unreachable("Unexpected scalar value type for insert vec elt!");
    }
    Register NewValReg;
    if (ValTy == LLT::scalar(8) || ValTy == LLT::scalar(16)) {
      NewValReg = MRI.createGenericVirtualRegister(S32);
      MIRBuilder.buildAnyExt(NewValReg, ValReg);
    } else {
      NewValReg = ValReg;
    }
    MIRBuilder.buildInstr(AIE2::G_AIE_INSERT_VECTOR_ELT, {DstVecReg},
                          {SrcVecReg, NewValReg, IdxReg});
    break;
  }
  default:
    llvm_unreachable("Unexpected vector size for insert vector elt!");
  }
  MI.removeFromParent();
  return true;
}

static RTLIB::Libcall getFCmpLibCall(CmpInst::Predicate Predicate,
                                     CmpInst::Predicate &IPredicate) {
  switch (Predicate) {
  default:
    llvm_unreachable("Unsupported FCmp predicate");
  case CmpInst::FCMP_OEQ:
    IPredicate = CmpInst::ICMP_EQ;
    return RTLIB::OEQ_F32;
  case CmpInst::FCMP_OGE:
    IPredicate = CmpInst::ICMP_SGE;
    return RTLIB::OGE_F32;
  case CmpInst::FCMP_OGT:
    IPredicate = CmpInst::ICMP_SGT;
    return RTLIB::OGT_F32;
  case CmpInst::FCMP_OLE:
    IPredicate = CmpInst::ICMP_SLE;
    return RTLIB::OLE_F32;
  case CmpInst::FCMP_OLT:
    IPredicate = CmpInst::ICMP_SLT;
    return RTLIB::OLT_F32;
  case CmpInst::FCMP_ORD:
    IPredicate = CmpInst::ICMP_EQ;
    return RTLIB::UO_F32;
  /* Unordered comparisons are built from
   * the complement of the ordered ones */
  case CmpInst::FCMP_UGE:
    IPredicate = CmpInst::ICMP_SGE;
    return RTLIB::OLT_F32;
  case CmpInst::FCMP_UGT:
    IPredicate = CmpInst::ICMP_SGT;
    return RTLIB::OLE_F32;
  case CmpInst::FCMP_ULE:
    IPredicate = CmpInst::ICMP_SLE;
    return RTLIB::OGT_F32;
  case CmpInst::FCMP_ULT:
    IPredicate = CmpInst::ICMP_SLT;
    return RTLIB::OGE_F32;
  case CmpInst::FCMP_UNE:
    IPredicate = CmpInst::ICMP_NE;
    return RTLIB::UNE_F32;
  case CmpInst::FCMP_UNO:
    IPredicate = CmpInst::ICMP_NE;
    return RTLIB::UO_F32;
  }
}

bool AIELegalizerInfo::legalizeG_FCMP(LegalizerHelper &Helper,
                                      MachineInstr &MI) const {
  MachineIRBuilder &MIRBuilder = Helper.MIRBuilder;
  LLVMContext &Ctx = MIRBuilder.getMF().getFunction().getContext();
  MachineRegisterInfo &MRI = *MIRBuilder.getMRI();
  assert(MRI.getType(MI.getOperand(2).getReg()) ==
             MRI.getType(MI.getOperand(3).getReg()) &&
         "Mismatched operands for G_FCMP");

  auto DstReg = MI.getOperand(0).getReg();
  auto FPredicate =
      static_cast<CmpInst::Predicate>(MI.getOperand(1).getPredicate());
  assert(CmpInst::isFPPredicate(FPredicate) && "Unsupported FCmp predicate");
  RTLIB::Libcall Libcall = RTLIB::UNKNOWN_LIBCALL;
  CmpInst::Predicate IPredicate = CmpInst::BAD_ICMP_PREDICATE;
  SmallVector<std::pair<RTLIB::Libcall, CmpInst::Predicate>, 2> Libcalls;
  switch (FPredicate) {
  case CmpInst::FCMP_TRUE:
  case CmpInst::FCMP_FALSE: {
    MIRBuilder.buildConstant(DstReg, FPredicate == CmpInst::FCMP_TRUE ? 1 : 0);
    MI.eraseFromParent();
    return true;
  }
  /* Compose ordered and unequal operands as follows:
   * a != b ==> a > b || a < b */
  case CmpInst::FCMP_ONE:
    Libcalls.push_back(std::make_pair(RTLIB::OGT_F32, CmpInst::ICMP_SGT));
    Libcalls.push_back(std::make_pair(RTLIB::OLT_F32, CmpInst::ICMP_SLT));
    break;
  /* Compose unordered or equal operands as follows:
   * (unord(a, b) or a == b) ==> a == b || a != b */
  case CmpInst::FCMP_UEQ:
    Libcalls.push_back(std::make_pair(RTLIB::OEQ_F32, CmpInst::ICMP_EQ));
    Libcalls.push_back(std::make_pair(RTLIB::UO_F32, CmpInst::ICMP_NE));
    break;
  default:
    Libcall = getFCmpLibCall(FPredicate, IPredicate);
    Libcalls.push_back(std::make_pair(Libcall, IPredicate));
    break;
  }
  auto *ArgTy = Type::getFloatTy(Ctx);
  auto *RetTy = Type::getInt32Ty(Ctx);

  SmallVector<Register, 2> Results;
  for (auto &[LibEntry, Predicate] : Libcalls) {
    auto LibcallResult = MRI.createGenericVirtualRegister(LLT::scalar(32));
    auto Status = createLibcall(MIRBuilder, LibEntry, {LibcallResult, RetTy, 0},
                                {{MI.getOperand(2).getReg(), ArgTy, 0},
                                 {MI.getOperand(3).getReg(), ArgTy, 0}});

    if (Status != LegalizerHelper::Legalized)
      return false;

    auto NewDstReg =
        Libcalls.size() == 1
            ? DstReg
            : MRI.createGenericVirtualRegister(MRI.getType(DstReg));

    CmpInst::Predicate IDestPred = Predicate;
    // Compare against 0. Example, a ole b is transformed to ole(a, b) <= 0
    assert(CmpInst::isIntPredicate(IDestPred) && "Expected Int Predicate");
    auto Zero = MIRBuilder.buildConstant(LLT::scalar(32), 0);
    MIRBuilder.buildICmp(IDestPred, NewDstReg, LibcallResult, Zero);
    Results.push_back(NewDstReg);
  }
  // OR the results when we have two libcalls
  if (Results.size() != 1) {
    assert(Results.size() == 2 && "Unexpected Number of Results");
    MIRBuilder.buildOr(DstReg, Results[0], Results[1]);
  }
  MI.eraseFromParent();
  return true;
}

bool AIELegalizerInfo::legalizeG_FPTRUNC(LegalizerHelper &Helper,
                                         MachineInstr &MI) const {
  MachineIRBuilder &MIRBuilder = Helper.MIRBuilder;
  MachineRegisterInfo &MRI = *MIRBuilder.getMRI();

  Register DstReg = MI.getOperand(0).getReg();
  Register SrcReg = MI.getOperand(1).getReg();
  LLT DstTy = MRI.getType(DstReg);
  LLT SrcTy = MRI.getType(SrcReg);

  // We only handle single precision to bfloat16 conversion
  if (DstTy != LLT::scalar(16) || SrcTy != LLT::scalar(32))
    return false;

  MIRBuilder.buildIntrinsic(Intrinsic::aie2_float_to_bfloat16, DstReg, false, false)
      .addUse(SrcReg);

  MI.eraseFromParent();
  return true;
}

bool AIELegalizerInfo::legalizeG_FPEXT(LegalizerHelper &Helper,
                                       MachineInstr &MI) const {
  MachineIRBuilder &MIRBuilder = Helper.MIRBuilder;
  MachineRegisterInfo &MRI = *MIRBuilder.getMRI();

  LLT S32 = LLT::scalar(32);

  Register DstReg = MI.getOperand(0).getReg();
  Register SrcReg = MI.getOperand(1).getReg();
  LLT DstTy = MRI.getType(DstReg);
  LLT SrcTy = MRI.getType(SrcReg);

  // We only handle bfloat16 to single precision conversion
  if (DstTy != LLT::scalar(32) || SrcTy != LLT::scalar(16))
    return false;

  Register AnyExt = MIRBuilder.buildAnyExt(S32, SrcReg).getReg(0);
  Register Cst = MIRBuilder.buildConstant(S32, 16).getReg(0);
  MIRBuilder.buildShl(DstReg, AnyExt, Cst);

  MI.eraseFromParent();
  return true;
}
