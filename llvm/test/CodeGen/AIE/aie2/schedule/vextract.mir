# NOTE: Assertions have been autogenerated by utils/update_mir_test_checks.py
#
# This file is licensed under the Apache License v2.0 with LLVM Exceptions.
# See https://llvm.org/LICENSE.txt for license information.
# SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
#
# (c) Copyright 2023-2024 Advanced Micro Devices, Inc. or its affiliates
# RUN: llc -march=aie2 -run-pass=postmisched %s -o - | FileCheck %s

# VEXTRACT
---
name:            bypass_signed
alignment:       16
body:             |
  bb.0.entry:
    ; CHECK-LABEL: name: bypass_signed
    ; CHECK: $x0 = VPUSH_LO_32 killed $r3, killed $x11
    ; CHECK-NEXT: $r0 = VEXTRACT_S8 $x0, $r16
    ; CHECK-NEXT: NOP
    ; CHECK-NEXT: $x1 = VPUSH_LO_32 killed $r0, killed $x0
    ; CHECK-NEXT: $r1 = VEXTRACT_S16 $x1, $r16
    ; CHECK-NEXT: NOP
    ; CHECK-NEXT: $x2 = VPUSH_LO_32 killed $r1, killed $x1
    ; CHECK-NEXT: $r2 = VEXTRACT_S32 $x2, $r16
    ; CHECK-NEXT: NOP
    ; CHECK-NEXT: $x3 = VPUSH_LO_32 killed $r2, killed $x2
    ; CHECK-NEXT: $l3 = VEXTRACT_S64 killed $x3, killed $r16
    ; CHECK-NEXT: NOP
    $x0 = VPUSH_LO_32  $r3,  $x11
    $r0 = VEXTRACT_S8  $x0,  $r16
    $x1 = VPUSH_LO_32  $r0,  $x0
    $r1 = VEXTRACT_S16 $x1,  $r16
    $x2 = VPUSH_LO_32  $r1,  $x1
    $r2 = VEXTRACT_S32 $x2,  $r16
    $x3 = VPUSH_LO_32  $r2,  $x2
    $l3 = VEXTRACT_S64 $x3,  $r16
...

---
name:            bypass_dynamic
alignment:       16
body:             |
  bb.0.entry:
    ; CHECK-LABEL: name: bypass_dynamic
    ; CHECK: $x0 = VPUSH_LO_32 killed $r3, killed $x11
    ; CHECK-NEXT: $r0 = VEXTRACT_D8 $x0, $r16, implicit $crvaddsign
    ; CHECK-NEXT: NOP
    ; CHECK-NEXT: $x1 = VPUSH_LO_32 killed $r0, killed $x0
    ; CHECK-NEXT: $r1 = VEXTRACT_D16 $x1, $r16, implicit $crvaddsign
    ; CHECK-NEXT: NOP
    ; CHECK-NEXT: $x2 = VPUSH_LO_32 killed $r1, killed $x1
    ; CHECK-NEXT: $r2 = VEXTRACT_D32 $x2, $r16, implicit $crvaddsign
    ; CHECK-NEXT: NOP
    ; CHECK-NEXT: $x3 = VPUSH_LO_32 killed $r2, killed $x2
    ; CHECK-NEXT: $l3 = VEXTRACT_D64 killed $x3, killed $r16, implicit $crvaddsign
    ; CHECK-NEXT: NOP
    $x0 = VPUSH_LO_32  $r3,  $x11
    $r0 = VEXTRACT_D8  $x0,  $r16, implicit $crvaddsign
    $x1 = VPUSH_LO_32  $r0,  $x0
    $r1 = VEXTRACT_D16 $x1,  $r16, implicit $crvaddsign
    $x2 = VPUSH_LO_32  $r1,  $x1
    $r2 = VEXTRACT_D32 $x2,  $r16, implicit $crvaddsign
    $x3 = VPUSH_LO_32  $r2,  $x2
    $l3 = VEXTRACT_D64 $x3,  $r16, implicit $crvaddsign
...

# Need to wait the full 7 cycles, no bypass between VLDA and VEXTRACT
---
name:            no_bypass_dynamic
alignment:       16
body:             |
  bb.0.entry:
    ; CHECK-LABEL: name: no_bypass_dynamic
    ; CHECK: $wl3 = VLDA_dmw_lda_w_ag_idx_imm killed $p2, 0
    ; CHECK-NEXT: NOP
    ; CHECK-NEXT: NOP
    ; CHECK-NEXT: NOP
    ; CHECK-NEXT: NOP
    ; CHECK-NEXT: NOP
    ; CHECK-NEXT: NOP
    ; CHECK-NEXT: $l1 = VEXTRACT_D64 $x3, $r16, implicit $crvaddsign
    ; CHECK-NEXT: $r2 = VEXTRACT_D32 $x3, $r16, implicit $crvaddsign
    ; CHECK-NEXT: $r3 = VEXTRACT_D16 $x3, $r16, implicit $crvaddsign
    ; CHECK-NEXT: $r4 = VEXTRACT_D8 killed $x3, killed $r16, implicit $crvaddsign
    ; CHECK-NEXT: NOP
    $wl3 = VLDA_dmw_lda_w_ag_idx_imm $p2, 0
    $l1 = VEXTRACT_D64 $x3, $r16, implicit $crvaddsign
    $r2 = VEXTRACT_D32 $x3, $r16, implicit $crvaddsign
    $r3 = VEXTRACT_D16 $x3, $r16, implicit $crvaddsign
    $r4 = VEXTRACT_D8  $x3, $r16, implicit $crvaddsign
...

---
name:            no_bypass_signed
alignment:       16
body:             |
  bb.0.entry:
    ; CHECK-LABEL: name: no_bypass_signed
    ; CHECK: $wl3 = VLDA_dmw_lda_w_ag_idx_imm killed $p2, 0
    ; CHECK-NEXT: NOP
    ; CHECK-NEXT: NOP
    ; CHECK-NEXT: NOP
    ; CHECK-NEXT: NOP
    ; CHECK-NEXT: NOP
    ; CHECK-NEXT: NOP
    ; CHECK-NEXT: $l1 = VEXTRACT_S64 $x3, $r16
    ; CHECK-NEXT: $r2 = VEXTRACT_S32 $x3, $r16
    ; CHECK-NEXT: $r3 = VEXTRACT_S16 $x3, $r16
    ; CHECK-NEXT: $r4 = VEXTRACT_S8 killed $x3, killed $r16
    ; CHECK-NEXT: NOP
    $wl3 = VLDA_dmw_lda_w_ag_idx_imm $p2, 0
    $l1 = VEXTRACT_S64 $x3, $r16
    $r2 = VEXTRACT_S32 $x3, $r16
    $r3 = VEXTRACT_S16 $x3, $r16
    $r4 = VEXTRACT_S8  $x3, $r16
...
