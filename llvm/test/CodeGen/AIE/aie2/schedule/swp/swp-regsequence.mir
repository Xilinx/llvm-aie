# NOTE: Assertions have been autogenerated by utils/update_mir_test_checks.py
#
# This file is licensed under the Apache License v2.0 with LLVM Exceptions.
# See https://llvm.org/LICENSE.txt for license information.
# SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
#
# (c) Copyright 2024 Advanced Micro Devices, Inc. or its affiliates

# RUN: llc --mtriple=aie2 --run-pass=pipeliner %s -o - \
# RUN: --aie-loop-min-tripcount=2 --aie-pipeliner-max-stagecount=2 --aie-postpipeliner-limit=1 | FileCheck %s

--- |

  define void @vmul_256(ptr noalias %out, ptr noalias readonly %in1, ptr noalias readonly %in2) {
  entry:
    ret void
  }
  define void @vmul_512(ptr noalias %out, ptr noalias readonly %in1, ptr noalias readonly %in2) {
  entry:
    ret void
  }
...

# Here we verify in which stage REG_SEQUENCE is placed after SW pipelining.
# This is mostly driven by the PropagateIncomingLatencies DAGMutator.

# When we only load 256 bit into a 512 bit vector, and the rest comes from an
# external source to fill the hi bits, we would like the REG_SEQUENCE to be
# placed close to consumer to avoid making the whole 512 bit register
# loop-carried and facilitate LICM for the constant part.
# When the whole 512 bit register is defined in the loop, we would like to keep
# the REG_SEQUENCE close to its sources to facilitate register coalescing and
# "tie" the sources together.

# We expect the REG_SEQUENCE for the load of %ir.in1 to be in the second stage, close to
# its VMUL consumer.
# FIXME: Make it happen
---
name:           vmul_256
alignment:       16
tracksRegLiveness: true
body:             |
  ; CHECK-LABEL: name: vmul_256
  ; CHECK: bb.0:
  ; CHECK-NEXT:   successors: %bb.3(0x80000000)
  ; CHECK-NEXT:   liveins: $p0, $p1, $p2, $r0, $r1, $s0
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:er = COPY $r0
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:er = COPY $r1
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:mss = COPY $s0
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:ep = COPY $p0
  ; CHECK-NEXT:   [[COPY4:%[0-9]+]]:ep = COPY $p1
  ; CHECK-NEXT:   [[COPY5:%[0-9]+]]:ep = COPY $p2
  ; CHECK-NEXT:   [[VBCST_32_:%[0-9]+]]:vec512 = VBCST_32 [[COPY]]
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.3:
  ; CHECK-NEXT:   successors: %bb.4(0x80000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[VLDA_dmw_lda_w_ag_pstm_nrm_imm:%[0-9]+]]:vec256, [[VLDA_dmw_lda_w_ag_pstm_nrm_imm1:%[0-9]+]]:ep = VLDA_dmw_lda_w_ag_pstm_nrm_imm [[COPY3]], 32 :: (load (<32 x s8>) from %ir.in1)
  ; CHECK-NEXT:   [[VLDA_dmw_lda_w_ag_pstm_nrm_imm2:%[0-9]+]]:vec256, [[VLDA_dmw_lda_w_ag_pstm_nrm_imm3:%[0-9]+]]:ep = VLDA_dmw_lda_w_ag_pstm_nrm_imm [[COPY4]], 32 :: (load (<32 x s8>) from %ir.in2)
  ; CHECK-NEXT:   [[REG_SEQUENCE:%[0-9]+]]:vec512 = REG_SEQUENCE [[VBCST_32_]].sub_256_lo, %subreg.sub_256_hi, [[VLDA_dmw_lda_w_ag_pstm_nrm_imm]], %subreg.sub_256_lo
  ; CHECK-NEXT:   [[REG_SEQUENCE1:%[0-9]+]]:vec512 = REG_SEQUENCE [[VLDA_dmw_lda_w_ag_pstm_nrm_imm2]], %subreg.sub_256_lo
  ; CHECK-NEXT:   [[ADD_add_r_ri:%[0-9]+]]:er = nsw ADD_add_r_ri [[COPY1]], -1, implicit-def $srcarry
  ; CHECK-NEXT:   PseudoJ_jump_imm %bb.4
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.4:
  ; CHECK-NEXT:   successors: %bb.4(0x40000000), %bb.5(0x40000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[PHI:%[0-9]+]]:er = PHI [[ADD_add_r_ri]], %bb.3, %30, %bb.4
  ; CHECK-NEXT:   [[PHI1:%[0-9]+]]:ep = PHI [[VLDA_dmw_lda_w_ag_pstm_nrm_imm1]], %bb.3, %28, %bb.4
  ; CHECK-NEXT:   [[PHI2:%[0-9]+]]:ep = PHI [[VLDA_dmw_lda_w_ag_pstm_nrm_imm3]], %bb.3, %33, %bb.4
  ; CHECK-NEXT:   [[PHI3:%[0-9]+]]:ep = PHI [[COPY5]], %bb.3, %35, %bb.4
  ; CHECK-NEXT:   [[PHI4:%[0-9]+]]:vec512 = PHI [[REG_SEQUENCE]], %bb.3, %29, %bb.4
  ; CHECK-NEXT:   [[PHI5:%[0-9]+]]:vec512 = PHI [[REG_SEQUENCE1]], %bb.3, %34, %bb.4
  ; CHECK-NEXT:   [[VLDA_dmw_lda_w_ag_pstm_nrm_imm4:%[0-9]+]]:vec256, [[VLDA_dmw_lda_w_ag_pstm_nrm_imm5:%[0-9]+]]:ep = VLDA_dmw_lda_w_ag_pstm_nrm_imm [[PHI1]], 32 :: (load unknown-size from %ir.in1, align 32)
  ; CHECK-NEXT:   [[REG_SEQUENCE2:%[0-9]+]]:vec512 = REG_SEQUENCE [[VBCST_32_]].sub_256_lo, %subreg.sub_256_hi, [[VLDA_dmw_lda_w_ag_pstm_nrm_imm4]], %subreg.sub_256_lo
  ; CHECK-NEXT:   [[ADD_add_r_ri1:%[0-9]+]]:er = nsw ADD_add_r_ri [[PHI]], -1, implicit-def $srcarry
  ; CHECK-NEXT:   [[VMUL_vmac_cm_core_dense:%[0-9]+]]:acc1024 = VMUL_vmac_cm_core_dense [[PHI4]], [[PHI5]], [[COPY]]
  ; CHECK-NEXT:   [[VLDA_dmw_lda_w_ag_pstm_nrm_imm6:%[0-9]+]]:vec256, [[VLDA_dmw_lda_w_ag_pstm_nrm_imm7:%[0-9]+]]:ep = VLDA_dmw_lda_w_ag_pstm_nrm_imm [[PHI2]], 32 :: (load unknown-size from %ir.in2, align 32)
  ; CHECK-NEXT:   [[REG_SEQUENCE3:%[0-9]+]]:vec512 = REG_SEQUENCE [[VLDA_dmw_lda_w_ag_pstm_nrm_imm6]], %subreg.sub_256_lo
  ; CHECK-NEXT:   [[VST_SRS_S8_S32_ag_pstm_nrm_imm:%[0-9]+]]:ep = VST_SRS_S8_S32_ag_pstm_nrm_imm [[PHI3]], 32, [[VMUL_vmac_cm_core_dense]], [[COPY2]], implicit-def $srsrs_of, implicit $crsat, implicit $crrnd :: (store (<32 x s8>) into %ir.out)
  ; CHECK-NEXT:   PseudoJNZ [[ADD_add_r_ri1]], %bb.4
  ; CHECK-NEXT:   PseudoJ_jump_imm %bb.5
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.5:
  ; CHECK-NEXT:   successors: %bb.2(0x80000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[PHI6:%[0-9]+]]:ep = PHI [[VST_SRS_S8_S32_ag_pstm_nrm_imm]], %bb.4
  ; CHECK-NEXT:   [[PHI7:%[0-9]+]]:vec512 = PHI [[REG_SEQUENCE2]], %bb.4
  ; CHECK-NEXT:   [[PHI8:%[0-9]+]]:vec512 = PHI [[REG_SEQUENCE3]], %bb.4
  ; CHECK-NEXT:   [[VMUL_vmac_cm_core_dense1:%[0-9]+]]:acc1024 = VMUL_vmac_cm_core_dense [[PHI7]], [[PHI8]], [[COPY]]
  ; CHECK-NEXT:   [[VST_SRS_S8_S32_ag_pstm_nrm_imm1:%[0-9]+]]:ep = VST_SRS_S8_S32_ag_pstm_nrm_imm [[PHI6]], 32, [[VMUL_vmac_cm_core_dense1]], [[COPY2]], implicit-def $srsrs_of, implicit $crsat, implicit $crrnd :: (store unknown-size into %ir.out, align 32)
  ; CHECK-NEXT:   PseudoJ_jump_imm %bb.2
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.2:
  ; CHECK-NEXT:   PseudoRET implicit $lr
  bb.1:
    liveins: $p0, $p1, $p2, $r0, $r1, $s0
    %0:er = COPY $r0
    %1:er = COPY $r1
    %5:mss = COPY $s0
    %10:ep = COPY $p0
    %11:ep = COPY $p1
    %12:ep = COPY $p2
    %20:vec512 = VBCST_32 %0

  bb.3:
    %30:er = PHI %1, %bb.1, %31, %bb.3
    %40:ep = PHI %10, %bb.1, %50, %bb.3
    %41:ep = PHI %11, %bb.1, %51, %bb.3
    %42:ep = PHI %12, %bb.1, %52, %bb.3
    %100:vec256, %50:ep = VLDA_dmw_lda_w_ag_pstm_nrm_imm %40, 32 :: (load (<32 x s8>) from %ir.in1)
    %101:vec256, %51:ep = VLDA_dmw_lda_w_ag_pstm_nrm_imm %41, 32 :: (load (<32 x s8>) from %ir.in2)
    %102:vec512 = REG_SEQUENCE %20.sub_256_lo, %subreg.sub_256_hi, %100, %subreg.sub_256_lo
    %103:vec512 = REG_SEQUENCE %101, %subreg.sub_256_lo
    %104:acc1024 = VMUL_vmac_cm_core_dense %102, %103, %0
    %52:ep = VST_SRS_S8_S32_ag_pstm_nrm_imm %42, 32, %104, %5, implicit-def $srsrs_of, implicit $crsat, implicit $crrnd :: (store (<32 x s8>) into %ir.out)
    %31:er = nsw ADD_add_r_ri %30, -1, implicit-def $srcarry
    PseudoJNZ %31, %bb.3

  bb.2:
    PseudoRET implicit $lr
...

# We expect the REG_SEQUENCE for the both loads to be in the first stage, close to
# their sources.
---
name:           vmul_512
alignment:       16
tracksRegLiveness: true
body:             |
  ; CHECK-LABEL: name: vmul_512
  ; CHECK: bb.0:
  ; CHECK-NEXT:   successors: %bb.3(0x80000000)
  ; CHECK-NEXT:   liveins: $p0, $p1, $p2, $r0, $r1, $s0
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:er = COPY $r0
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:er = COPY $r1
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:mss = COPY $s0
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:ep = COPY $p0
  ; CHECK-NEXT:   [[COPY4:%[0-9]+]]:ep = COPY $p1
  ; CHECK-NEXT:   [[COPY5:%[0-9]+]]:ep = COPY $p2
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.3:
  ; CHECK-NEXT:   successors: %bb.4(0x80000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[VLDA_dmw_lda_w_ag_pstm_nrm_imm:%[0-9]+]]:vec256, [[VLDA_dmw_lda_w_ag_pstm_nrm_imm1:%[0-9]+]]:ep = VLDA_dmw_lda_w_ag_pstm_nrm_imm [[COPY3]], 32 :: (load (<32 x s8>) from %ir.in1)
  ; CHECK-NEXT:   [[VLDA_dmw_lda_w_ag_pstm_nrm_imm2:%[0-9]+]]:vec256, [[VLDA_dmw_lda_w_ag_pstm_nrm_imm3:%[0-9]+]]:ep = VLDA_dmw_lda_w_ag_pstm_nrm_imm [[VLDA_dmw_lda_w_ag_pstm_nrm_imm1]], 32 :: (load (<32 x s8>) from %ir.in1)
  ; CHECK-NEXT:   [[VLDA_dmw_lda_w_ag_pstm_nrm_imm4:%[0-9]+]]:vec256, [[VLDA_dmw_lda_w_ag_pstm_nrm_imm5:%[0-9]+]]:ep = VLDA_dmw_lda_w_ag_pstm_nrm_imm [[COPY4]], 32 :: (load (<32 x s8>) from %ir.in2)
  ; CHECK-NEXT:   [[VLDA_dmw_lda_w_ag_pstm_nrm_imm6:%[0-9]+]]:vec256, [[VLDA_dmw_lda_w_ag_pstm_nrm_imm7:%[0-9]+]]:ep = VLDA_dmw_lda_w_ag_pstm_nrm_imm [[VLDA_dmw_lda_w_ag_pstm_nrm_imm5]], 32 :: (load (<32 x s8>) from %ir.in2)
  ; CHECK-NEXT:   [[REG_SEQUENCE:%[0-9]+]]:vec512 = REG_SEQUENCE [[VLDA_dmw_lda_w_ag_pstm_nrm_imm2]], %subreg.sub_256_hi, [[VLDA_dmw_lda_w_ag_pstm_nrm_imm]], %subreg.sub_256_lo
  ; CHECK-NEXT:   [[REG_SEQUENCE1:%[0-9]+]]:vec512 = REG_SEQUENCE [[VLDA_dmw_lda_w_ag_pstm_nrm_imm6]], %subreg.sub_256_hi, [[VLDA_dmw_lda_w_ag_pstm_nrm_imm4]], %subreg.sub_256_lo
  ; CHECK-NEXT:   [[ADD_add_r_ri:%[0-9]+]]:er = nsw ADD_add_r_ri [[COPY1]], -1, implicit-def $srcarry
  ; CHECK-NEXT:   PseudoJ_jump_imm %bb.4
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.4:
  ; CHECK-NEXT:   successors: %bb.4(0x40000000), %bb.5(0x40000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[PHI:%[0-9]+]]:er = PHI [[ADD_add_r_ri]], %bb.3, %36, %bb.4
  ; CHECK-NEXT:   [[PHI1:%[0-9]+]]:ep = PHI [[VLDA_dmw_lda_w_ag_pstm_nrm_imm3]], %bb.3, %41, %bb.4
  ; CHECK-NEXT:   [[PHI2:%[0-9]+]]:ep = PHI [[VLDA_dmw_lda_w_ag_pstm_nrm_imm7]], %bb.3, %44, %bb.4
  ; CHECK-NEXT:   [[PHI3:%[0-9]+]]:ep = PHI [[COPY5]], %bb.3, %46, %bb.4
  ; CHECK-NEXT:   [[PHI4:%[0-9]+]]:vec512 = PHI [[REG_SEQUENCE]], %bb.3, %42, %bb.4
  ; CHECK-NEXT:   [[PHI5:%[0-9]+]]:vec512 = PHI [[REG_SEQUENCE1]], %bb.3, %45, %bb.4
  ; CHECK-NEXT:   [[VLDA_dmw_lda_w_ag_pstm_nrm_imm8:%[0-9]+]]:vec256, [[VLDA_dmw_lda_w_ag_pstm_nrm_imm9:%[0-9]+]]:ep = VLDA_dmw_lda_w_ag_pstm_nrm_imm [[PHI2]], 32 :: (load unknown-size from %ir.in2, align 32)
  ; CHECK-NEXT:   [[ADD_add_r_ri1:%[0-9]+]]:er = nsw ADD_add_r_ri [[PHI]], -1, implicit-def $srcarry
  ; CHECK-NEXT:   [[VLDA_dmw_lda_w_ag_pstm_nrm_imm10:%[0-9]+]]:vec256, [[VLDA_dmw_lda_w_ag_pstm_nrm_imm11:%[0-9]+]]:ep = VLDA_dmw_lda_w_ag_pstm_nrm_imm [[PHI1]], 32 :: (load unknown-size from %ir.in1, align 32)
  ; CHECK-NEXT:   [[VMUL_vmac_cm_core_dense:%[0-9]+]]:acc1024 = VMUL_vmac_cm_core_dense [[PHI4]], [[PHI5]], [[COPY]]
  ; CHECK-NEXT:   [[VLDA_dmw_lda_w_ag_pstm_nrm_imm12:%[0-9]+]]:vec256, [[VLDA_dmw_lda_w_ag_pstm_nrm_imm13:%[0-9]+]]:ep = VLDA_dmw_lda_w_ag_pstm_nrm_imm [[VLDA_dmw_lda_w_ag_pstm_nrm_imm11]], 32 :: (load unknown-size from %ir.in1, align 32)
  ; CHECK-NEXT:   [[REG_SEQUENCE2:%[0-9]+]]:vec512 = REG_SEQUENCE [[VLDA_dmw_lda_w_ag_pstm_nrm_imm12]], %subreg.sub_256_hi, [[VLDA_dmw_lda_w_ag_pstm_nrm_imm10]], %subreg.sub_256_lo
  ; CHECK-NEXT:   [[VLDA_dmw_lda_w_ag_pstm_nrm_imm14:%[0-9]+]]:vec256, [[VLDA_dmw_lda_w_ag_pstm_nrm_imm15:%[0-9]+]]:ep = VLDA_dmw_lda_w_ag_pstm_nrm_imm [[VLDA_dmw_lda_w_ag_pstm_nrm_imm9]], 32 :: (load unknown-size from %ir.in2, align 32)
  ; CHECK-NEXT:   [[REG_SEQUENCE3:%[0-9]+]]:vec512 = REG_SEQUENCE [[VLDA_dmw_lda_w_ag_pstm_nrm_imm14]], %subreg.sub_256_hi, [[VLDA_dmw_lda_w_ag_pstm_nrm_imm8]], %subreg.sub_256_lo
  ; CHECK-NEXT:   [[VST_SRS_S8_S32_ag_pstm_nrm_imm:%[0-9]+]]:ep = VST_SRS_S8_S32_ag_pstm_nrm_imm [[PHI3]], 32, [[VMUL_vmac_cm_core_dense]], [[COPY2]], implicit-def $srsrs_of, implicit $crsat, implicit $crrnd :: (store (<32 x s8>) into %ir.out)
  ; CHECK-NEXT:   PseudoJNZ [[ADD_add_r_ri1]], %bb.4
  ; CHECK-NEXT:   PseudoJ_jump_imm %bb.5
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.5:
  ; CHECK-NEXT:   successors: %bb.2(0x80000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[PHI6:%[0-9]+]]:ep = PHI [[VST_SRS_S8_S32_ag_pstm_nrm_imm]], %bb.4
  ; CHECK-NEXT:   [[PHI7:%[0-9]+]]:vec512 = PHI [[REG_SEQUENCE2]], %bb.4
  ; CHECK-NEXT:   [[PHI8:%[0-9]+]]:vec512 = PHI [[REG_SEQUENCE3]], %bb.4
  ; CHECK-NEXT:   [[VMUL_vmac_cm_core_dense1:%[0-9]+]]:acc1024 = VMUL_vmac_cm_core_dense [[PHI7]], [[PHI8]], [[COPY]]
  ; CHECK-NEXT:   [[VST_SRS_S8_S32_ag_pstm_nrm_imm1:%[0-9]+]]:ep = VST_SRS_S8_S32_ag_pstm_nrm_imm [[PHI6]], 32, [[VMUL_vmac_cm_core_dense1]], [[COPY2]], implicit-def $srsrs_of, implicit $crsat, implicit $crrnd :: (store unknown-size into %ir.out, align 32)
  ; CHECK-NEXT:   PseudoJ_jump_imm %bb.2
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.2:
  ; CHECK-NEXT:   PseudoRET implicit $lr
  bb.1:
    liveins: $p0, $p1, $p2, $r0, $r1, $s0
    %0:er = COPY $r0
    %1:er = COPY $r1
    %5:mss = COPY $s0
    %10:ep = COPY $p0
    %11:ep = COPY $p1
    %12:ep = COPY $p2

  bb.3:
    %30:er = PHI %1, %bb.1, %31, %bb.3
    %40:ep = PHI %10, %bb.1, %60, %bb.3
    %41:ep = PHI %11, %bb.1, %61, %bb.3
    %42:ep = PHI %12, %bb.1, %52, %bb.3
    %100:vec256, %50:ep = VLDA_dmw_lda_w_ag_pstm_nrm_imm %40, 32 :: (load (<32 x s8>) from %ir.in1)
    %101:vec256, %60:ep = VLDA_dmw_lda_w_ag_pstm_nrm_imm %50, 32 :: (load (<32 x s8>) from %ir.in1)
    %102:vec256, %51:ep = VLDA_dmw_lda_w_ag_pstm_nrm_imm %41, 32 :: (load (<32 x s8>) from %ir.in2)
    %103:vec256, %61:ep = VLDA_dmw_lda_w_ag_pstm_nrm_imm %51, 32 :: (load (<32 x s8>) from %ir.in2)
    %104:vec512 = REG_SEQUENCE %101, %subreg.sub_256_hi, %100, %subreg.sub_256_lo
    %105:vec512 = REG_SEQUENCE %103, %subreg.sub_256_hi, %102, %subreg.sub_256_lo
    %106:acc1024 = VMUL_vmac_cm_core_dense %104, %105, %0
    %52:ep = VST_SRS_S8_S32_ag_pstm_nrm_imm %42, 32, %106, %5, implicit-def $srsrs_of, implicit $crsat, implicit $crrnd :: (store (<32 x s8>) into %ir.out)
    %31:er = nsw ADD_add_r_ri %30, -1, implicit-def $srcarry
    PseudoJNZ %31, %bb.3

  bb.2:
    PseudoRET implicit $lr
...
