# NOTE: Assertions have been autogenerated by utils/update_mir_test_checks.py
#
# This file is licensed under the Apache License v2.0 with LLVM Exceptions.
# See https://llvm.org/LICENSE.txt for license information.
# SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
#
# (c) Copyright 2024 Advanced Micro Devices, Inc. or its affiliates
# RUN: llc -march=aie2 -run-pass=machine-scheduler %s -o - | FileCheck %s

# A smaller example reduced from conv2d_inner.mir
# There are two test cases:
#  - One where the reg pressure is low enough to start scheduling the
#    VSHIFT_ALIGN early (in bottom-up), even if they increase the pressure
#  - Another example where the pressure is artificially increased with 3 eY
#    registers that are live across the loop. The VLDA should be scheduled
#    earlier to help reduce the pressure and avoid spills.


---
name: conv2d_innermost_low_pressure
tracksRegLiveness: true
body: |
  ; CHECK-LABEL: name: conv2d_innermost_low_pressure
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   successors: %bb.1(0x80000000)
  ; CHECK-NEXT:   liveins: $p0, $m0, $cm0, $cm1, $s0, $d1, $x0, $y2, $r0, $d0_3d
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:vec512 = COPY $x0
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:vec512 = COPY $x0
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vec512 = COPY $x0
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vec512 = COPY $x0
  ; CHECK-NEXT:   [[COPY4:%[0-9]+]]:vec512 = COPY $x0
  ; CHECK-NEXT:   [[COPY5:%[0-9]+]]:vec512 = COPY $x0
  ; CHECK-NEXT:   [[COPY6:%[0-9]+]]:vec512 = COPY $x0
  ; CHECK-NEXT:   [[COPY7:%[0-9]+]]:vec512 = COPY $x0
  ; CHECK-NEXT:   [[COPY8:%[0-9]+]]:acc1024 = COPY $cm0
  ; CHECK-NEXT:   [[COPY9:%[0-9]+]]:vec512 = COPY $x0
  ; CHECK-NEXT:   [[COPY10:%[0-9]+]]:er = COPY $r0
  ; CHECK-NEXT:   [[COPY11:%[0-9]+]]:ep_as_32bit = COPY $p0
  ; CHECK-NEXT:   [[COPY12:%[0-9]+]]:ep_as_32bit = COPY $p0
  ; CHECK-NEXT:   [[COPY13:%[0-9]+]]:eds = COPY $d0_3d
  ; CHECK-NEXT:   [[COPY14:%[0-9]+]]:acc1024 = COPY $cm0
  ; CHECK-NEXT:   [[COPY15:%[0-9]+]]:er = COPY $r0
  ; CHECK-NEXT:   [[COPY16:%[0-9]+]]:mss = COPY $s0
  ; CHECK-NEXT:   PseudoJ_jump_imm %bb.1
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.1:
  ; CHECK-NEXT:   successors: %bb.2(0x40000000), %bb.1(0x40000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY17:%[0-9]+]]:vec512 = COPY [[COPY1]]
  ; CHECK-NEXT:   [[COPY8:%[0-9]+]]:acc1024 = VMAC_vmac_cm_core_dense [[COPY8]], [[COPY4]], [[COPY17]], [[COPY10]]
  ; CHECK-NEXT:   [[COPY14:%[0-9]+]]:acc1024 = VMAC_vmac_cm_core_dense [[COPY14]], [[COPY5]], [[COPY17]], [[COPY10]]
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:vec512 = COPY [[COPY9]]
  ; CHECK-NEXT:   undef [[COPY9:%[0-9]+]].sub_256_lo:vec512, [[COPY11:%[0-9]+]]:ep_as_32bit = VLDA_dmw_lda_w_ag_pstm_nrm_imm [[COPY11]], 32
  ; CHECK-NEXT:   [[COPY9:%[0-9]+]].sub_256_hi:vec512, [[COPY11:%[0-9]+]]:ep_as_32bit = VLDA_dmw_lda_w_ag_pstm_nrm_imm [[COPY11]], 32
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vec512 = VSHIFT_ALIGN [[COPY2]], [[COPY16]], [[COPY6]], [[COPY10]]
  ; CHECK-NEXT:   undef [[COPY6:%[0-9]+]].sub_256_lo:vec512, [[COPY12:%[0-9]+]]:ep_as_32bit = VLDA_dmw_lda_w_ag_pstm_nrm_imm [[COPY12]], 32
  ; CHECK-NEXT:   [[COPY6:%[0-9]+]].sub_256_hi:vec512, [[COPY12:%[0-9]+]]:ep_as_32bit = VLDA_dmw_lda_w_ag_pstm_nrm_imm [[COPY12]], 32
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vec512 = VSHIFT_ALIGN [[COPY3]], [[COPY16]], [[COPY7]], [[COPY10]]
  ; CHECK-NEXT:   undef [[COPY7:%[0-9]+]].sub_256_lo:vec512, [[COPY12:%[0-9]+]]:ep_as_32bit = VLDA_dmw_lda_w_ag_pstm_nrm_imm [[COPY12]], 32
  ; CHECK-NEXT:   [[COPY7:%[0-9]+]].sub_256_hi:vec512, [[COPY12:%[0-9]+]]:ep_as_32bit, [[COPY13:%[0-9]+]].sub_dim_count:eds, [[COPY13:%[0-9]+]].sub_hi_dim_then_sub_dim_count:eds = VLDA_3D_dmw_lda_w [[COPY12]], [[COPY13]]
  ; CHECK-NEXT:   [[COPY4:%[0-9]+]]:vec512 = VSHUFFLE [[COPY2]], [[COPY3]], [[COPY10]]
  ; CHECK-NEXT:   [[COPY5:%[0-9]+]]:vec512 = VSHUFFLE [[COPY4]], [[COPY]], [[COPY10]]
  ; CHECK-NEXT:   PseudoJNZ [[COPY15]], %bb.1
  ; CHECK-NEXT:   PseudoJ_jump_imm %bb.2
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.2:
  ; CHECK-NEXT:   PseudoRET implicit $lr
  bb.0.entry:
    liveins: $p0, $m0, $cm0, $cm1, $s0, $d1, $x0, $y2, $r0, $d0_3d

    %0:mss = COPY $s0
    %1:vec512 = COPY $x0
    %2:vec512 = COPY $x0
    %3:vec512 = COPY $x0
    %4:vec512 = COPY $x0
    %5:vec512 = COPY $x0
    %6:vec512 = COPY $x0
    %7:vec512 = COPY $x0
    %8:vec512 = COPY $x0
    %9:vec512 = COPY $x0
    %10:er = COPY $r0
    %11:ep_as_32bit = COPY $p0
    %12:ep_as_32bit = COPY $p0
    %13:eds = COPY $d0_3d
    %14:acc1024 = COPY $cm0
    %15:acc1024 = COPY $cm0
    %16:er = COPY $r0
    PseudoJ_jump_imm %bb.1

  bb.1:
    successors: %bb.2, %bb.1

    %20:vec512 = COPY %2
    %2:vec512 = COPY %9
    %14:acc1024 = VMAC_vmac_cm_core_dense %14, %5, %20, %10
    %15:acc1024 = VMAC_vmac_cm_core_dense %15, %6, %20, %10
    %3:vec512 = VSHIFT_ALIGN %3, %0, %7, %10
    %4:vec512 = VSHIFT_ALIGN %4, %0, %8, %10
    %5:vec512 = VSHUFFLE %3, %4, %10
    %6:vec512 = VSHUFFLE %5, %1, %10
    undef %9.sub_256_lo:vec512, %11:ep_as_32bit = VLDA_dmw_lda_w_ag_pstm_nrm_imm %11, 32
    %9.sub_256_hi:vec512, %11:ep_as_32bit = VLDA_dmw_lda_w_ag_pstm_nrm_imm %11, 32
    undef %7.sub_256_lo:vec512, %12:ep_as_32bit = VLDA_dmw_lda_w_ag_pstm_nrm_imm %12, 32
    %7.sub_256_hi:vec512, %12:ep_as_32bit = VLDA_dmw_lda_w_ag_pstm_nrm_imm %12, 32
    undef %8.sub_256_lo:vec512, %12:ep_as_32bit = VLDA_dmw_lda_w_ag_pstm_nrm_imm %12, 32
    %8.sub_256_hi:vec512, %12:ep_as_32bit, %13.sub_dim_count:eds, %13.sub_hi_dim_then_sub_dim_count:eds = VLDA_3D_dmw_lda_w %12, %13
    PseudoJNZ %16, %bb.1
    PseudoJ_jump_imm %bb.2

  bb.2:
    PseudoRET implicit $lr
...

# Increase the pressure for the W/X/Y regunits with three Y registers live
# across the loop: %17, %18, %19
---
name: conv2d_innermost_high_pressure
tracksRegLiveness: true
body: |
  ; CHECK-LABEL: name: conv2d_innermost_high_pressure
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   successors: %bb.1(0x80000000)
  ; CHECK-NEXT:   liveins: $p0, $m0, $cm0, $cm1, $s0, $d1, $x0, $y2, $r0, $d0_3d
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:vec512 = COPY $x0
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:vec512 = COPY $x0
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vec1024 = COPY $y2
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vec1024 = COPY $y2
  ; CHECK-NEXT:   [[COPY4:%[0-9]+]]:acc1024 = COPY $cm0
  ; CHECK-NEXT:   [[COPY5:%[0-9]+]]:er = COPY $r0
  ; CHECK-NEXT:   [[COPY6:%[0-9]+]]:ep_as_32bit = COPY $p0
  ; CHECK-NEXT:   [[COPY7:%[0-9]+]]:ep_as_32bit = COPY $p0
  ; CHECK-NEXT:   [[COPY8:%[0-9]+]]:eds = COPY $d0_3d
  ; CHECK-NEXT:   [[COPY9:%[0-9]+]]:acc1024 = COPY $cm0
  ; CHECK-NEXT:   [[COPY10:%[0-9]+]]:er = COPY $r0
  ; CHECK-NEXT:   [[COPY11:%[0-9]+]]:vec512 = COPY $x0
  ; CHECK-NEXT:   [[COPY12:%[0-9]+]]:vec512 = COPY $x0
  ; CHECK-NEXT:   [[COPY13:%[0-9]+]]:vec1024 = COPY $y2
  ; CHECK-NEXT:   [[COPY14:%[0-9]+]]:vec512 = COPY $x0
  ; CHECK-NEXT:   [[COPY15:%[0-9]+]]:vec512 = COPY $x0
  ; CHECK-NEXT:   [[COPY16:%[0-9]+]]:vec512 = COPY $x0
  ; CHECK-NEXT:   [[COPY17:%[0-9]+]]:vec512 = COPY $x0
  ; CHECK-NEXT:   [[COPY18:%[0-9]+]]:vec512 = COPY $x0
  ; CHECK-NEXT:   [[COPY19:%[0-9]+]]:mss = COPY $s0
  ; CHECK-NEXT:   PseudoJ_jump_imm %bb.1
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.1:
  ; CHECK-NEXT:   successors: %bb.2(0x40000000), %bb.1(0x40000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY20:%[0-9]+]]:vec512 = COPY [[COPY11]]
  ; CHECK-NEXT:   [[COPY4:%[0-9]+]]:acc1024 = VMAC_vmac_cm_core_dense [[COPY4]], [[COPY14]], [[COPY20]], [[COPY5]], implicit [[COPY2]], implicit [[COPY3]], implicit [[COPY13]]
  ; CHECK-NEXT:   [[COPY12:%[0-9]+]]:vec512 = VSHIFT_ALIGN [[COPY12]], [[COPY19]], [[COPY16]], [[COPY5]]
  ; CHECK-NEXT:   [[COPY9:%[0-9]+]]:acc1024 = VMAC_vmac_cm_core_dense [[COPY9]], [[COPY15]], [[COPY20]], [[COPY5]]
  ; CHECK-NEXT:   [[COPY11:%[0-9]+]]:vec512 = COPY [[COPY18]]
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:vec512 = VSHIFT_ALIGN [[COPY1]], [[COPY19]], [[COPY17]], [[COPY5]]
  ; CHECK-NEXT:   undef [[COPY18:%[0-9]+]].sub_256_lo:vec512, [[COPY6:%[0-9]+]]:ep_as_32bit = VLDA_dmw_lda_w_ag_pstm_nrm_imm [[COPY6]], 32
  ; CHECK-NEXT:   [[COPY18:%[0-9]+]].sub_256_hi:vec512, [[COPY6:%[0-9]+]]:ep_as_32bit = VLDA_dmw_lda_w_ag_pstm_nrm_imm [[COPY6]], 32
  ; CHECK-NEXT:   undef [[COPY16:%[0-9]+]].sub_256_lo:vec512, [[COPY7:%[0-9]+]]:ep_as_32bit = VLDA_dmw_lda_w_ag_pstm_nrm_imm [[COPY7]], 32
  ; CHECK-NEXT:   [[COPY16:%[0-9]+]].sub_256_hi:vec512, [[COPY7:%[0-9]+]]:ep_as_32bit = VLDA_dmw_lda_w_ag_pstm_nrm_imm [[COPY7]], 32
  ; CHECK-NEXT:   undef [[COPY17:%[0-9]+]].sub_256_lo:vec512, [[COPY7:%[0-9]+]]:ep_as_32bit = VLDA_dmw_lda_w_ag_pstm_nrm_imm [[COPY7]], 32
  ; CHECK-NEXT:   [[COPY17:%[0-9]+]].sub_256_hi:vec512, [[COPY7:%[0-9]+]]:ep_as_32bit, [[COPY8:%[0-9]+]].sub_dim_count:eds, [[COPY8:%[0-9]+]].sub_hi_dim_then_sub_dim_count:eds = VLDA_3D_dmw_lda_w [[COPY7]], [[COPY8]]
  ; CHECK-NEXT:   [[COPY14:%[0-9]+]]:vec512 = VSHUFFLE [[COPY12]], [[COPY1]], [[COPY5]]
  ; CHECK-NEXT:   [[COPY15:%[0-9]+]]:vec512 = VSHUFFLE [[COPY14]], [[COPY]], [[COPY5]]
  ; CHECK-NEXT:   PseudoJNZ [[COPY10]], %bb.1
  ; CHECK-NEXT:   PseudoJ_jump_imm %bb.2
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.2:
  ; CHECK-NEXT:   PseudoRET implicit $lr
  bb.0.entry:
    liveins: $p0, $m0, $cm0, $cm1, $s0, $d1, $x0, $y2, $r0, $d0_3d

    %0:mss = COPY $s0
    %1:vec512 = COPY $x0
    %2:vec512 = COPY $x0
    %3:vec512 = COPY $x0
    %4:vec512 = COPY $x0
    %5:vec512 = COPY $x0
    %6:vec512 = COPY $x0
    %7:vec512 = COPY $x0
    %8:vec512 = COPY $x0
    %9:vec512 = COPY $x0
    %10:er = COPY $r0
    %11:ep_as_32bit = COPY $p0
    %12:ep_as_32bit = COPY $p0
    %13:eds = COPY $d0_3d
    %14:acc1024 = COPY $cm0
    %15:acc1024 = COPY $cm0
    %16:er = COPY $r0

    %17:vec1024 = COPY $y2
    %18:vec1024 = COPY $y2
    %19:vec1024 = COPY $y2
    PseudoJ_jump_imm %bb.1

  bb.1:
    successors: %bb.2, %bb.1

    %20:vec512 = COPY %2
    %2:vec512 = COPY %9
    %14:acc1024 = VMAC_vmac_cm_core_dense %14, %5, %20, %10, implicit %17, implicit %18, implicit %19
    %15:acc1024 = VMAC_vmac_cm_core_dense %15, %6, %20, %10
    %3:vec512 = VSHIFT_ALIGN %3, %0, %7, %10
    %4:vec512 = VSHIFT_ALIGN %4, %0, %8, %10
    %5:vec512 = VSHUFFLE %3, %4, %10
    %6:vec512 = VSHUFFLE %5, %1, %10
    undef %9.sub_256_lo:vec512, %11:ep_as_32bit = VLDA_dmw_lda_w_ag_pstm_nrm_imm %11, 32
    %9.sub_256_hi:vec512, %11:ep_as_32bit = VLDA_dmw_lda_w_ag_pstm_nrm_imm %11, 32
    undef %7.sub_256_lo:vec512, %12:ep_as_32bit = VLDA_dmw_lda_w_ag_pstm_nrm_imm %12, 32
    %7.sub_256_hi:vec512, %12:ep_as_32bit = VLDA_dmw_lda_w_ag_pstm_nrm_imm %12, 32
    undef %8.sub_256_lo:vec512, %12:ep_as_32bit = VLDA_dmw_lda_w_ag_pstm_nrm_imm %12, 32
    %8.sub_256_hi:vec512, %12:ep_as_32bit, %13.sub_dim_count:eds, %13.sub_hi_dim_then_sub_dim_count:eds = VLDA_3D_dmw_lda_w %12, %13
    PseudoJNZ %16, %bb.1
    PseudoJ_jump_imm %bb.2

  bb.2:
    PseudoRET implicit $lr
...
