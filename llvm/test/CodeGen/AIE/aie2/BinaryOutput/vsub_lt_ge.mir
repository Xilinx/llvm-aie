#
# This file is licensed under the Apache License v2.0 with LLVM Exceptions.
# See https://llvm.org/LICENSE.txt for license information.
# SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
#
# (c) Copyright 2023-2024 Advanced Micro Devices, Inc. or its affiliates
# CHECK lines automatically generated using update_encodings.py
# RUN: llc %llcflags --filetype=obj -o %t
# RUN: llvm-objdump --triple=aie2 -dr --no-print-imm-hex %t | FileCheck --ignore-case %s
# RUN: llc %llcflags --filetype=asm -o %t2
# RUN: llvm-mc -triple aie2 -filetype=obj -o %t %t2
# RUN: llvm-objdump --triple=aie2 -dr --no-print-imm-hex %t | FileCheck --ignore-case %s

# CHECK: 0: 19 09 75 18 vsub_lt.d8 x0, r31:r30, x1, x2
# CHECK: 4: 19 1a 85 18 vsub_lt.d8 x1, r17:r16, x3, x4
# CHECK: 8: b9 20 55 19 vsub_lt.d16 x2, r21, x4, x1
# CHECK: c: d9 29 f5 19 vsub_lt.d32 x3, r23, x5, x3
# CHECK: 10: 19 89 25 18 vsub_lt.s8 x0, r21:r20, x1, x2
# CHECK: 14: 19 89 05 1b vsub_lt.s8 x6, r17:r16, x1, x2
# CHECK: 18: b9 a0 65 19 vsub_lt.s16 x2, r22, x4, x1
# CHECK: 1c: d9 a9 a5 19 vsub_lt.s32 x3, r18, x5, x3
# CHECK: 20: 19 09 c6 1a vsub_ge.d8 x5, r25:r24, x1, x2
# CHECK: 24: 19 09 36 1a vsub_ge.d8 x4, r23:r22, x1, x2
# CHECK: 28: b9 20 36 19 vsub_ge.d16 x2, r19, x4, x1
# CHECK: 2c: d9 29 c6 19 vsub_ge.d32 x3, r20, x5, x3
# CHECK: 30: 19 89 06 18 vsub_ge.s8 x0, r17:r16, x1, x2
# CHECK: 34: 19 89 e6 1b vsub_ge.s8 x7, r29:r28, x1, x2
# CHECK: 38: b9 a0 16 19 vsub_ge.s16 x2, r17, x4, x1
# CHECK: 3c: d9 a9 86 19 vsub_ge.s32 x3, r16, x5, x3

---
name:            test_vsub_lt_ge
alignment:       16
body:             |
  bb.0 (align 16):
    ; 8.3.3 VSUB_LT / VSUB_GE â€“ Combined vector subtraction and comparison

    ; VSUB_LT
    $x0, $l7  = VSUB_LT_D8  $x1, $x2, implicit $crvaddsign
    $x1, $l0  = VSUB_LT_D8  $x3, $x4, implicit $crvaddsign
    $x2, $r21 = VSUB_LT_D16 $x4, $x1, implicit $crvaddsign
    $x3, $r23 = VSUB_LT_D32 $x5, $x3, implicit $crvaddsign
    $x0, $l2  = VSUB_LT_S8  $x1, $x2
    $x6, $l0  = VSUB_LT_S8  $x1, $x2
    $x2, $r22 = VSUB_LT_S16 $x4, $x1
    $x3, $r18 = VSUB_LT_S32 $x5, $x3

    ; VSUB_GE
    $x5, $l4  = VSUB_GE_D8  $x1, $x2, implicit $crvaddsign
    $x4, $l3  = VSUB_GE_D8  $x1, $x2, implicit $crvaddsign
    $x2, $r19 = VSUB_GE_D16 $x4, $x1, implicit $crvaddsign
    $x3, $r20 = VSUB_GE_D32 $x5, $x3, implicit $crvaddsign
    $x0, $l0  = VSUB_GE_S8  $x1, $x2
    $x7, $l6  = VSUB_GE_S8  $x1, $x2
    $x2, $r17 = VSUB_GE_S16 $x4, $x1
    $x3, $r16 = VSUB_GE_S32 $x5, $x3
...
