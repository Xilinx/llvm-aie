# NOTE: Assertions have been autogenerated by utils/update_mir_test_checks.py
#
# This file is licensed under the Apache License v2.0 with LLVM Exceptions.
# See https://llvm.org/LICENSE.txt for license information.
# SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
#
# (c) Copyright 2023-2024 Advanced Micro Devices, Inc. or its affiliates
# RUN: llc --mtriple=aie2 --run-pass=early-machinelicm %s -o - | FileCheck  %s

# %24:mss = COPY %20 can be safely hoisted, it doesn't increase the register
# pressure beyond any threshold.
---
name:            low_s_pressure
alignment:       16
tracksRegLiveness: true
body:             |
  ; CHECK-LABEL: name: low_s_pressure
  ; CHECK: bb.0:
  ; CHECK-NEXT:   successors: %bb.1(0x80000000)
  ; CHECK-NEXT:   liveins: $r0, $p0
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:ep = COPY $p0
  ; CHECK-NEXT:   [[MOV_RLC_imm10_pseudo:%[0-9]+]]:er = MOV_RLC_imm10_pseudo 16
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:er = COPY $r0
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:mss = COPY [[COPY1]]
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.1:
  ; CHECK-NEXT:   successors: %bb.1(0x40000000), %bb.2(0x40000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[PHI:%[0-9]+]]:er = PHI [[MOV_RLC_imm10_pseudo]], %bb.0, %4, %bb.1
  ; CHECK-NEXT:   [[PHI1:%[0-9]+]]:ep = PHI [[COPY]], %bb.0, %6, %bb.1
  ; CHECK-NEXT:   [[VLDA_UPS_S64_D32_ag_idx_imm:%[0-9]+]]:acc512 = VLDA_UPS_S64_D32_ag_idx_imm [[COPY2]], [[PHI1]], 0, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<8 x s32>))
  ; CHECK-NEXT:   [[PADD_imm9_pseudo:%[0-9]+]]:ep = PADD_imm9_pseudo [[PHI1]], 32
  ; CHECK-NEXT:   [[ADD_add_r_ri:%[0-9]+]]:er = nsw ADD_add_r_ri [[PHI]], -1, implicit-def $srcarry
  ; CHECK-NEXT:   [[EQZ:%[0-9]+]]:er = EQZ [[ADD_add_r_ri]]
  ; CHECK-NEXT:   PseudoJZ [[EQZ]], %bb.1
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.2:
  ; CHECK-NEXT:   $bml0 = COPY [[VLDA_UPS_S64_D32_ag_idx_imm]]
  ; CHECK-NEXT:   PseudoRET implicit $lr, implicit $bml0
  bb.1:
    liveins: $r0, $p0
    %1:ep = COPY $p0
    %19:er = MOV_RLC_imm10_pseudo 16
    %20:er = COPY $r0

  bb.3:
    %3:er = PHI %19, %bb.1, %16, %bb.3
    %4:ep = PHI %1, %bb.1, %8, %bb.3
    %24:mss = COPY %20
    %9:acc512 = VLDA_UPS_S64_D32_ag_idx_imm %24, %4, 0, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<8 x s32>))
    %8:ep = PADD_imm9_pseudo  %4, 32
    %16:er = nsw ADD_add_r_ri %3, -1, implicit-def $srcarry
    %21:er = EQZ %16
    PseudoJZ %21, %bb.3

  bb.2:
    $bml0 = COPY %9
    PseudoRET implicit $lr, implicit $bml0

...

# Moving %24:mss = COPY %20 into bb.1 would increase the pressure on S regs to
# the limit. Prefer not hoisting.
---
name:            high_s_pressure
alignment:       16
tracksRegLiveness: true
body:             |
  ; CHECK-LABEL: name: high_s_pressure
  ; CHECK: bb.0:
  ; CHECK-NEXT:   successors: %bb.1(0x80000000)
  ; CHECK-NEXT:   liveins: $r0, $p0
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:ep = COPY $p0
  ; CHECK-NEXT:   [[MOV_RLC_imm10_pseudo:%[0-9]+]]:er = MOV_RLC_imm10_pseudo 16
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:er = COPY $r0
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:mss = COPY [[COPY1]]
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:mss = COPY [[COPY1]]
  ; CHECK-NEXT:   [[COPY4:%[0-9]+]]:mss = COPY [[COPY1]]
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.1:
  ; CHECK-NEXT:   successors: %bb.1(0x40000000), %bb.2(0x40000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[PHI:%[0-9]+]]:er = PHI [[MOV_RLC_imm10_pseudo]], %bb.0, %7, %bb.1
  ; CHECK-NEXT:   [[PHI1:%[0-9]+]]:ep = PHI [[COPY]], %bb.0, %9, %bb.1
  ; CHECK-NEXT:   [[COPY5:%[0-9]+]]:mss = COPY [[COPY1]]
  ; CHECK-NEXT:   [[VLDA_UPS_S64_D32_ag_idx_imm:%[0-9]+]]:acc512 = VLDA_UPS_S64_D32_ag_idx_imm [[COPY5]], [[PHI1]], 0, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<8 x s32>))
  ; CHECK-NEXT:   [[PADD_imm9_pseudo:%[0-9]+]]:ep = PADD_imm9_pseudo [[PHI1]], 32
  ; CHECK-NEXT:   [[ADD_add_r_ri:%[0-9]+]]:er = nsw ADD_add_r_ri [[PHI]], -1, implicit-def $srcarry
  ; CHECK-NEXT:   [[EQZ:%[0-9]+]]:er = EQZ [[ADD_add_r_ri]]
  ; CHECK-NEXT:   PseudoJZ [[EQZ]], %bb.1
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.2:
  ; CHECK-NEXT:   $bml0 = COPY [[VLDA_UPS_S64_D32_ag_idx_imm]]
  ; CHECK-NEXT:   PseudoRET implicit $lr, implicit $bml0, implicit [[COPY2]], implicit [[COPY3]], implicit [[COPY4]]
  bb.1:
    liveins: $r0, $p0
    %1:ep = COPY $p0
    %19:er = MOV_RLC_imm10_pseudo 16
    %20:er = COPY $r0
    %30:mss = COPY %20
    %31:mss = COPY %20
    %32:mss = COPY %20

  bb.3:
    %3:er = PHI %19, %bb.1, %16, %bb.3
    %4:ep = PHI %1, %bb.1, %8, %bb.3
    %24:mss = COPY %20
    %9:acc512 = VLDA_UPS_S64_D32_ag_idx_imm %24, %4, 0, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<8 x s32>))
    %8:ep = PADD_imm9_pseudo  %4, 32
    %16:er = nsw ADD_add_r_ri %3, -1, implicit-def $srcarry
    %21:er = EQZ %16
    PseudoJZ %21, %bb.3

  bb.2:
    $bml0 = COPY %9
    PseudoRET implicit $lr, implicit $bml0, implicit %30, implicit %31, implicit %32
...


# The pressure inside the loop after hoisting would exactly match the number
# of S registers. The current LICM logic will only hoist 3 copies, even though
# everything could be hoisted.
---
name:            limit_s_pressure_inside
alignment:       16
tracksRegLiveness: true
body:             |
  ; CHECK-LABEL: name: limit_s_pressure_inside
  ; CHECK: bb.0:
  ; CHECK-NEXT:   successors: %bb.1(0x80000000)
  ; CHECK-NEXT:   liveins: $r0, $p0
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:ep = COPY $p0
  ; CHECK-NEXT:   [[MOV_RLC_imm10_pseudo:%[0-9]+]]:er = MOV_RLC_imm10_pseudo 16
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:er = COPY $r0
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:er = COPY $r0
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:er = COPY $r0
  ; CHECK-NEXT:   [[COPY4:%[0-9]+]]:er = COPY $r0
  ; CHECK-NEXT:   [[COPY5:%[0-9]+]]:mss = COPY [[COPY1]]
  ; CHECK-NEXT:   [[COPY6:%[0-9]+]]:mss = COPY [[COPY2]]
  ; CHECK-NEXT:   [[COPY7:%[0-9]+]]:mss = COPY [[COPY3]]
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.1:
  ; CHECK-NEXT:   successors: %bb.1(0x40000000), %bb.2(0x40000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[PHI:%[0-9]+]]:er = PHI [[MOV_RLC_imm10_pseudo]], %bb.0, %7, %bb.1
  ; CHECK-NEXT:   [[PHI1:%[0-9]+]]:ep = PHI [[COPY]], %bb.0, %9, %bb.1
  ; CHECK-NEXT:   [[VLDA_UPS_S64_D32_ag_idx_imm:%[0-9]+]]:acc512 = VLDA_UPS_S64_D32_ag_idx_imm [[COPY5]], [[PHI1]], 0, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<8 x s32>))
  ; CHECK-NEXT:   [[VLDA_UPS_S64_D32_ag_idx_imm1:%[0-9]+]]:acc512 = VLDA_UPS_S64_D32_ag_idx_imm [[COPY6]], [[PHI1]], 0, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<8 x s32>))
  ; CHECK-NEXT:   [[VLDA_UPS_S64_D32_ag_idx_imm2:%[0-9]+]]:acc512 = VLDA_UPS_S64_D32_ag_idx_imm [[COPY7]], [[PHI1]], 0, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<8 x s32>))
  ; CHECK-NEXT:   [[COPY8:%[0-9]+]]:mss = COPY [[COPY4]]
  ; CHECK-NEXT:   [[VLDA_UPS_S64_D32_ag_idx_imm3:%[0-9]+]]:acc512 = VLDA_UPS_S64_D32_ag_idx_imm [[COPY8]], [[PHI1]], 0, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<8 x s32>))
  ; CHECK-NEXT:   [[PADD_imm9_pseudo:%[0-9]+]]:ep = PADD_imm9_pseudo [[PHI1]], 32
  ; CHECK-NEXT:   [[ADD_add_r_ri:%[0-9]+]]:er = nsw ADD_add_r_ri [[PHI]], -1, implicit-def $srcarry
  ; CHECK-NEXT:   [[EQZ:%[0-9]+]]:er = EQZ [[ADD_add_r_ri]]
  ; CHECK-NEXT:   PseudoJZ [[EQZ]], %bb.1
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.2:
  ; CHECK-NEXT:   PseudoRET implicit $lr, implicit [[VLDA_UPS_S64_D32_ag_idx_imm]], implicit [[VLDA_UPS_S64_D32_ag_idx_imm1]], implicit [[VLDA_UPS_S64_D32_ag_idx_imm2]], implicit [[VLDA_UPS_S64_D32_ag_idx_imm3]]
  bb.1:
    liveins: $r0, $p0
    %1:ep = COPY $p0
    %19:er = MOV_RLC_imm10_pseudo 16
    %21:er = COPY $r0
    %22:er = COPY $r0
    %23:er = COPY $r0
    %24:er = COPY $r0

  bb.3:
    %3:er = PHI %19, %bb.1, %16, %bb.3
    %4:ep = PHI %1, %bb.1, %8, %bb.3
    %71:mss = COPY %21
    %91:acc512 = VLDA_UPS_S64_D32_ag_idx_imm %71, %4, 0, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<8 x s32>))
    %72:mss = COPY %22
    %92:acc512 = VLDA_UPS_S64_D32_ag_idx_imm %72, %4, 0, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<8 x s32>))
    %73:mss = COPY %23
    %93:acc512 = VLDA_UPS_S64_D32_ag_idx_imm %73, %4, 0, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<8 x s32>))
    %74:mss = COPY %24
    %94:acc512 = VLDA_UPS_S64_D32_ag_idx_imm %74, %4, 0, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<8 x s32>))
    %8:ep = PADD_imm9_pseudo  %4, 32
    %16:er = nsw ADD_add_r_ri %3, -1, implicit-def $srcarry
    %17:er = EQZ %16
    PseudoJZ %17, %bb.3

  bb.2:
    PseudoRET implicit $lr, implicit %91, implicit %92, implicit %93, implicit %94
...

# This shows the limit of LICM's reg pressure tracking and updating.
# The loop requires 2 free S registers for allocating the %74 and %75 live
# ranges. But 3 S registers are hoisted, leaving only 1 free. This would cause
# one of them to be spilled, which is bad in AIE2 as it requires bouncing
# through a GPR on top of the load/store.
# Without allowing S regs to spill "back" to GPRs, this degrades performance.
---
name:            high_s_pressure_inside
alignment:       16
tracksRegLiveness: true
body:             |
  ; CHECK-LABEL: name: high_s_pressure_inside
  ; CHECK: bb.0:
  ; CHECK-NEXT:   successors: %bb.1(0x80000000)
  ; CHECK-NEXT:   liveins: $r0, $p0
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:ep = COPY $p0
  ; CHECK-NEXT:   [[MOV_RLC_imm10_pseudo:%[0-9]+]]:er = MOV_RLC_imm10_pseudo 16
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:er = COPY $r0
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:er = COPY $r0
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:er = COPY $r0
  ; CHECK-NEXT:   [[COPY4:%[0-9]+]]:er = COPY $r0
  ; CHECK-NEXT:   [[COPY5:%[0-9]+]]:er = COPY $r0
  ; CHECK-NEXT:   [[COPY6:%[0-9]+]]:mss = COPY [[COPY1]]
  ; CHECK-NEXT:   [[COPY7:%[0-9]+]]:mss = COPY [[COPY2]]
  ; CHECK-NEXT:   [[COPY8:%[0-9]+]]:mss = COPY [[COPY3]]
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.1:
  ; CHECK-NEXT:   successors: %bb.1(0x40000000), %bb.2(0x40000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[PHI:%[0-9]+]]:er = PHI [[MOV_RLC_imm10_pseudo]], %bb.0, %8, %bb.1
  ; CHECK-NEXT:   [[PHI1:%[0-9]+]]:ep = PHI [[COPY]], %bb.0, %10, %bb.1
  ; CHECK-NEXT:   [[VLDA_UPS_S64_D32_ag_idx_imm:%[0-9]+]]:acc512 = VLDA_UPS_S64_D32_ag_idx_imm [[COPY6]], [[PHI1]], 0, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<8 x s32>))
  ; CHECK-NEXT:   [[VLDA_UPS_S64_D32_ag_idx_imm1:%[0-9]+]]:acc512 = VLDA_UPS_S64_D32_ag_idx_imm [[COPY7]], [[PHI1]], 0, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<8 x s32>))
  ; CHECK-NEXT:   [[VLDA_UPS_S64_D32_ag_idx_imm2:%[0-9]+]]:acc512 = VLDA_UPS_S64_D32_ag_idx_imm [[COPY8]], [[PHI1]], 0, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<8 x s32>))
  ; CHECK-NEXT:   [[COPY9:%[0-9]+]]:mss = COPY [[COPY4]]
  ; CHECK-NEXT:   [[COPY10:%[0-9]+]]:mss = COPY [[COPY5]]
  ; CHECK-NEXT:   [[VLDA_UPS_S64_D32_ag_idx_imm3:%[0-9]+]]:acc512 = VLDA_UPS_S64_D32_ag_idx_imm [[COPY9]], [[PHI1]], 0, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<8 x s32>))
  ; CHECK-NEXT:   [[VLDA_UPS_S64_D32_ag_idx_imm4:%[0-9]+]]:acc512 = VLDA_UPS_S64_D32_ag_idx_imm [[COPY10]], [[PHI1]], 0, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<8 x s32>))
  ; CHECK-NEXT:   [[PADD_imm9_pseudo:%[0-9]+]]:ep = PADD_imm9_pseudo [[PHI1]], 32
  ; CHECK-NEXT:   [[ADD_add_r_ri:%[0-9]+]]:er = nsw ADD_add_r_ri [[PHI]], -1, implicit-def $srcarry
  ; CHECK-NEXT:   [[EQZ:%[0-9]+]]:er = EQZ [[ADD_add_r_ri]]
  ; CHECK-NEXT:   PseudoJZ [[EQZ]], %bb.1
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.2:
  ; CHECK-NEXT:   PseudoRET implicit $lr, implicit [[VLDA_UPS_S64_D32_ag_idx_imm]], implicit [[VLDA_UPS_S64_D32_ag_idx_imm1]], implicit [[VLDA_UPS_S64_D32_ag_idx_imm2]], implicit [[VLDA_UPS_S64_D32_ag_idx_imm3]]
  bb.1:
    liveins: $r0, $p0
    %1:ep = COPY $p0
    %19:er = MOV_RLC_imm10_pseudo 16
    %21:er = COPY $r0
    %22:er = COPY $r0
    %23:er = COPY $r0
    %24:er = COPY $r0
    %25:er = COPY $r0

  bb.3:
    %3:er = PHI %19, %bb.1, %16, %bb.3
    %4:ep = PHI %1, %bb.1, %8, %bb.3
    %71:mss = COPY %21
    %91:acc512 = VLDA_UPS_S64_D32_ag_idx_imm %71, %4, 0, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<8 x s32>))
    %72:mss = COPY %22
    %92:acc512 = VLDA_UPS_S64_D32_ag_idx_imm %72, %4, 0, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<8 x s32>))
    %73:mss = COPY %23
    %93:acc512 = VLDA_UPS_S64_D32_ag_idx_imm %73, %4, 0, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<8 x s32>))
    %74:mss = COPY %24
    %75:mss = COPY %25
    %94:acc512 = VLDA_UPS_S64_D32_ag_idx_imm %74, %4, 0, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<8 x s32>))
    %95:acc512 = VLDA_UPS_S64_D32_ag_idx_imm %75, %4, 0, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<8 x s32>))
    %8:ep = PADD_imm9_pseudo  %4, 32
    %16:er = nsw ADD_add_r_ri %3, -1, implicit-def $srcarry
    %17:er = EQZ %16
    PseudoJZ %17, %bb.3

  bb.2:
    PseudoRET implicit $lr, implicit %91, implicit %92, implicit %93, implicit %94
...
