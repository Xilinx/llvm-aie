# NOTE: Assertions have been autogenerated by utils/update_mir_test_checks.py
#
# This file is licensed under the Apache License v2.0 with LLVM Exceptions.
# See https://llvm.org/LICENSE.txt for license information.
# SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
#
# (c) Copyright 2023-2024 Advanced Micro Devices, Inc. or its affiliates
# RUN: llc -O2 --mtriple=aie2 --run-pass=instruction-select %s -o - | FileCheck %s

# This checks instruction selection of vector loads and stores that
# require multiple load/store instructions.
# This should generate the right addresses for the parts, and update the machine
# memory operands accordingly.
# We test representatives of 256, 512 and 1024 bit vectors

--- |
  define dso_local noundef <32 x i32> @loadv32int32(ptr noalias nocapture readonly %p, ptr noalias nocapture writeonly %q) {
  entry:
    %incdec.ptr = getelementptr inbounds <32 x i32>, ptr %p, i20 1
    %0 = load <32 x i32>, ptr %p, align 128, !tbaa !0
    store <32 x i32> %0, ptr %q, align 128, !tbaa !0
    %1 = load <32 x i32>, ptr %incdec.ptr, align 128, !tbaa !0
    ret <32 x i32> %1
  }

  define dso_local noundef <32 x i16> @loadv32int16(ptr noalias nocapture readonly %p, ptr noalias nocapture writeonly %q) {
  entry:
    %incdec.ptr = getelementptr inbounds <32 x i16>, ptr %p, i20 1
    %0 = load <32 x i16>, ptr %p, align 64, !tbaa !0
    store <32 x i16> %0, ptr %q, align 64, !tbaa !0
    %1 = load <32 x i16>, ptr %incdec.ptr, align 64, !tbaa !0
    ret <32 x i16> %1
  }

  define dso_local noundef <32 x i8> @loadv32int8(ptr noalias nocapture readonly %p, ptr noalias nocapture writeonly %q) {
  entry:
    %incdec.ptr = getelementptr inbounds <32 x i8>, ptr %p, i20 1
    %0 = load <32 x i8>, ptr %p, align 32, !tbaa !0
    store <32 x i8> %0, ptr %q, align 32, !tbaa !0
    %1 = load <32 x i8>, ptr %incdec.ptr, align 32, !tbaa !0
    ret <32 x i8> %1
  }

  !0 = !{!1, !1, i64 0}
  !1 = !{!"omnipotent char", !2, i64 0}
  !2 = !{!"Simple C++ TBAA"}

...

---
name:            loadv32int32
alignment:       16
legalized:       true
regBankSelected: true
selected:        false
body:             |
  bb.1.entry:
    liveins: $p0, $p1

    ; CHECK-LABEL: name: loadv32int32
    ; CHECK: liveins: $p0, $p1
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: [[COPY:%[0-9]+]]:ep = COPY $p0
    ; CHECK-NEXT: [[COPY1:%[0-9]+]]:ep = COPY $p1
    ; CHECK-NEXT: [[PADD_imm_pseudo:%[0-9]+]]:ep = PADD_imm_pseudo [[COPY]], 128
    ; CHECK-NEXT: [[VLDA_dmw_lda_w_ag_idx_imm:%[0-9]+]]:mwa = VLDA_dmw_lda_w_ag_idx_imm [[COPY]], 96 :: (load (<32 x s32>) from %ir.p, !tbaa !0)
    ; CHECK-NEXT: [[VLDA_dmw_lda_w_ag_idx_imm1:%[0-9]+]]:mwa = VLDA_dmw_lda_w_ag_idx_imm [[COPY]], 64 :: (load (<32 x s32>) from %ir.p, !tbaa !0)
    ; CHECK-NEXT: [[DEF:%[0-9]+]]:mshfldst = IMPLICIT_DEF
    ; CHECK-NEXT: [[INSERT_SUBREG:%[0-9]+]]:mshfldst = INSERT_SUBREG [[DEF]], [[VLDA_dmw_lda_w_ag_idx_imm1]], %subreg.sub_256_lo
    ; CHECK-NEXT: [[INSERT_SUBREG1:%[0-9]+]]:mshfldst = INSERT_SUBREG [[INSERT_SUBREG]], [[VLDA_dmw_lda_w_ag_idx_imm]], %subreg.sub_256_hi
    ; CHECK-NEXT: [[VLDA_dmw_lda_w_ag_idx_imm2:%[0-9]+]]:mwa = VLDA_dmw_lda_w_ag_idx_imm [[COPY]], 32 :: (load (<32 x s32>) from %ir.p, !tbaa !0)
    ; CHECK-NEXT: [[VLDA_dmw_lda_w_ag_idx_imm3:%[0-9]+]]:mwa = VLDA_dmw_lda_w_ag_idx_imm [[COPY]], 0 :: (load (<32 x s32>) from %ir.p, !tbaa !0)
    ; CHECK-NEXT: [[DEF1:%[0-9]+]]:mshfldst = IMPLICIT_DEF
    ; CHECK-NEXT: [[INSERT_SUBREG2:%[0-9]+]]:mshfldst = INSERT_SUBREG [[DEF1]], [[VLDA_dmw_lda_w_ag_idx_imm3]], %subreg.sub_256_lo
    ; CHECK-NEXT: [[INSERT_SUBREG3:%[0-9]+]]:mshfldst = INSERT_SUBREG [[INSERT_SUBREG2]], [[VLDA_dmw_lda_w_ag_idx_imm2]], %subreg.sub_256_hi
    ; CHECK-NEXT: [[DEF2:%[0-9]+]]:vec1024 = IMPLICIT_DEF
    ; CHECK-NEXT: [[INSERT_SUBREG4:%[0-9]+]]:vec1024 = INSERT_SUBREG [[DEF2]], [[INSERT_SUBREG3]], %subreg.sub_512_lo
    ; CHECK-NEXT: [[INSERT_SUBREG5:%[0-9]+]]:vec1024 = INSERT_SUBREG [[INSERT_SUBREG4]], [[INSERT_SUBREG1]], %subreg.sub_512_hi
    ; CHECK-NEXT: [[COPY2:%[0-9]+]]:vec512 = COPY [[INSERT_SUBREG5]].sub_512_lo
    ; CHECK-NEXT: [[COPY3:%[0-9]+]]:vec512 = COPY [[INSERT_SUBREG5]].sub_512_hi
    ; CHECK-NEXT: [[COPY4:%[0-9]+]]:vec256 = COPY [[COPY3]].sub_256_lo
    ; CHECK-NEXT: [[COPY5:%[0-9]+]]:vec256 = COPY [[COPY3]].sub_256_hi
    ; CHECK-NEXT: VST_dmw_sts_w_ag_idx_imm [[COPY5]], [[COPY1]], 96 :: (store (<8 x s32>) into %ir.q + 96, basealign 128, !tbaa !0)
    ; CHECK-NEXT: VST_dmw_sts_w_ag_idx_imm [[COPY4]], [[COPY1]], 64 :: (store (<8 x s32>) into %ir.q + 64, align 64, basealign 128, !tbaa !0)
    ; CHECK-NEXT: [[COPY6:%[0-9]+]]:vec256 = COPY [[COPY2]].sub_256_lo
    ; CHECK-NEXT: [[COPY7:%[0-9]+]]:vec256 = COPY [[COPY2]].sub_256_hi
    ; CHECK-NEXT: VST_dmw_sts_w_ag_idx_imm [[COPY7]], [[COPY1]], 32 :: (store (<8 x s32>) into %ir.q + 32, basealign 128, !tbaa !0)
    ; CHECK-NEXT: VST_dmw_sts_w_ag_idx_imm [[COPY6]], [[COPY1]], 0 :: (store (<8 x s32>) into %ir.q, align 128, !tbaa !0)
    ; CHECK-NEXT: [[VLDA_dmw_lda_w_ag_idx_imm4:%[0-9]+]]:mwa = VLDA_dmw_lda_w_ag_idx_imm [[PADD_imm_pseudo]], 96 :: (load (<32 x s32>) from %ir.incdec.ptr, !tbaa !0)
    ; CHECK-NEXT: [[VLDA_dmw_lda_w_ag_idx_imm5:%[0-9]+]]:mwa = VLDA_dmw_lda_w_ag_idx_imm [[PADD_imm_pseudo]], 64 :: (load (<32 x s32>) from %ir.incdec.ptr, !tbaa !0)
    ; CHECK-NEXT: [[DEF3:%[0-9]+]]:mshfldst = IMPLICIT_DEF
    ; CHECK-NEXT: [[INSERT_SUBREG6:%[0-9]+]]:mshfldst = INSERT_SUBREG [[DEF3]], [[VLDA_dmw_lda_w_ag_idx_imm5]], %subreg.sub_256_lo
    ; CHECK-NEXT: [[INSERT_SUBREG7:%[0-9]+]]:mshfldst = INSERT_SUBREG [[INSERT_SUBREG6]], [[VLDA_dmw_lda_w_ag_idx_imm4]], %subreg.sub_256_hi
    ; CHECK-NEXT: [[VLDA_dmw_lda_w_ag_idx_imm6:%[0-9]+]]:mwa = VLDA_dmw_lda_w_ag_idx_imm [[PADD_imm_pseudo]], 32 :: (load (<32 x s32>) from %ir.incdec.ptr, !tbaa !0)
    ; CHECK-NEXT: [[VLDA_dmw_lda_w_ag_idx_imm7:%[0-9]+]]:mwa = VLDA_dmw_lda_w_ag_idx_imm [[PADD_imm_pseudo]], 0 :: (load (<32 x s32>) from %ir.incdec.ptr, !tbaa !0)
    ; CHECK-NEXT: [[DEF4:%[0-9]+]]:mshfldst = IMPLICIT_DEF
    ; CHECK-NEXT: [[INSERT_SUBREG8:%[0-9]+]]:mshfldst = INSERT_SUBREG [[DEF4]], [[VLDA_dmw_lda_w_ag_idx_imm7]], %subreg.sub_256_lo
    ; CHECK-NEXT: [[INSERT_SUBREG9:%[0-9]+]]:mshfldst = INSERT_SUBREG [[INSERT_SUBREG8]], [[VLDA_dmw_lda_w_ag_idx_imm6]], %subreg.sub_256_hi
    ; CHECK-NEXT: [[DEF5:%[0-9]+]]:vec1024 = IMPLICIT_DEF
    ; CHECK-NEXT: [[INSERT_SUBREG10:%[0-9]+]]:vec1024 = INSERT_SUBREG [[DEF5]], [[INSERT_SUBREG9]], %subreg.sub_512_lo
    ; CHECK-NEXT: [[INSERT_SUBREG11:%[0-9]+]]:vec1024 = INSERT_SUBREG [[INSERT_SUBREG10]], [[INSERT_SUBREG7]], %subreg.sub_512_hi
    ; CHECK-NEXT: $y2 = COPY [[INSERT_SUBREG11]]
    ; CHECK-NEXT: PseudoRET implicit $lr, implicit $y2
    %1:ptrregbank(p0) = COPY $p0
    %2:ptrregbank(p0) = COPY $p1
    %3:modregbank(s20) = G_CONSTANT i20 128
    %4:ptrregbank(p0) = G_PTR_ADD %1, %3(s20)
    %5:vregbank(<32 x s32>) = G_LOAD %1(p0) :: (load (<32 x s32>) from %ir.p, !tbaa !0)
    G_STORE %5(<32 x s32>), %2(p0) :: (store (<32 x s32>) into %ir.q, !tbaa !0)
    %0:vregbank(<32 x s32>) = G_LOAD %4(p0) :: (load (<32 x s32>) from %ir.incdec.ptr, !tbaa !0)
    $y2 = COPY %0(<32 x s32>)
    PseudoRET implicit $lr, implicit $y2

...

---
name:            loadv32int16
alignment:       16
legalized:       true
regBankSelected: true
selected:        false
body:             |
  bb.1.entry:
    liveins: $p0, $p1

    ; CHECK-LABEL: name: loadv32int16
    ; CHECK: liveins: $p0, $p1
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: [[COPY:%[0-9]+]]:ep = COPY $p0
    ; CHECK-NEXT: [[COPY1:%[0-9]+]]:ep = COPY $p1
    ; CHECK-NEXT: [[PADD_imm_pseudo:%[0-9]+]]:ep = PADD_imm_pseudo [[COPY]], 64
    ; CHECK-NEXT: [[VLDA_dmw_lda_w_ag_idx_imm:%[0-9]+]]:vec256 = VLDA_dmw_lda_w_ag_idx_imm [[COPY]], 32 :: (load (<16 x s16>) from %ir.p + 32, basealign 64, !tbaa !0)
    ; CHECK-NEXT: [[VLDA_dmw_lda_w_ag_idx_imm1:%[0-9]+]]:vec256 = VLDA_dmw_lda_w_ag_idx_imm [[COPY]], 0 :: (load (<16 x s16>) from %ir.p, align 64, !tbaa !0)
    ; CHECK-NEXT: [[REG_SEQUENCE:%[0-9]+]]:vec512 = REG_SEQUENCE [[VLDA_dmw_lda_w_ag_idx_imm1]], %subreg.sub_256_lo, [[VLDA_dmw_lda_w_ag_idx_imm]], %subreg.sub_256_hi
    ; CHECK-NEXT: [[COPY2:%[0-9]+]]:vec256 = COPY [[REG_SEQUENCE]].sub_256_lo
    ; CHECK-NEXT: [[COPY3:%[0-9]+]]:vec256 = COPY [[REG_SEQUENCE]].sub_256_hi
    ; CHECK-NEXT: VST_dmw_sts_w_ag_idx_imm [[COPY3]], [[COPY1]], 32 :: (store (<16 x s16>) into %ir.q + 32, basealign 64, !tbaa !0)
    ; CHECK-NEXT: VST_dmw_sts_w_ag_idx_imm [[COPY2]], [[COPY1]], 0 :: (store (<16 x s16>) into %ir.q, align 64, !tbaa !0)
    ; CHECK-NEXT: [[VLDA_dmw_lda_w_ag_idx_imm2:%[0-9]+]]:vec256 = VLDA_dmw_lda_w_ag_idx_imm [[PADD_imm_pseudo]], 32 :: (load (<16 x s16>) from %ir.incdec.ptr + 32, basealign 64, !tbaa !0)
    ; CHECK-NEXT: [[VLDA_dmw_lda_w_ag_idx_imm3:%[0-9]+]]:vec256 = VLDA_dmw_lda_w_ag_idx_imm [[PADD_imm_pseudo]], 0 :: (load (<16 x s16>) from %ir.incdec.ptr, align 64, !tbaa !0)
    ; CHECK-NEXT: [[REG_SEQUENCE1:%[0-9]+]]:vec512 = REG_SEQUENCE [[VLDA_dmw_lda_w_ag_idx_imm3]], %subreg.sub_256_lo, [[VLDA_dmw_lda_w_ag_idx_imm2]], %subreg.sub_256_hi
    ; CHECK-NEXT: $x0 = COPY [[REG_SEQUENCE1]]
    %1:ptrregbank(p0) = COPY $p0
    %2:ptrregbank(p0) = COPY $p1
    %3:modregbank(s20) = G_CONSTANT i20 64
    %4:ptrregbank(p0) = G_PTR_ADD %1, %3(s20)
    %5:vregbank(<32 x s16>) = G_LOAD %1(p0) :: (load (<32 x s16>) from %ir.p, !tbaa !0)
    G_STORE %5(<32 x s16>), %2(p0) :: (store (<32 x s16>) into %ir.q, !tbaa !0)
    %0:vregbank(<32 x s16>) = G_LOAD %4(p0) :: (load (<32 x s16>) from %ir.incdec.ptr, !tbaa !0)
    $x0 = COPY %0(<32 x s16>)

...

---
name:            loadv32int8
alignment:       16
legalized:       true
regBankSelected: true
selected:        false
body:             |
  bb.1.entry:
    liveins: $p0, $p1

    ; CHECK-LABEL: name: loadv32int8
    ; CHECK: liveins: $p0, $p1
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: [[COPY:%[0-9]+]]:ep = COPY $p0
    ; CHECK-NEXT: [[COPY1:%[0-9]+]]:ep = COPY $p1
    ; CHECK-NEXT: [[VLDA_dmw_lda_w_ag_idx_imm:%[0-9]+]]:vec256 = VLDA_dmw_lda_w_ag_idx_imm [[COPY]], 0 :: (load (<32 x s8>) from %ir.p, !tbaa !0)
    ; CHECK-NEXT: VST_dmw_sts_w_ag_idx_imm [[VLDA_dmw_lda_w_ag_idx_imm]], [[COPY1]], 0 :: (store (<32 x s8>) into %ir.q, !tbaa !0)
    ; CHECK-NEXT: [[VLDA_dmw_lda_w_ag_idx_imm1:%[0-9]+]]:vec256 = VLDA_dmw_lda_w_ag_idx_imm [[COPY]], 32 :: (load (<32 x s8>) from %ir.incdec.ptr, !tbaa !0)
    ; CHECK-NEXT: $wl0 = COPY [[VLDA_dmw_lda_w_ag_idx_imm1]]
    %1:ptrregbank(p0) = COPY $p0
    %2:ptrregbank(p0) = COPY $p1
    %3:modregbank(s20) = G_CONSTANT i20 32
    %5:vregbank(<32 x s8>) = G_LOAD %1(p0) :: (load (<32 x s8>) from %ir.p, !tbaa !0)
    G_STORE %5(<32 x s8>), %2(p0) :: (store (<32 x s8>) into %ir.q, !tbaa !0)
    %0:vregbank(<32 x s8>) = G_AIE_OFFSET_LOAD %1(p0), %3(s20) :: (load (<32 x s8>) from %ir.incdec.ptr, !tbaa !0)
    $wl0 = COPY %0(<32 x s8>)
...
