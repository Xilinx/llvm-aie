# NOTE: Assertions have been autogenerated by utils/update_mir_test_checks.py
#
# This file is licensed under the Apache License v2.0 with LLVM Exceptions.
# See https://llvm.org/LICENSE.txt for license information.
# SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
#
# (c) Copyright 2023-2024 Advanced Micro Devices, Inc. or its affiliates
# RUN: llc -mtriple aie2 -run-pass=regbankselect -regbankselect-fast %s -verify-machineinstrs -o - | FileCheck %s
# RUN: llc -mtriple aie2 -run-pass=regbankselect -regbankselect-greedy %s -verify-machineinstrs -o - | FileCheck %s

---
name:            VSRS_D8_S32_mv_w_srs
alignment:       16
legalized:       true
body:             |
  bb.1.entry:
    liveins: $cm0, $r0, $r1
    ; CHECK-LABEL: name: VSRS_D8_S32_mv_w_srs
    ; CHECK: liveins: $cm0, $r0, $r1
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: [[COPY:%[0-9]+]]:accregbank(<16 x s64>) = COPY $cm0
    ; CHECK-NEXT: [[COPY1:%[0-9]+]]:gprregbank(s32) = COPY $r0
    ; CHECK-NEXT: [[COPY2:%[0-9]+]]:gprregbank(s32) = COPY $r1
    ; CHECK-NEXT: [[INT:%[0-9]+]]:vregbank(<32 x s8>) = G_INTRINSIC intrinsic(@llvm.aie2.I256.v32.acc32.srs), [[COPY]](<16 x s64>), [[COPY1]](s32), [[COPY2]](s32)
    ; CHECK-NEXT: $wl0 = COPY [[INT]](<32 x s8>)
    ; CHECK-NEXT: PseudoRET implicit $lr, implicit $wl0
    %1:_(<16 x s64>) = COPY $cm0
    %2:_(s32) = COPY $r0
    %3:_(s32) = COPY $r1
    %0:_(<32 x s8>) = G_INTRINSIC intrinsic(@llvm.aie2.I256.v32.acc32.srs), %1:_(<16 x s64>), %2:_(s32), %3:_(s32)
    $wl0 = COPY %0:_(<32 x s8>)
    PseudoRET implicit $lr, implicit $wl0
...

---
name:            VSRS_D16_S64_mv_w_srs
alignment:       16
legalized:       true
body:             |
  bb.1.entry:
    liveins: $cm0, $r0, $r1
    ; CHECK-LABEL: name: VSRS_D16_S64_mv_w_srs
    ; CHECK: liveins: $cm0, $r0, $r1
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: [[COPY:%[0-9]+]]:accregbank(<16 x s64>) = COPY $cm0
    ; CHECK-NEXT: [[COPY1:%[0-9]+]]:gprregbank(s32) = COPY $r0
    ; CHECK-NEXT: [[COPY2:%[0-9]+]]:gprregbank(s32) = COPY $r1
    ; CHECK-NEXT: [[INT:%[0-9]+]]:vregbank(<16 x s16>) = G_INTRINSIC intrinsic(@llvm.aie2.I256.v16.acc64.srs), [[COPY]](<16 x s64>), [[COPY1]](s32), [[COPY2]](s32)
    ; CHECK-NEXT: $wl0 = COPY [[INT]](<16 x s16>)
    ; CHECK-NEXT: PseudoRET implicit $lr, implicit $wl0
    %1:_(<16 x s64>) = COPY $cm0
    %2:_(s32) = COPY $r0
    %3:_(s32) = COPY $r1
    %0:_(<16 x s16>) = G_INTRINSIC intrinsic(@llvm.aie2.I256.v16.acc64.srs), %1:_(<16 x s64>), %2:_(s32), %3:_(s32)
    $wl0 = COPY %0:_(<16 x s16>)
    PseudoRET implicit $lr, implicit $wl0
...

---
name:            VSRS_D16_S32_mv_x_srs
alignment:       16
legalized:       true
body:             |
  bb.1.entry:
    liveins: $cm0, $r0, $r1
    ; CHECK-LABEL: name: VSRS_D16_S32_mv_x_srs
    ; CHECK: liveins: $cm0, $r0, $r1
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: [[COPY:%[0-9]+]]:accregbank(<16 x s64>) = COPY $cm0
    ; CHECK-NEXT: [[COPY1:%[0-9]+]]:gprregbank(s32) = COPY $r0
    ; CHECK-NEXT: [[COPY2:%[0-9]+]]:gprregbank(s32) = COPY $r1
    ; CHECK-NEXT: [[INT:%[0-9]+]]:vregbank(<32 x s16>) = G_INTRINSIC intrinsic(@llvm.aie2.I512.v32.acc32.srs), [[COPY]](<16 x s64>), [[COPY1]](s32), [[COPY2]](s32)
    ; CHECK-NEXT: $x0 = COPY [[INT]](<32 x s16>)
    ; CHECK-NEXT: PseudoRET implicit $lr, implicit $x0
    %1:_(<16 x s64>) = COPY $cm0
    %2:_(s32) = COPY $r0
    %3:_(s32) = COPY $r1
    %0:_(<32 x s16>) = G_INTRINSIC intrinsic(@llvm.aie2.I512.v32.acc32.srs), %1:_(<16 x s64>), %2:_(s32), %3:_(s32)
    $x0 = COPY %0:_(<32 x s16>)
    PseudoRET implicit $lr, implicit $x0
...

---
name:            VSRS_D32_S64_mv_x_srs
alignment:       16
legalized:       true
body:             |
  bb.1.entry:
    liveins: $cm0, $r0, $r1
    ; CHECK-LABEL: name: VSRS_D32_S64_mv_x_srs
    ; CHECK: liveins: $cm0, $r0, $r1
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: [[COPY:%[0-9]+]]:accregbank(<16 x s64>) = COPY $cm0
    ; CHECK-NEXT: [[COPY1:%[0-9]+]]:gprregbank(s32) = COPY $r0
    ; CHECK-NEXT: [[COPY2:%[0-9]+]]:gprregbank(s32) = COPY $r1
    ; CHECK-NEXT: [[INT:%[0-9]+]]:vregbank(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.I512.v16.acc64.srs), [[COPY]](<16 x s64>), [[COPY1]](s32), [[COPY2]](s32)
    ; CHECK-NEXT: $x0 = COPY [[INT]](<16 x s32>)
    ; CHECK-NEXT: PseudoRET implicit $lr, implicit $x0
    %1:_(<16 x s64>) = COPY $cm0
    %2:_(s32) = COPY $r0
    %3:_(s32) = COPY $r1
    %0:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.I512.v16.acc64.srs), %1:_(<16 x s64>), %2:_(s32), %3:_(s32)
    $x0 = COPY %0:_(<16 x s32>)
    PseudoRET implicit $lr, implicit $x0
...

---
name:            VSRS_D32_S64_mv_w_srs
alignment:       16
legalized:       true
body:             |
  bb.1.entry:
    liveins: $bml0, $r0, $r1
    ; CHECK-LABEL: name: VSRS_D32_S64_mv_w_srs
    ; CHECK: liveins: $bml0, $r0, $r1
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: [[COPY:%[0-9]+]]:accregbank(<8 x s64>) = COPY $bml0
    ; CHECK-NEXT: [[COPY1:%[0-9]+]]:gprregbank(s32) = COPY $r0
    ; CHECK-NEXT: [[COPY2:%[0-9]+]]:gprregbank(s32) = COPY $r1
    ; CHECK-NEXT: [[INT:%[0-9]+]]:vregbank(<8 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.I256.v8.acc64.srs), [[COPY]](<8 x s64>), [[COPY1]](s32), [[COPY2]](s32)
    ; CHECK-NEXT: $wl0 = COPY [[INT]](<8 x s32>)
    ; CHECK-NEXT: PseudoRET implicit $lr, implicit $wl0
    %1:_(<8 x s64>) = COPY $bml0
    %2:_(s32) = COPY $r0
    %3:_(s32) = COPY $r1
    %0:_(<8 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.I256.v8.acc64.srs), %1:_(<8 x s64>), %2:_(s32), %3:_(s32)
    $wl0 = COPY %0:_(<8 x s32>)
    PseudoRET implicit $lr, implicit $wl0
...

---
name:            VSRS_D16_S32_mv_w_srs
alignment:       16
legalized:       true
body:             |
  bb.1.entry:
    liveins: $bml0, $r0, $r1
    ; CHECK-LABEL: name: VSRS_D16_S32_mv_w_srs
    ; CHECK: liveins: $bml0, $r0, $r1
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: [[COPY:%[0-9]+]]:accregbank(<8 x s64>) = COPY $bml0
    ; CHECK-NEXT: [[COPY1:%[0-9]+]]:gprregbank(s32) = COPY $r0
    ; CHECK-NEXT: [[COPY2:%[0-9]+]]:gprregbank(s32) = COPY $r1
    ; CHECK-NEXT: [[INT:%[0-9]+]]:vregbank(<16 x s16>) = G_INTRINSIC intrinsic(@llvm.aie2.I256.v16.acc32.srs), [[COPY]](<8 x s64>), [[COPY1]](s32), [[COPY2]](s32)
    ; CHECK-NEXT: $wl0 = COPY [[INT]](<16 x s16>)
    ; CHECK-NEXT: PseudoRET implicit $lr, implicit $wl0
    %1:_(<8 x s64>) = COPY $bml0
    %2:_(s32) = COPY $r0
    %3:_(s32) = COPY $r1
    %0:_(<16 x s16>) = G_INTRINSIC intrinsic(@llvm.aie2.I256.v16.acc32.srs), %1:_(<8 x s64>), %2:_(s32), %3:_(s32)
    $wl0 = COPY %0:_(<16 x s16>)
    PseudoRET implicit $lr, implicit $wl0
...

---
name:            VSRS_S8_S32_mv_w_srs
alignment:       16
legalized:       true
body:             |
  bb.1.entry:
    liveins: $cm0, $r0
    ; CHECK-LABEL: name: VSRS_S8_S32_mv_w_srs
    ; CHECK: liveins: $cm0, $r0
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: [[COPY:%[0-9]+]]:accregbank(<16 x s64>) = COPY $cm0
    ; CHECK-NEXT: [[COPY1:%[0-9]+]]:gprregbank(s32) = COPY $r0
    ; CHECK-NEXT: [[C:%[0-9]+]]:gprregbank(s32) = G_CONSTANT i32 1
    ; CHECK-NEXT: [[INT:%[0-9]+]]:vregbank(<32 x s8>) = G_INTRINSIC intrinsic(@llvm.aie2.I256.v32.acc32.srs), [[COPY]](<16 x s64>), [[COPY1]](s32), [[C]](s32)
    ; CHECK-NEXT: $wl0 = COPY [[INT]](<32 x s8>)
    ; CHECK-NEXT: PseudoRET implicit $lr, implicit $wl0
    %1:_(<16 x s64>) = COPY $cm0
    %2:_(s32) = COPY $r0
    %3:_(s32) = G_CONSTANT i32 1
    %0:_(<32 x s8>) = G_INTRINSIC intrinsic(@llvm.aie2.I256.v32.acc32.srs), %1:_(<16 x s64>), %2:_(s32), %3:_(s32)
    $wl0 = COPY %0:_(<32 x s8>)
    PseudoRET implicit $lr, implicit $wl0
...

---
name:            VSRS_S16_S64_mv_w_srs
alignment:       16
legalized:       true
body:             |
  bb.1.entry:
    liveins: $cm0, $r0
    ; CHECK-LABEL: name: VSRS_S16_S64_mv_w_srs
    ; CHECK: liveins: $cm0, $r0
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: [[COPY:%[0-9]+]]:accregbank(<16 x s64>) = COPY $cm0
    ; CHECK-NEXT: [[COPY1:%[0-9]+]]:gprregbank(s32) = COPY $r0
    ; CHECK-NEXT: [[C:%[0-9]+]]:gprregbank(s32) = G_CONSTANT i32 1
    ; CHECK-NEXT: [[INT:%[0-9]+]]:vregbank(<16 x s16>) = G_INTRINSIC intrinsic(@llvm.aie2.I256.v16.acc64.srs), [[COPY]](<16 x s64>), [[COPY1]](s32), [[C]](s32)
    ; CHECK-NEXT: $wl0 = COPY [[INT]](<16 x s16>)
    ; CHECK-NEXT: PseudoRET implicit $lr, implicit $wl0
    %1:_(<16 x s64>) = COPY $cm0
    %2:_(s32) = COPY $r0
    %3:_(s32) = G_CONSTANT i32 1
    %0:_(<16 x s16>) = G_INTRINSIC intrinsic(@llvm.aie2.I256.v16.acc64.srs), %1:_(<16 x s64>), %2:_(s32), %3:_(s32)
    $wl0 = COPY %0:_(<16 x s16>)
    PseudoRET implicit $lr, implicit $wl0
...

---
name:            VSRS_S16_S32_mv_x_srs
alignment:       16
legalized:       true
body:             |
  bb.1.entry:
    liveins: $cm0, $r0
    ; CHECK-LABEL: name: VSRS_S16_S32_mv_x_srs
    ; CHECK: liveins: $cm0, $r0
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: [[COPY:%[0-9]+]]:accregbank(<16 x s64>) = COPY $cm0
    ; CHECK-NEXT: [[COPY1:%[0-9]+]]:gprregbank(s32) = COPY $r0
    ; CHECK-NEXT: [[C:%[0-9]+]]:gprregbank(s32) = G_CONSTANT i32 1
    ; CHECK-NEXT: [[INT:%[0-9]+]]:vregbank(<32 x s16>) = G_INTRINSIC intrinsic(@llvm.aie2.I512.v32.acc32.srs), [[COPY]](<16 x s64>), [[COPY1]](s32), [[C]](s32)
    ; CHECK-NEXT: $x0 = COPY [[INT]](<32 x s16>)
    ; CHECK-NEXT: PseudoRET implicit $lr, implicit $x0
    %1:_(<16 x s64>) = COPY $cm0
    %2:_(s32) = COPY $r0
    %3:_(s32) = G_CONSTANT i32 1
    %0:_(<32 x s16>) = G_INTRINSIC intrinsic(@llvm.aie2.I512.v32.acc32.srs), %1:_(<16 x s64>), %2:_(s32), %3:_(s32)
    $x0 = COPY %0:_(<32 x s16>)
    PseudoRET implicit $lr, implicit $x0
...

---
name:            VSRS_S32_S64_mv_x_srs
alignment:       16
legalized:       true
body:             |
  bb.1.entry:
    liveins: $cm0, $r0
    ; CHECK-LABEL: name: VSRS_S32_S64_mv_x_srs
    ; CHECK: liveins: $cm0, $r0
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: [[COPY:%[0-9]+]]:accregbank(<16 x s64>) = COPY $cm0
    ; CHECK-NEXT: [[COPY1:%[0-9]+]]:gprregbank(s32) = COPY $r0
    ; CHECK-NEXT: [[C:%[0-9]+]]:gprregbank(s32) = G_CONSTANT i32 1
    ; CHECK-NEXT: [[INT:%[0-9]+]]:vregbank(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.I512.v16.acc64.srs), [[COPY]](<16 x s64>), [[COPY1]](s32), [[C]](s32)
    ; CHECK-NEXT: $x0 = COPY [[INT]](<16 x s32>)
    ; CHECK-NEXT: PseudoRET implicit $lr, implicit $x0
    %1:_(<16 x s64>) = COPY $cm0
    %2:_(s32) = COPY $r0
    %3:_(s32) = G_CONSTANT i32 1
    %0:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.I512.v16.acc64.srs), %1:_(<16 x s64>), %2:_(s32), %3:_(s32)
    $x0 = COPY %0:_(<16 x s32>)
    PseudoRET implicit $lr, implicit $x0
...

---
name:            VSRS_S32_S64_mv_w_srs
alignment:       16
legalized:       true
body:             |
  bb.1.entry:
    liveins: $bml0, $r0
    ; CHECK-LABEL: name: VSRS_S32_S64_mv_w_srs
    ; CHECK: liveins: $bml0, $r0
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: [[COPY:%[0-9]+]]:accregbank(<8 x s64>) = COPY $bml0
    ; CHECK-NEXT: [[COPY1:%[0-9]+]]:gprregbank(s32) = COPY $r0
    ; CHECK-NEXT: [[C:%[0-9]+]]:gprregbank(s32) = G_CONSTANT i32 1
    ; CHECK-NEXT: [[INT:%[0-9]+]]:vregbank(<8 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.I256.v8.acc64.srs), [[COPY]](<8 x s64>), [[COPY1]](s32), [[C]](s32)
    ; CHECK-NEXT: $wl0 = COPY [[INT]](<8 x s32>)
    ; CHECK-NEXT: PseudoRET implicit $lr, implicit $wl0
    %1:_(<8 x s64>) = COPY $bml0
    %2:_(s32) = COPY $r0
    %3:_(s32) = G_CONSTANT i32 1
    %0:_(<8 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.I256.v8.acc64.srs), %1:_(<8 x s64>), %2:_(s32), %3:_(s32)
    $wl0 = COPY %0:_(<8 x s32>)
    PseudoRET implicit $lr, implicit $wl0
...

---
name:            VSRS_S16_S32_mv_w_srs
alignment:       16
legalized:       true
body:             |
  bb.1.entry:
    liveins: $bml0, $r0
    ; CHECK-LABEL: name: VSRS_S16_S32_mv_w_srs
    ; CHECK: liveins: $bml0, $r0
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: [[COPY:%[0-9]+]]:accregbank(<8 x s64>) = COPY $bml0
    ; CHECK-NEXT: [[COPY1:%[0-9]+]]:gprregbank(s32) = COPY $r0
    ; CHECK-NEXT: [[C:%[0-9]+]]:gprregbank(s32) = G_CONSTANT i32 1
    ; CHECK-NEXT: [[INT:%[0-9]+]]:vregbank(<16 x s16>) = G_INTRINSIC intrinsic(@llvm.aie2.I256.v16.acc32.srs), [[COPY]](<8 x s64>), [[COPY1]](s32), [[C]](s32)
    ; CHECK-NEXT: $wl0 = COPY [[INT]](<16 x s16>)
    ; CHECK-NEXT: PseudoRET implicit $lr, implicit $wl0
    %1:_(<8 x s64>) = COPY $bml0
    %2:_(s32) = COPY $r0
    %3:_(s32) = G_CONSTANT i32 1
    %0:_(<16 x s16>) = G_INTRINSIC intrinsic(@llvm.aie2.I256.v16.acc32.srs), %1:_(<8 x s64>), %2:_(s32), %3:_(s32)
    $wl0 = COPY %0:_(<16 x s16>)
    PseudoRET implicit $lr, implicit $wl0
...
