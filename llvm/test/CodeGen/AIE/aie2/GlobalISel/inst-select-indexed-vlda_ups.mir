# NOTE: Assertions have been autogenerated by utils/update_mir_test_checks.py
#
# This file is licensed under the Apache License v2.0 with LLVM Exceptions.
# See https://llvm.org/LICENSE.txt for license information.
# SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
#
# (c) Copyright 2023-2024 Advanced Micro Devices, Inc. or its affiliates
# RUN: llc -mtriple aie2 -run-pass=instruction-select %s -verify-machineinstrs -o - | FileCheck %s

---
name:            VLDA_UPS_S32_D16
alignment:       16
legalized:       true
regBankSelected: true
body:             |
  bb.1.entry:
    liveins: $p0
    ; CHECK-LABEL: name: VLDA_UPS_S32_D16
    ; CHECK: liveins: $p0
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: [[COPY:%[0-9]+]]:ep = COPY $p0
    ; CHECK-NEXT: [[COPY1:%[0-9]+]]:edj = COPY $m0
    ; CHECK-NEXT: [[MOV_PD_imm10_pseudo:%[0-9]+]]:edj = MOV_PD_imm10_pseudo 16
    ; CHECK-NEXT: [[MOV_PD_imm10_pseudo1:%[0-9]+]]:edj = MOV_PD_imm10_pseudo 128
    ; CHECK-NEXT: [[COPY2:%[0-9]+]]:er = COPY $r1
    ; CHECK-NEXT: [[COPY3:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S32_D16_ag_idx:%[0-9]+]]:acc512 = VLDA_UPS_S32_D16_ag_idx [[COPY3]], [[COPY]], [[COPY1]], implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<16 x s16>))
    ; CHECK-NEXT: [[COPY4:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S32_D16_ag_idx_imm:%[0-9]+]]:acc512 = VLDA_UPS_S32_D16_ag_idx_imm [[COPY4]], [[COPY]], 0, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<16 x s16>))
    ; CHECK-NEXT: [[COPY5:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S32_D16_ag_idx1:%[0-9]+]]:acc512 = VLDA_UPS_S32_D16_ag_idx [[COPY5]], [[COPY]], [[MOV_PD_imm10_pseudo]], implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<16 x s16>))
    ; CHECK-NEXT: [[COPY6:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S32_D16_ag_idx_imm1:%[0-9]+]]:acc512 = VLDA_UPS_S32_D16_ag_idx_imm [[COPY6]], [[COPY]], -128, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<16 x s16>))
    ; CHECK-NEXT: [[COPY7:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S32_D16_ag_idx_imm2:%[0-9]+]]:acc512 = VLDA_UPS_S32_D16_ag_idx_imm [[COPY7]], [[COPY]], 96, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<16 x s16>))
    ; CHECK-NEXT: [[COPY8:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S32_D16_ag_idx2:%[0-9]+]]:acc512 = VLDA_UPS_S32_D16_ag_idx [[COPY8]], [[COPY]], [[MOV_PD_imm10_pseudo1]], implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<16 x s16>))
    ; CHECK-NEXT: PseudoRET implicit $lr, implicit [[VLDA_UPS_S32_D16_ag_idx]], implicit [[VLDA_UPS_S32_D16_ag_idx_imm]], implicit [[VLDA_UPS_S32_D16_ag_idx1]], implicit [[VLDA_UPS_S32_D16_ag_idx_imm1]], implicit [[VLDA_UPS_S32_D16_ag_idx_imm2]], implicit [[VLDA_UPS_S32_D16_ag_idx2]]
    %0:ptrregbank(p0) = COPY $p0
    %7:modregbank(s20) = COPY $m0
    %8:modregbank(s20) = G_CONSTANT i20 0
    %9:modregbank(s20) = G_CONSTANT i20 16
    %10:modregbank(s20) = G_CONSTANT i20 -128
    %11:modregbank(s20) = G_CONSTANT i20 96
    %12:modregbank(s20) = G_CONSTANT i20 128
    %101:gprregbank(s32) = COPY $r1
    %102:gprregbank(s32) = G_CONSTANT i32 0
    %25:vregbank(<16 x s16>) = G_AIE_OFFSET_LOAD %0, %7 :: (load (<16 x s16>))
    %103:accregbank(<8 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc32.v16.I256.ups), %25:vregbank(<16 x s16>), %101:gprregbank(s32), %102:gprregbank(s32)
    %26:vregbank(<16 x s16>) = G_AIE_OFFSET_LOAD %0, %8 :: (load (<16 x s16>))
    %104:accregbank(<8 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc32.v16.I256.ups), %26:vregbank(<16 x s16>), %101:gprregbank(s32), %102:gprregbank(s32)
    %27:vregbank(<16 x s16>) = G_AIE_OFFSET_LOAD %0, %9 :: (load (<16 x s16>))
    %105:accregbank(<8 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc32.v16.I256.ups), %27:vregbank(<16 x s16>), %101:gprregbank(s32), %102:gprregbank(s32)
    %28:vregbank(<16 x s16>) = G_AIE_OFFSET_LOAD %0, %10 :: (load (<16 x s16>))
    %106:accregbank(<8 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc32.v16.I256.ups), %28:vregbank(<16 x s16>), %101:gprregbank(s32), %102:gprregbank(s32)
    %29:vregbank(<16 x s16>) = G_AIE_OFFSET_LOAD %0, %11 :: (load (<16 x s16>))
    %107:accregbank(<8 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc32.v16.I256.ups), %29:vregbank(<16 x s16>), %101:gprregbank(s32), %102:gprregbank(s32)
    %30:vregbank(<16 x s16>) = G_AIE_OFFSET_LOAD %0, %12 :: (load (<16 x s16>))
    %108:accregbank(<8 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc32.v16.I256.ups), %30:vregbank(<16 x s16>), %101:gprregbank(s32), %102:gprregbank(s32)
    PseudoRET implicit $lr, implicit %103, implicit %104, implicit %105, implicit %106, implicit %107, implicit %108
...

---
name:            VLDA_UPS_S32_D16_dyn
alignment:       16
legalized:       true
regBankSelected: true
body:             |
  bb.1.entry:
    liveins: $p0
    ; CHECK-LABEL: name: VLDA_UPS_S32_D16_dyn
    ; CHECK: liveins: $p0
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: [[COPY:%[0-9]+]]:ep = COPY $p0
    ; CHECK-NEXT: [[COPY1:%[0-9]+]]:edj = COPY $m0
    ; CHECK-NEXT: [[MOV_PD_imm10_pseudo:%[0-9]+]]:edj = MOV_PD_imm10_pseudo 16
    ; CHECK-NEXT: [[MOV_PD_imm10_pseudo1:%[0-9]+]]:edj = MOV_PD_imm10_pseudo 128
    ; CHECK-NEXT: [[COPY2:%[0-9]+]]:er = COPY $r1
    ; CHECK-NEXT: [[COPY3:%[0-9]+]]:er = COPY $r1
    ; CHECK-NEXT: $crupssign = COPY [[COPY3]]
    ; CHECK-NEXT: [[COPY4:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S32_D16_ag_idx:%[0-9]+]]:acc512 = VLDA_UPS_S32_D16_ag_idx [[COPY4]], [[COPY]], [[COPY1]], implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<16 x s16>))
    ; CHECK-NEXT: $crupssign = MOV_scalar_imm10_pseudo 0
    ; CHECK-NEXT: $crupssign = COPY [[COPY3]]
    ; CHECK-NEXT: [[COPY5:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S32_D16_ag_idx_imm:%[0-9]+]]:acc512 = VLDA_UPS_S32_D16_ag_idx_imm [[COPY5]], [[COPY]], 0, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<16 x s16>))
    ; CHECK-NEXT: $crupssign = MOV_scalar_imm10_pseudo 0
    ; CHECK-NEXT: $crupssign = COPY [[COPY3]]
    ; CHECK-NEXT: [[COPY6:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S32_D16_ag_idx1:%[0-9]+]]:acc512 = VLDA_UPS_S32_D16_ag_idx [[COPY6]], [[COPY]], [[MOV_PD_imm10_pseudo]], implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<16 x s16>))
    ; CHECK-NEXT: $crupssign = MOV_scalar_imm10_pseudo 0
    ; CHECK-NEXT: $crupssign = COPY [[COPY3]]
    ; CHECK-NEXT: [[COPY7:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S32_D16_ag_idx_imm1:%[0-9]+]]:acc512 = VLDA_UPS_S32_D16_ag_idx_imm [[COPY7]], [[COPY]], -128, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<16 x s16>))
    ; CHECK-NEXT: $crupssign = MOV_scalar_imm10_pseudo 0
    ; CHECK-NEXT: $crupssign = COPY [[COPY3]]
    ; CHECK-NEXT: [[COPY8:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S32_D16_ag_idx_imm2:%[0-9]+]]:acc512 = VLDA_UPS_S32_D16_ag_idx_imm [[COPY8]], [[COPY]], 96, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<16 x s16>))
    ; CHECK-NEXT: $crupssign = MOV_scalar_imm10_pseudo 0
    ; CHECK-NEXT: $crupssign = COPY [[COPY3]]
    ; CHECK-NEXT: [[COPY9:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S32_D16_ag_idx2:%[0-9]+]]:acc512 = VLDA_UPS_S32_D16_ag_idx [[COPY9]], [[COPY]], [[MOV_PD_imm10_pseudo1]], implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<16 x s16>))
    ; CHECK-NEXT: $crupssign = MOV_scalar_imm10_pseudo 0
    ; CHECK-NEXT: PseudoRET implicit $lr, implicit [[VLDA_UPS_S32_D16_ag_idx]], implicit [[VLDA_UPS_S32_D16_ag_idx_imm]], implicit [[VLDA_UPS_S32_D16_ag_idx1]], implicit [[VLDA_UPS_S32_D16_ag_idx_imm1]], implicit [[VLDA_UPS_S32_D16_ag_idx_imm2]], implicit [[VLDA_UPS_S32_D16_ag_idx2]]
    %0:ptrregbank(p0) = COPY $p0
    %7:modregbank(s20) = COPY $m0
    %8:modregbank(s20) = G_CONSTANT i20 0
    %9:modregbank(s20) = G_CONSTANT i20 16
    %10:modregbank(s20) = G_CONSTANT i20 -128
    %11:modregbank(s20) = G_CONSTANT i20 96
    %12:modregbank(s20) = G_CONSTANT i20 128
    %101:gprregbank(s32) = COPY $r1
    %102:gprregbank(s32) = COPY $r1
    %25:vregbank(<16 x s16>) = G_AIE_OFFSET_LOAD %0, %7 :: (load (<16 x s16>))
    %103:accregbank(<8 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc32.v16.I256.ups), %25:vregbank(<16 x s16>), %101:gprregbank(s32), %102:gprregbank(s32)
    %26:vregbank(<16 x s16>) = G_AIE_OFFSET_LOAD %0, %8 :: (load (<16 x s16>))
    %104:accregbank(<8 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc32.v16.I256.ups), %26:vregbank(<16 x s16>), %101:gprregbank(s32), %102:gprregbank(s32)
    %27:vregbank(<16 x s16>) = G_AIE_OFFSET_LOAD %0, %9 :: (load (<16 x s16>))
    %105:accregbank(<8 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc32.v16.I256.ups), %27:vregbank(<16 x s16>), %101:gprregbank(s32), %102:gprregbank(s32)
    %28:vregbank(<16 x s16>) = G_AIE_OFFSET_LOAD %0, %10 :: (load (<16 x s16>))
    %106:accregbank(<8 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc32.v16.I256.ups), %28:vregbank(<16 x s16>), %101:gprregbank(s32), %102:gprregbank(s32)
    %29:vregbank(<16 x s16>) = G_AIE_OFFSET_LOAD %0, %11 :: (load (<16 x s16>))
    %107:accregbank(<8 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc32.v16.I256.ups), %29:vregbank(<16 x s16>), %101:gprregbank(s32), %102:gprregbank(s32)
    %30:vregbank(<16 x s16>) = G_AIE_OFFSET_LOAD %0, %12 :: (load (<16 x s16>))
    %108:accregbank(<8 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc32.v16.I256.ups), %30:vregbank(<16 x s16>), %101:gprregbank(s32), %102:gprregbank(s32)
    PseudoRET implicit $lr, implicit %103, implicit %104, implicit %105, implicit %106, implicit %107, implicit %108
...

---
name:            VLDA_UPS_S32_S16
alignment:       16
legalized:       true
regBankSelected: true
body:             |
  bb.1.entry:
    liveins: $p0
    ; CHECK-LABEL: name: VLDA_UPS_S32_S16
    ; CHECK: liveins: $p0
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: [[COPY:%[0-9]+]]:ep = COPY $p0
    ; CHECK-NEXT: [[COPY1:%[0-9]+]]:edj = COPY $m0
    ; CHECK-NEXT: [[MOV_PD_imm10_pseudo:%[0-9]+]]:edj = MOV_PD_imm10_pseudo 16
    ; CHECK-NEXT: [[MOV_PD_imm10_pseudo1:%[0-9]+]]:edj = MOV_PD_imm10_pseudo 128
    ; CHECK-NEXT: [[COPY2:%[0-9]+]]:er = COPY $r1
    ; CHECK-NEXT: [[COPY3:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S32_S16_ag_idx:%[0-9]+]]:acc512 = VLDA_UPS_S32_S16_ag_idx [[COPY3]], [[COPY]], [[COPY1]], implicit-def $srups_of, implicit $crsat :: (load (<16 x s16>))
    ; CHECK-NEXT: [[COPY4:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S32_S16_ag_idx_imm:%[0-9]+]]:acc512 = VLDA_UPS_S32_S16_ag_idx_imm [[COPY4]], [[COPY]], 0, implicit-def $srups_of, implicit $crsat :: (load (<16 x s16>))
    ; CHECK-NEXT: [[COPY5:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S32_S16_ag_idx1:%[0-9]+]]:acc512 = VLDA_UPS_S32_S16_ag_idx [[COPY5]], [[COPY]], [[MOV_PD_imm10_pseudo]], implicit-def $srups_of, implicit $crsat :: (load (<16 x s16>))
    ; CHECK-NEXT: [[COPY6:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S32_S16_ag_idx_imm1:%[0-9]+]]:acc512 = VLDA_UPS_S32_S16_ag_idx_imm [[COPY6]], [[COPY]], -128, implicit-def $srups_of, implicit $crsat :: (load (<16 x s16>))
    ; CHECK-NEXT: [[COPY7:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S32_S16_ag_idx_imm2:%[0-9]+]]:acc512 = VLDA_UPS_S32_S16_ag_idx_imm [[COPY7]], [[COPY]], 96, implicit-def $srups_of, implicit $crsat :: (load (<16 x s16>))
    ; CHECK-NEXT: [[COPY8:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S32_S16_ag_idx2:%[0-9]+]]:acc512 = VLDA_UPS_S32_S16_ag_idx [[COPY8]], [[COPY]], [[MOV_PD_imm10_pseudo1]], implicit-def $srups_of, implicit $crsat :: (load (<16 x s16>))
    ; CHECK-NEXT: PseudoRET implicit $lr, implicit [[VLDA_UPS_S32_S16_ag_idx]], implicit [[VLDA_UPS_S32_S16_ag_idx_imm]], implicit [[VLDA_UPS_S32_S16_ag_idx1]], implicit [[VLDA_UPS_S32_S16_ag_idx_imm1]], implicit [[VLDA_UPS_S32_S16_ag_idx_imm2]], implicit [[VLDA_UPS_S32_S16_ag_idx2]]
    %0:ptrregbank(p0) = COPY $p0
    %7:modregbank(s20) = COPY $m0
    %8:modregbank(s20) = G_CONSTANT i20 0
    %9:modregbank(s20) = G_CONSTANT i20 16
    %10:modregbank(s20) = G_CONSTANT i20 -128
    %11:modregbank(s20) = G_CONSTANT i20 96
    %12:modregbank(s20) = G_CONSTANT i20 128
    %101:gprregbank(s32) = COPY $r1
    %102:gprregbank(s32) = G_CONSTANT i32 1
    %25:vregbank(<16 x s16>) = G_AIE_OFFSET_LOAD %0, %7 :: (load (<16 x s16>))
    %103:accregbank(<8 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc32.v16.I256.ups), %25:vregbank(<16 x s16>), %101:gprregbank(s32), %102:gprregbank(s32)
    %26:vregbank(<16 x s16>) = G_AIE_OFFSET_LOAD %0, %8 :: (load (<16 x s16>))
    %104:accregbank(<8 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc32.v16.I256.ups), %26:vregbank(<16 x s16>), %101:gprregbank(s32), %102:gprregbank(s32)
    %27:vregbank(<16 x s16>) = G_AIE_OFFSET_LOAD %0, %9 :: (load (<16 x s16>))
    %105:accregbank(<8 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc32.v16.I256.ups), %27:vregbank(<16 x s16>), %101:gprregbank(s32), %102:gprregbank(s32)
    %28:vregbank(<16 x s16>) = G_AIE_OFFSET_LOAD %0, %10 :: (load (<16 x s16>))
    %106:accregbank(<8 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc32.v16.I256.ups), %28:vregbank(<16 x s16>), %101:gprregbank(s32), %102:gprregbank(s32)
    %29:vregbank(<16 x s16>) = G_AIE_OFFSET_LOAD %0, %11 :: (load (<16 x s16>))
    %107:accregbank(<8 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc32.v16.I256.ups), %29:vregbank(<16 x s16>), %101:gprregbank(s32), %102:gprregbank(s32)
    %30:vregbank(<16 x s16>) = G_AIE_OFFSET_LOAD %0, %12 :: (load (<16 x s16>))
    %108:accregbank(<8 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc32.v16.I256.ups), %30:vregbank(<16 x s16>), %101:gprregbank(s32), %102:gprregbank(s32)
    PseudoRET implicit $lr, implicit %103, implicit %104, implicit %105, implicit %106, implicit %107, implicit %108
...


---
name:            VLDA_UPS_S64_D32
alignment:       16
legalized:       true
regBankSelected: true
body:             |
  bb.1.entry:
    liveins: $p0
    ; CHECK-LABEL: name: VLDA_UPS_S64_D32
    ; CHECK: liveins: $p0
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: [[COPY:%[0-9]+]]:ep = COPY $p0
    ; CHECK-NEXT: [[COPY1:%[0-9]+]]:edj = COPY $m0
    ; CHECK-NEXT: [[MOV_PD_imm10_pseudo:%[0-9]+]]:edj = MOV_PD_imm10_pseudo 16
    ; CHECK-NEXT: [[MOV_PD_imm10_pseudo1:%[0-9]+]]:edj = MOV_PD_imm10_pseudo 128
    ; CHECK-NEXT: [[COPY2:%[0-9]+]]:er = COPY $r1
    ; CHECK-NEXT: [[COPY3:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S64_D32_ag_idx:%[0-9]+]]:acc512 = VLDA_UPS_S64_D32_ag_idx [[COPY3]], [[COPY]], [[COPY1]], implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<8 x s32>))
    ; CHECK-NEXT: [[COPY4:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S64_D32_ag_idx_imm:%[0-9]+]]:acc512 = VLDA_UPS_S64_D32_ag_idx_imm [[COPY4]], [[COPY]], 0, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<8 x s32>))
    ; CHECK-NEXT: [[COPY5:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S64_D32_ag_idx1:%[0-9]+]]:acc512 = VLDA_UPS_S64_D32_ag_idx [[COPY5]], [[COPY]], [[MOV_PD_imm10_pseudo]], implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<8 x s32>))
    ; CHECK-NEXT: [[COPY6:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S64_D32_ag_idx_imm1:%[0-9]+]]:acc512 = VLDA_UPS_S64_D32_ag_idx_imm [[COPY6]], [[COPY]], -128, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<8 x s32>))
    ; CHECK-NEXT: [[COPY7:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S64_D32_ag_idx_imm2:%[0-9]+]]:acc512 = VLDA_UPS_S64_D32_ag_idx_imm [[COPY7]], [[COPY]], 96, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<8 x s32>))
    ; CHECK-NEXT: [[COPY8:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S64_D32_ag_idx2:%[0-9]+]]:acc512 = VLDA_UPS_S64_D32_ag_idx [[COPY8]], [[COPY]], [[MOV_PD_imm10_pseudo1]], implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<8 x s32>))
    ; CHECK-NEXT: PseudoRET implicit $lr, implicit [[VLDA_UPS_S64_D32_ag_idx]], implicit [[VLDA_UPS_S64_D32_ag_idx_imm]], implicit [[VLDA_UPS_S64_D32_ag_idx1]], implicit [[VLDA_UPS_S64_D32_ag_idx_imm1]], implicit [[VLDA_UPS_S64_D32_ag_idx_imm2]], implicit [[VLDA_UPS_S64_D32_ag_idx2]]
    %0:ptrregbank(p0) = COPY $p0
    %7:modregbank(s20) = COPY $m0
    %8:modregbank(s20) = G_CONSTANT i20 0
    %9:modregbank(s20) = G_CONSTANT i20 16
    %10:modregbank(s20) = G_CONSTANT i20 -128
    %11:modregbank(s20) = G_CONSTANT i20 96
    %12:modregbank(s20) = G_CONSTANT i20 128
    %101:gprregbank(s32) = COPY $r1
    %102:gprregbank(s32) = G_CONSTANT i32 0
    %25:vregbank(<8 x s32>) = G_AIE_OFFSET_LOAD %0, %7 :: (load (<8 x s32>))
    %103:accregbank(<8 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc64.v8.I256.ups), %25:vregbank(<8 x s32>), %101:gprregbank(s32), %102:gprregbank(s32)
    %26:vregbank(<8 x s32>) = G_AIE_OFFSET_LOAD %0, %8 :: (load (<8 x s32>))
    %104:accregbank(<8 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc64.v8.I256.ups), %26:vregbank(<8 x s32>), %101:gprregbank(s32), %102:gprregbank(s32)
    %27:vregbank(<8 x s32>) = G_AIE_OFFSET_LOAD %0, %9 :: (load (<8 x s32>))
    %105:accregbank(<8 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc64.v8.I256.ups), %27:vregbank(<8 x s32>), %101:gprregbank(s32), %102:gprregbank(s32)
    %28:vregbank(<8 x s32>) = G_AIE_OFFSET_LOAD %0, %10 :: (load (<8 x s32>))
    %106:accregbank(<8 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc64.v8.I256.ups), %28:vregbank(<8 x s32>), %101:gprregbank(s32), %102:gprregbank(s32)
    %29:vregbank(<8 x s32>) = G_AIE_OFFSET_LOAD %0, %11 :: (load (<8 x s32>))
    %107:accregbank(<8 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc64.v8.I256.ups), %29:vregbank(<8 x s32>), %101:gprregbank(s32), %102:gprregbank(s32)
    %30:vregbank(<8 x s32>) = G_AIE_OFFSET_LOAD %0, %12 :: (load (<8 x s32>))
    %108:accregbank(<8 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc64.v8.I256.ups), %30:vregbank(<8 x s32>), %101:gprregbank(s32), %102:gprregbank(s32)
    PseudoRET implicit $lr, implicit %103, implicit %104, implicit %105, implicit %106, implicit %107, implicit %108
...

---
name:            VLDA_UPS_S64_D32_dyn
alignment:       16
legalized:       true
regBankSelected: true
body:             |
  bb.1.entry:
    liveins: $p0
    ; CHECK-LABEL: name: VLDA_UPS_S64_D32_dyn
    ; CHECK: liveins: $p0
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: [[COPY:%[0-9]+]]:ep = COPY $p0
    ; CHECK-NEXT: [[COPY1:%[0-9]+]]:edj = COPY $m0
    ; CHECK-NEXT: [[MOV_PD_imm10_pseudo:%[0-9]+]]:edj = MOV_PD_imm10_pseudo 16
    ; CHECK-NEXT: [[MOV_PD_imm10_pseudo1:%[0-9]+]]:edj = MOV_PD_imm10_pseudo 128
    ; CHECK-NEXT: [[COPY2:%[0-9]+]]:er = COPY $r1
    ; CHECK-NEXT: [[COPY3:%[0-9]+]]:er = COPY $r1
    ; CHECK-NEXT: $crupssign = COPY [[COPY3]]
    ; CHECK-NEXT: [[COPY4:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S64_D32_ag_idx:%[0-9]+]]:acc512 = VLDA_UPS_S64_D32_ag_idx [[COPY4]], [[COPY]], [[COPY1]], implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<8 x s32>))
    ; CHECK-NEXT: $crupssign = MOV_scalar_imm10_pseudo 0
    ; CHECK-NEXT: $crupssign = COPY [[COPY3]]
    ; CHECK-NEXT: [[COPY5:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S64_D32_ag_idx_imm:%[0-9]+]]:acc512 = VLDA_UPS_S64_D32_ag_idx_imm [[COPY5]], [[COPY]], 0, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<8 x s32>))
    ; CHECK-NEXT: $crupssign = MOV_scalar_imm10_pseudo 0
    ; CHECK-NEXT: $crupssign = COPY [[COPY3]]
    ; CHECK-NEXT: [[COPY6:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S64_D32_ag_idx1:%[0-9]+]]:acc512 = VLDA_UPS_S64_D32_ag_idx [[COPY6]], [[COPY]], [[MOV_PD_imm10_pseudo]], implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<8 x s32>))
    ; CHECK-NEXT: $crupssign = MOV_scalar_imm10_pseudo 0
    ; CHECK-NEXT: $crupssign = COPY [[COPY3]]
    ; CHECK-NEXT: [[COPY7:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S64_D32_ag_idx_imm1:%[0-9]+]]:acc512 = VLDA_UPS_S64_D32_ag_idx_imm [[COPY7]], [[COPY]], -128, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<8 x s32>))
    ; CHECK-NEXT: $crupssign = MOV_scalar_imm10_pseudo 0
    ; CHECK-NEXT: $crupssign = COPY [[COPY3]]
    ; CHECK-NEXT: [[COPY8:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S64_D32_ag_idx_imm2:%[0-9]+]]:acc512 = VLDA_UPS_S64_D32_ag_idx_imm [[COPY8]], [[COPY]], 96, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<8 x s32>))
    ; CHECK-NEXT: $crupssign = MOV_scalar_imm10_pseudo 0
    ; CHECK-NEXT: $crupssign = COPY [[COPY3]]
    ; CHECK-NEXT: [[COPY9:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S64_D32_ag_idx2:%[0-9]+]]:acc512 = VLDA_UPS_S64_D32_ag_idx [[COPY9]], [[COPY]], [[MOV_PD_imm10_pseudo1]], implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<8 x s32>))
    ; CHECK-NEXT: $crupssign = MOV_scalar_imm10_pseudo 0
    ; CHECK-NEXT: PseudoRET implicit $lr, implicit [[VLDA_UPS_S64_D32_ag_idx]], implicit [[VLDA_UPS_S64_D32_ag_idx_imm]], implicit [[VLDA_UPS_S64_D32_ag_idx1]], implicit [[VLDA_UPS_S64_D32_ag_idx_imm1]], implicit [[VLDA_UPS_S64_D32_ag_idx_imm2]], implicit [[VLDA_UPS_S64_D32_ag_idx2]]
    %0:ptrregbank(p0) = COPY $p0
    %7:modregbank(s20) = COPY $m0
    %8:modregbank(s20) = G_CONSTANT i20 0
    %9:modregbank(s20) = G_CONSTANT i20 16
    %10:modregbank(s20) = G_CONSTANT i20 -128
    %11:modregbank(s20) = G_CONSTANT i20 96
    %12:modregbank(s20) = G_CONSTANT i20 128
    %101:gprregbank(s32) = COPY $r1
    %102:gprregbank(s32) = COPY $r1
    %25:vregbank(<8 x s32>) = G_AIE_OFFSET_LOAD %0, %7 :: (load (<8 x s32>))
    %103:accregbank(<8 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc64.v8.I256.ups), %25:vregbank(<8 x s32>), %101:gprregbank(s32), %102:gprregbank(s32)
    %26:vregbank(<8 x s32>) = G_AIE_OFFSET_LOAD %0, %8 :: (load (<8 x s32>))
    %104:accregbank(<8 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc64.v8.I256.ups), %26:vregbank(<8 x s32>), %101:gprregbank(s32), %102:gprregbank(s32)
    %27:vregbank(<8 x s32>) = G_AIE_OFFSET_LOAD %0, %9 :: (load (<8 x s32>))
    %105:accregbank(<8 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc64.v8.I256.ups), %27:vregbank(<8 x s32>), %101:gprregbank(s32), %102:gprregbank(s32)
    %28:vregbank(<8 x s32>) = G_AIE_OFFSET_LOAD %0, %10 :: (load (<8 x s32>))
    %106:accregbank(<8 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc64.v8.I256.ups), %28:vregbank(<8 x s32>), %101:gprregbank(s32), %102:gprregbank(s32)
    %29:vregbank(<8 x s32>) = G_AIE_OFFSET_LOAD %0, %11 :: (load (<8 x s32>))
    %107:accregbank(<8 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc64.v8.I256.ups), %29:vregbank(<8 x s32>), %101:gprregbank(s32), %102:gprregbank(s32)
    %30:vregbank(<8 x s32>) = G_AIE_OFFSET_LOAD %0, %12 :: (load (<8 x s32>))
    %108:accregbank(<8 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc64.v8.I256.ups), %30:vregbank(<8 x s32>), %101:gprregbank(s32), %102:gprregbank(s32)
    PseudoRET implicit $lr, implicit %103, implicit %104, implicit %105, implicit %106, implicit %107, implicit %108
...


---
name:            VLDA_UPS_S64_S32
alignment:       16
legalized:       true
regBankSelected: true
body:             |
  bb.1.entry:
    liveins: $p0
    ; CHECK-LABEL: name: VLDA_UPS_S64_S32
    ; CHECK: liveins: $p0
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: [[COPY:%[0-9]+]]:ep = COPY $p0
    ; CHECK-NEXT: [[COPY1:%[0-9]+]]:edj = COPY $m0
    ; CHECK-NEXT: [[MOV_PD_imm10_pseudo:%[0-9]+]]:edj = MOV_PD_imm10_pseudo 16
    ; CHECK-NEXT: [[MOV_PD_imm10_pseudo1:%[0-9]+]]:edj = MOV_PD_imm10_pseudo 128
    ; CHECK-NEXT: [[COPY2:%[0-9]+]]:er = COPY $r1
    ; CHECK-NEXT: [[COPY3:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S64_S32_ag_idx:%[0-9]+]]:acc512 = VLDA_UPS_S64_S32_ag_idx [[COPY3]], [[COPY]], [[COPY1]], implicit-def $srups_of, implicit $crsat :: (load (<8 x s32>))
    ; CHECK-NEXT: [[COPY4:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S64_S32_ag_idx_imm:%[0-9]+]]:acc512 = VLDA_UPS_S64_S32_ag_idx_imm [[COPY4]], [[COPY]], 0, implicit-def $srups_of, implicit $crsat :: (load (<8 x s32>))
    ; CHECK-NEXT: [[COPY5:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S64_S32_ag_idx1:%[0-9]+]]:acc512 = VLDA_UPS_S64_S32_ag_idx [[COPY5]], [[COPY]], [[MOV_PD_imm10_pseudo]], implicit-def $srups_of, implicit $crsat :: (load (<8 x s32>))
    ; CHECK-NEXT: [[COPY6:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S64_S32_ag_idx_imm1:%[0-9]+]]:acc512 = VLDA_UPS_S64_S32_ag_idx_imm [[COPY6]], [[COPY]], -128, implicit-def $srups_of, implicit $crsat :: (load (<8 x s32>))
    ; CHECK-NEXT: [[COPY7:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S64_S32_ag_idx_imm2:%[0-9]+]]:acc512 = VLDA_UPS_S64_S32_ag_idx_imm [[COPY7]], [[COPY]], 96, implicit-def $srups_of, implicit $crsat :: (load (<8 x s32>))
    ; CHECK-NEXT: [[COPY8:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S64_S32_ag_idx2:%[0-9]+]]:acc512 = VLDA_UPS_S64_S32_ag_idx [[COPY8]], [[COPY]], [[MOV_PD_imm10_pseudo1]], implicit-def $srups_of, implicit $crsat :: (load (<8 x s32>))
    ; CHECK-NEXT: PseudoRET implicit $lr, implicit [[VLDA_UPS_S64_S32_ag_idx]], implicit [[VLDA_UPS_S64_S32_ag_idx_imm]], implicit [[VLDA_UPS_S64_S32_ag_idx1]], implicit [[VLDA_UPS_S64_S32_ag_idx_imm1]], implicit [[VLDA_UPS_S64_S32_ag_idx_imm2]], implicit [[VLDA_UPS_S64_S32_ag_idx2]]
    %0:ptrregbank(p0) = COPY $p0
    %7:modregbank(s20) = COPY $m0
    %8:modregbank(s20) = G_CONSTANT i20 0
    %9:modregbank(s20) = G_CONSTANT i20 16
    %10:modregbank(s20) = G_CONSTANT i20 -128
    %11:modregbank(s20) = G_CONSTANT i20 96
    %12:modregbank(s20) = G_CONSTANT i20 128
    %101:gprregbank(s32) = COPY $r1
    %102:gprregbank(s32) = G_CONSTANT i32 1
    %25:vregbank(<8 x s32>) = G_AIE_OFFSET_LOAD %0, %7 :: (load (<8 x s32>))
    %103:accregbank(<8 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc64.v8.I256.ups), %25:vregbank(<8 x s32>), %101:gprregbank(s32), %102:gprregbank(s32)
    %26:vregbank(<8 x s32>) = G_AIE_OFFSET_LOAD %0, %8 :: (load (<8 x s32>))
    %104:accregbank(<8 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc64.v8.I256.ups), %26:vregbank(<8 x s32>), %101:gprregbank(s32), %102:gprregbank(s32)
    %27:vregbank(<8 x s32>) = G_AIE_OFFSET_LOAD %0, %9 :: (load (<8 x s32>))
    %105:accregbank(<8 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc64.v8.I256.ups), %27:vregbank(<8 x s32>), %101:gprregbank(s32), %102:gprregbank(s32)
    %28:vregbank(<8 x s32>) = G_AIE_OFFSET_LOAD %0, %10 :: (load (<8 x s32>))
    %106:accregbank(<8 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc64.v8.I256.ups), %28:vregbank(<8 x s32>), %101:gprregbank(s32), %102:gprregbank(s32)
    %29:vregbank(<8 x s32>) = G_AIE_OFFSET_LOAD %0, %11 :: (load (<8 x s32>))
    %107:accregbank(<8 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc64.v8.I256.ups), %29:vregbank(<8 x s32>), %101:gprregbank(s32), %102:gprregbank(s32)
    %30:vregbank(<8 x s32>) = G_AIE_OFFSET_LOAD %0, %12 :: (load (<8 x s32>))
    %108:accregbank(<8 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc64.v8.I256.ups), %30:vregbank(<8 x s32>), %101:gprregbank(s32), %102:gprregbank(s32)
    PseudoRET implicit $lr, implicit %103, implicit %104, implicit %105, implicit %106, implicit %107, implicit %108
...


---
name:            VLDA_UPS_S32_D8
alignment:       16
legalized:       true
regBankSelected: true
body:             |
  bb.1.entry:
    liveins: $p0
    ; CHECK-LABEL: name: VLDA_UPS_S32_D8
    ; CHECK: liveins: $p0
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: [[COPY:%[0-9]+]]:ep = COPY $p0
    ; CHECK-NEXT: [[COPY1:%[0-9]+]]:edj = COPY $m0
    ; CHECK-NEXT: [[MOV_PD_imm10_pseudo:%[0-9]+]]:edj = MOV_PD_imm10_pseudo 16
    ; CHECK-NEXT: [[MOV_PD_imm10_pseudo1:%[0-9]+]]:edj = MOV_PD_imm10_pseudo 128
    ; CHECK-NEXT: [[COPY2:%[0-9]+]]:er = COPY $r1
    ; CHECK-NEXT: [[COPY3:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S32_D8_ag_idx:%[0-9]+]]:acc1024 = VLDA_UPS_S32_D8_ag_idx [[COPY3]], [[COPY]], [[COPY1]], implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<32 x s8>))
    ; CHECK-NEXT: [[COPY4:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S32_D8_ag_idx_imm:%[0-9]+]]:acc1024 = VLDA_UPS_S32_D8_ag_idx_imm [[COPY4]], [[COPY]], 0, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<32 x s8>))
    ; CHECK-NEXT: [[COPY5:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S32_D8_ag_idx1:%[0-9]+]]:acc1024 = VLDA_UPS_S32_D8_ag_idx [[COPY5]], [[COPY]], [[MOV_PD_imm10_pseudo]], implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<32 x s8>))
    ; CHECK-NEXT: [[COPY6:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S32_D8_ag_idx_imm1:%[0-9]+]]:acc1024 = VLDA_UPS_S32_D8_ag_idx_imm [[COPY6]], [[COPY]], -128, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<32 x s8>))
    ; CHECK-NEXT: [[COPY7:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S32_D8_ag_idx_imm2:%[0-9]+]]:acc1024 = VLDA_UPS_S32_D8_ag_idx_imm [[COPY7]], [[COPY]], 96, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<32 x s8>))
    ; CHECK-NEXT: [[COPY8:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S32_D8_ag_idx2:%[0-9]+]]:acc1024 = VLDA_UPS_S32_D8_ag_idx [[COPY8]], [[COPY]], [[MOV_PD_imm10_pseudo1]], implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<32 x s8>))
    ; CHECK-NEXT: PseudoRET implicit $lr, implicit [[VLDA_UPS_S32_D8_ag_idx]], implicit [[VLDA_UPS_S32_D8_ag_idx_imm]], implicit [[VLDA_UPS_S32_D8_ag_idx1]], implicit [[VLDA_UPS_S32_D8_ag_idx_imm1]], implicit [[VLDA_UPS_S32_D8_ag_idx_imm2]], implicit [[VLDA_UPS_S32_D8_ag_idx2]]
    %0:ptrregbank(p0) = COPY $p0
    %7:modregbank(s20) = COPY $m0
    %8:modregbank(s20) = G_CONSTANT i20 0
    %9:modregbank(s20) = G_CONSTANT i20 16
    %10:modregbank(s20) = G_CONSTANT i20 -128
    %11:modregbank(s20) = G_CONSTANT i20 96
    %12:modregbank(s20) = G_CONSTANT i20 128
    %101:gprregbank(s32) = COPY $r1
    %102:gprregbank(s32) = G_CONSTANT i32 0
    %25:vregbank(<32 x s8>) = G_AIE_OFFSET_LOAD %0, %7 :: (load (<32 x s8>))
    %103:accregbank(<16 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc32.v32.I256.ups), %25:vregbank(<32 x s8>), %101:gprregbank(s32), %102:gprregbank(s32)
    %26:vregbank(<32 x s8>) = G_AIE_OFFSET_LOAD %0, %8 :: (load (<32 x s8>))
    %104:accregbank(<16 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc32.v32.I256.ups), %26:vregbank(<32 x s8>), %101:gprregbank(s32), %102:gprregbank(s32)
    %27:vregbank(<32 x s8>) = G_AIE_OFFSET_LOAD %0, %9 :: (load (<32 x s8>))
    %105:accregbank(<16 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc32.v32.I256.ups), %27:vregbank(<32 x s8>), %101:gprregbank(s32), %102:gprregbank(s32)
    %28:vregbank(<32 x s8>) = G_AIE_OFFSET_LOAD %0, %10 :: (load (<32 x s8>))
    %106:accregbank(<16 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc32.v32.I256.ups), %28:vregbank(<32 x s8>), %101:gprregbank(s32), %102:gprregbank(s32)
    %29:vregbank(<32 x s8>) = G_AIE_OFFSET_LOAD %0, %11 :: (load (<32 x s8>))
    %107:accregbank(<16 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc32.v32.I256.ups), %29:vregbank(<32 x s8>), %101:gprregbank(s32), %102:gprregbank(s32)
    %30:vregbank(<32 x s8>) = G_AIE_OFFSET_LOAD %0, %12 :: (load (<32 x s8>))
    %108:accregbank(<16 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc32.v32.I256.ups), %30:vregbank(<32 x s8>), %101:gprregbank(s32), %102:gprregbank(s32)
    PseudoRET implicit $lr, implicit %103, implicit %104, implicit %105, implicit %106, implicit %107, implicit %108
...

---
name:            VLDA_UPS_S32_D8_dyn
alignment:       16
legalized:       true
regBankSelected: true
body:             |
  bb.1.entry:
    liveins: $p0
    ; CHECK-LABEL: name: VLDA_UPS_S32_D8_dyn
    ; CHECK: liveins: $p0
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: [[COPY:%[0-9]+]]:ep = COPY $p0
    ; CHECK-NEXT: [[COPY1:%[0-9]+]]:edj = COPY $m0
    ; CHECK-NEXT: [[MOV_PD_imm10_pseudo:%[0-9]+]]:edj = MOV_PD_imm10_pseudo 16
    ; CHECK-NEXT: [[MOV_PD_imm10_pseudo1:%[0-9]+]]:edj = MOV_PD_imm10_pseudo 128
    ; CHECK-NEXT: [[COPY2:%[0-9]+]]:er = COPY $r1
    ; CHECK-NEXT: [[COPY3:%[0-9]+]]:er = COPY $r1
    ; CHECK-NEXT: $crupssign = COPY [[COPY3]]
    ; CHECK-NEXT: [[COPY4:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S32_D8_ag_idx:%[0-9]+]]:acc1024 = VLDA_UPS_S32_D8_ag_idx [[COPY4]], [[COPY]], [[COPY1]], implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<8 x s32>))
    ; CHECK-NEXT: $crupssign = MOV_scalar_imm10_pseudo 0
    ; CHECK-NEXT: $crupssign = COPY [[COPY3]]
    ; CHECK-NEXT: [[COPY5:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S32_D8_ag_idx_imm:%[0-9]+]]:acc1024 = VLDA_UPS_S32_D8_ag_idx_imm [[COPY5]], [[COPY]], 0, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<8 x s32>))
    ; CHECK-NEXT: $crupssign = MOV_scalar_imm10_pseudo 0
    ; CHECK-NEXT: $crupssign = COPY [[COPY3]]
    ; CHECK-NEXT: [[COPY6:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S32_D8_ag_idx1:%[0-9]+]]:acc1024 = VLDA_UPS_S32_D8_ag_idx [[COPY6]], [[COPY]], [[MOV_PD_imm10_pseudo]], implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<8 x s32>))
    ; CHECK-NEXT: $crupssign = MOV_scalar_imm10_pseudo 0
    ; CHECK-NEXT: $crupssign = COPY [[COPY3]]
    ; CHECK-NEXT: [[COPY7:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S32_D8_ag_idx_imm1:%[0-9]+]]:acc1024 = VLDA_UPS_S32_D8_ag_idx_imm [[COPY7]], [[COPY]], -128, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<8 x s32>))
    ; CHECK-NEXT: $crupssign = MOV_scalar_imm10_pseudo 0
    ; CHECK-NEXT: $crupssign = COPY [[COPY3]]
    ; CHECK-NEXT: [[COPY8:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S32_D8_ag_idx_imm2:%[0-9]+]]:acc1024 = VLDA_UPS_S32_D8_ag_idx_imm [[COPY8]], [[COPY]], 96, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<8 x s32>))
    ; CHECK-NEXT: $crupssign = MOV_scalar_imm10_pseudo 0
    ; CHECK-NEXT: $crupssign = COPY [[COPY3]]
    ; CHECK-NEXT: [[COPY9:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S32_D8_ag_idx2:%[0-9]+]]:acc1024 = VLDA_UPS_S32_D8_ag_idx [[COPY9]], [[COPY]], [[MOV_PD_imm10_pseudo1]], implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<8 x s32>))
    ; CHECK-NEXT: $crupssign = MOV_scalar_imm10_pseudo 0
    ; CHECK-NEXT: PseudoRET implicit $lr, implicit [[VLDA_UPS_S32_D8_ag_idx]], implicit [[VLDA_UPS_S32_D8_ag_idx_imm]], implicit [[VLDA_UPS_S32_D8_ag_idx1]], implicit [[VLDA_UPS_S32_D8_ag_idx_imm1]], implicit [[VLDA_UPS_S32_D8_ag_idx_imm2]], implicit [[VLDA_UPS_S32_D8_ag_idx2]]
    %0:ptrregbank(p0) = COPY $p0
    %7:modregbank(s20) = COPY $m0
    %8:modregbank(s20) = G_CONSTANT i20 0
    %9:modregbank(s20) = G_CONSTANT i20 16
    %10:modregbank(s20) = G_CONSTANT i20 -128
    %11:modregbank(s20) = G_CONSTANT i20 96
    %12:modregbank(s20) = G_CONSTANT i20 128
    %101:gprregbank(s32) = COPY $r1
    %102:gprregbank(s32) = COPY $r1
    %25:vregbank(<8 x s32>) = G_AIE_OFFSET_LOAD %0, %7 :: (load (<8 x s32>))
    %103:accregbank(<16 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc32.v32.I256.ups), %25:vregbank(<8 x s32>), %101:gprregbank(s32), %102:gprregbank(s32)
    %26:vregbank(<8 x s32>) = G_AIE_OFFSET_LOAD %0, %8 :: (load (<8 x s32>))
    %104:accregbank(<16 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc32.v32.I256.ups), %26:vregbank(<8 x s32>), %101:gprregbank(s32), %102:gprregbank(s32)
    %27:vregbank(<8 x s32>) = G_AIE_OFFSET_LOAD %0, %9 :: (load (<8 x s32>))
    %105:accregbank(<16 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc32.v32.I256.ups), %27:vregbank(<8 x s32>), %101:gprregbank(s32), %102:gprregbank(s32)
    %28:vregbank(<8 x s32>) = G_AIE_OFFSET_LOAD %0, %10 :: (load (<8 x s32>))
    %106:accregbank(<16 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc32.v32.I256.ups), %28:vregbank(<8 x s32>), %101:gprregbank(s32), %102:gprregbank(s32)
    %29:vregbank(<8 x s32>) = G_AIE_OFFSET_LOAD %0, %11 :: (load (<8 x s32>))
    %107:accregbank(<16 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc32.v32.I256.ups), %29:vregbank(<8 x s32>), %101:gprregbank(s32), %102:gprregbank(s32)
    %30:vregbank(<8 x s32>) = G_AIE_OFFSET_LOAD %0, %12 :: (load (<8 x s32>))
    %108:accregbank(<16 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc32.v32.I256.ups), %30:vregbank(<8 x s32>), %101:gprregbank(s32), %102:gprregbank(s32)
    PseudoRET implicit $lr, implicit %103, implicit %104, implicit %105, implicit %106, implicit %107, implicit %108
...


---
name:            VLDA_UPS_S32_S8
alignment:       16
legalized:       true
regBankSelected: true
body:             |
  bb.1.entry:
    liveins: $p0
    ; CHECK-LABEL: name: VLDA_UPS_S32_S8
    ; CHECK: liveins: $p0
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: [[COPY:%[0-9]+]]:ep = COPY $p0
    ; CHECK-NEXT: [[COPY1:%[0-9]+]]:edj = COPY $m0
    ; CHECK-NEXT: [[MOV_PD_imm10_pseudo:%[0-9]+]]:edj = MOV_PD_imm10_pseudo 16
    ; CHECK-NEXT: [[MOV_PD_imm10_pseudo1:%[0-9]+]]:edj = MOV_PD_imm10_pseudo 128
    ; CHECK-NEXT: [[COPY2:%[0-9]+]]:er = COPY $r1
    ; CHECK-NEXT: [[COPY3:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S32_S8_ag_idx:%[0-9]+]]:acc1024 = VLDA_UPS_S32_S8_ag_idx [[COPY3]], [[COPY]], [[COPY1]], implicit-def $srups_of, implicit $crsat :: (load (<32 x s8>))
    ; CHECK-NEXT: [[COPY4:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S32_S8_ag_idx_imm:%[0-9]+]]:acc1024 = VLDA_UPS_S32_S8_ag_idx_imm [[COPY4]], [[COPY]], 0, implicit-def $srups_of, implicit $crsat :: (load (<32 x s8>))
    ; CHECK-NEXT: [[COPY5:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S32_S8_ag_idx1:%[0-9]+]]:acc1024 = VLDA_UPS_S32_S8_ag_idx [[COPY5]], [[COPY]], [[MOV_PD_imm10_pseudo]], implicit-def $srups_of, implicit $crsat :: (load (<32 x s8>))
    ; CHECK-NEXT: [[COPY6:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S32_S8_ag_idx_imm1:%[0-9]+]]:acc1024 = VLDA_UPS_S32_S8_ag_idx_imm [[COPY6]], [[COPY]], -128, implicit-def $srups_of, implicit $crsat :: (load (<32 x s8>))
    ; CHECK-NEXT: [[COPY7:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S32_S8_ag_idx_imm2:%[0-9]+]]:acc1024 = VLDA_UPS_S32_S8_ag_idx_imm [[COPY7]], [[COPY]], 96, implicit-def $srups_of, implicit $crsat :: (load (<32 x s8>))
    ; CHECK-NEXT: [[COPY8:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S32_S8_ag_idx2:%[0-9]+]]:acc1024 = VLDA_UPS_S32_S8_ag_idx [[COPY8]], [[COPY]], [[MOV_PD_imm10_pseudo1]], implicit-def $srups_of, implicit $crsat :: (load (<32 x s8>))
    ; CHECK-NEXT: PseudoRET implicit $lr, implicit [[VLDA_UPS_S32_S8_ag_idx]], implicit [[VLDA_UPS_S32_S8_ag_idx_imm]], implicit [[VLDA_UPS_S32_S8_ag_idx1]], implicit [[VLDA_UPS_S32_S8_ag_idx_imm1]], implicit [[VLDA_UPS_S32_S8_ag_idx_imm2]], implicit [[VLDA_UPS_S32_S8_ag_idx2]]
    %0:ptrregbank(p0) = COPY $p0
    %7:modregbank(s20) = COPY $m0
    %8:modregbank(s20) = G_CONSTANT i20 0
    %9:modregbank(s20) = G_CONSTANT i20 16
    %10:modregbank(s20) = G_CONSTANT i20 -128
    %11:modregbank(s20) = G_CONSTANT i20 96
    %12:modregbank(s20) = G_CONSTANT i20 128
    %101:gprregbank(s32) = COPY $r1
    %102:gprregbank(s32) = G_CONSTANT i32 1
    %25:vregbank(<32 x s8>) = G_AIE_OFFSET_LOAD %0, %7 :: (load (<32 x s8>))
    %103:accregbank(<16 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc32.v32.I256.ups), %25:vregbank(<32 x s8>), %101:gprregbank(s32), %102:gprregbank(s32)
    %26:vregbank(<32 x s8>) = G_AIE_OFFSET_LOAD %0, %8 :: (load (<32 x s8>))
    %104:accregbank(<16 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc32.v32.I256.ups), %26:vregbank(<32 x s8>), %101:gprregbank(s32), %102:gprregbank(s32)
    %27:vregbank(<32 x s8>) = G_AIE_OFFSET_LOAD %0, %9 :: (load (<32 x s8>))
    %105:accregbank(<16 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc32.v32.I256.ups), %27:vregbank(<32 x s8>), %101:gprregbank(s32), %102:gprregbank(s32)
    %28:vregbank(<32 x s8>) = G_AIE_OFFSET_LOAD %0, %10 :: (load (<32 x s8>))
    %106:accregbank(<16 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc32.v32.I256.ups), %28:vregbank(<32 x s8>), %101:gprregbank(s32), %102:gprregbank(s32)
    %29:vregbank(<32 x s8>) = G_AIE_OFFSET_LOAD %0, %11 :: (load (<32 x s8>))
    %107:accregbank(<16 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc32.v32.I256.ups), %29:vregbank(<32 x s8>), %101:gprregbank(s32), %102:gprregbank(s32)
    %30:vregbank(<32 x s8>) = G_AIE_OFFSET_LOAD %0, %12 :: (load (<32 x s8>))
    %108:accregbank(<16 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc32.v32.I256.ups), %30:vregbank(<32 x s8>), %101:gprregbank(s32), %102:gprregbank(s32)
    PseudoRET implicit $lr, implicit %103, implicit %104, implicit %105, implicit %106, implicit %107, implicit %108
...


---
name:            VLDA_UPS_S64_D16
alignment:       16
legalized:       true
regBankSelected: true
body:             |
  bb.1.entry:
    liveins: $p0
    ; CHECK-LABEL: name: VLDA_UPS_S64_D16
    ; CHECK: liveins: $p0
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: [[COPY:%[0-9]+]]:ep = COPY $p0
    ; CHECK-NEXT: [[COPY1:%[0-9]+]]:edj = COPY $m0
    ; CHECK-NEXT: [[MOV_PD_imm10_pseudo:%[0-9]+]]:edj = MOV_PD_imm10_pseudo 16
    ; CHECK-NEXT: [[MOV_PD_imm10_pseudo1:%[0-9]+]]:edj = MOV_PD_imm10_pseudo 128
    ; CHECK-NEXT: [[COPY2:%[0-9]+]]:er = COPY $r1
    ; CHECK-NEXT: [[COPY3:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S64_D16_ag_idx:%[0-9]+]]:acc1024 = VLDA_UPS_S64_D16_ag_idx [[COPY3]], [[COPY]], [[COPY1]], implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<16 x s16>))
    ; CHECK-NEXT: [[COPY4:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S64_D16_ag_idx_imm:%[0-9]+]]:acc1024 = VLDA_UPS_S64_D16_ag_idx_imm [[COPY4]], [[COPY]], 0, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<16 x s16>))
    ; CHECK-NEXT: [[COPY5:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S64_D16_ag_idx1:%[0-9]+]]:acc1024 = VLDA_UPS_S64_D16_ag_idx [[COPY5]], [[COPY]], [[MOV_PD_imm10_pseudo]], implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<16 x s16>))
    ; CHECK-NEXT: [[COPY6:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S64_D16_ag_idx_imm1:%[0-9]+]]:acc1024 = VLDA_UPS_S64_D16_ag_idx_imm [[COPY6]], [[COPY]], -128, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<16 x s16>))
    ; CHECK-NEXT: [[COPY7:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S64_D16_ag_idx_imm2:%[0-9]+]]:acc1024 = VLDA_UPS_S64_D16_ag_idx_imm [[COPY7]], [[COPY]], 96, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<16 x s16>))
    ; CHECK-NEXT: [[COPY8:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S64_D16_ag_idx2:%[0-9]+]]:acc1024 = VLDA_UPS_S64_D16_ag_idx [[COPY8]], [[COPY]], [[MOV_PD_imm10_pseudo1]], implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<16 x s16>))
    ; CHECK-NEXT: PseudoRET implicit $lr, implicit [[VLDA_UPS_S64_D16_ag_idx]], implicit [[VLDA_UPS_S64_D16_ag_idx_imm]], implicit [[VLDA_UPS_S64_D16_ag_idx1]], implicit [[VLDA_UPS_S64_D16_ag_idx_imm1]], implicit [[VLDA_UPS_S64_D16_ag_idx_imm2]], implicit [[VLDA_UPS_S64_D16_ag_idx2]]
    %0:ptrregbank(p0) = COPY $p0
    %7:modregbank(s20) = COPY $m0
    %8:modregbank(s20) = G_CONSTANT i20 0
    %9:modregbank(s20) = G_CONSTANT i20 16
    %10:modregbank(s20) = G_CONSTANT i20 -128
    %11:modregbank(s20) = G_CONSTANT i20 96
    %12:modregbank(s20) = G_CONSTANT i20 128
    %101:gprregbank(s32) = COPY $r1
    %102:gprregbank(s32) = G_CONSTANT i32 0
    %25:vregbank(<16 x s16>) = G_AIE_OFFSET_LOAD %0, %7 :: (load (<16 x s16>))
    %103:accregbank(<16 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc64.v16.I256.ups), %25:vregbank(<16 x s16>), %101:gprregbank(s32), %102:gprregbank(s32)
    %26:vregbank(<16 x s16>) = G_AIE_OFFSET_LOAD %0, %8 :: (load (<16 x s16>))
    %104:accregbank(<16 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc64.v16.I256.ups), %26:vregbank(<16 x s16>), %101:gprregbank(s32), %102:gprregbank(s32)
    %27:vregbank(<16 x s16>) = G_AIE_OFFSET_LOAD %0, %9 :: (load (<16 x s16>))
    %105:accregbank(<16 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc64.v16.I256.ups), %27:vregbank(<16 x s16>), %101:gprregbank(s32), %102:gprregbank(s32)
    %28:vregbank(<16 x s16>) = G_AIE_OFFSET_LOAD %0, %10 :: (load (<16 x s16>))
    %106:accregbank(<16 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc64.v16.I256.ups), %28:vregbank(<16 x s16>), %101:gprregbank(s32), %102:gprregbank(s32)
    %29:vregbank(<16 x s16>) = G_AIE_OFFSET_LOAD %0, %11 :: (load (<16 x s16>))
    %107:accregbank(<16 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc64.v16.I256.ups), %29:vregbank(<16 x s16>), %101:gprregbank(s32), %102:gprregbank(s32)
    %30:vregbank(<16 x s16>) = G_AIE_OFFSET_LOAD %0, %12 :: (load (<16 x s16>))
    %108:accregbank(<16 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc64.v16.I256.ups), %30:vregbank(<16 x s16>), %101:gprregbank(s32), %102:gprregbank(s32)
    PseudoRET implicit $lr, implicit %103, implicit %104, implicit %105, implicit %106, implicit %107, implicit %108
...

---
name:            VLDA_UPS_S64_D16_dyn
alignment:       16
legalized:       true
regBankSelected: true
body:             |
  bb.1.entry:
    liveins: $p0
    ; CHECK-LABEL: name: VLDA_UPS_S64_D16_dyn
    ; CHECK: liveins: $p0
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: [[COPY:%[0-9]+]]:ep = COPY $p0
    ; CHECK-NEXT: [[COPY1:%[0-9]+]]:edj = COPY $m0
    ; CHECK-NEXT: [[MOV_PD_imm10_pseudo:%[0-9]+]]:edj = MOV_PD_imm10_pseudo 16
    ; CHECK-NEXT: [[MOV_PD_imm10_pseudo1:%[0-9]+]]:edj = MOV_PD_imm10_pseudo 128
    ; CHECK-NEXT: [[COPY2:%[0-9]+]]:er = COPY $r1
    ; CHECK-NEXT: [[COPY3:%[0-9]+]]:er = COPY $r1
    ; CHECK-NEXT: $crupssign = COPY [[COPY3]]
    ; CHECK-NEXT: [[COPY4:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S64_D16_ag_idx:%[0-9]+]]:acc1024 = VLDA_UPS_S64_D16_ag_idx [[COPY4]], [[COPY]], [[COPY1]], implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<16 x s16>))
    ; CHECK-NEXT: $crupssign = MOV_scalar_imm10_pseudo 0
    ; CHECK-NEXT: $crupssign = COPY [[COPY3]]
    ; CHECK-NEXT: [[COPY5:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S64_D16_ag_idx_imm:%[0-9]+]]:acc1024 = VLDA_UPS_S64_D16_ag_idx_imm [[COPY5]], [[COPY]], 0, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<16 x s16>))
    ; CHECK-NEXT: $crupssign = MOV_scalar_imm10_pseudo 0
    ; CHECK-NEXT: $crupssign = COPY [[COPY3]]
    ; CHECK-NEXT: [[COPY6:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S64_D16_ag_idx1:%[0-9]+]]:acc1024 = VLDA_UPS_S64_D16_ag_idx [[COPY6]], [[COPY]], [[MOV_PD_imm10_pseudo]], implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<16 x s16>))
    ; CHECK-NEXT: $crupssign = MOV_scalar_imm10_pseudo 0
    ; CHECK-NEXT: $crupssign = COPY [[COPY3]]
    ; CHECK-NEXT: [[COPY7:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S64_D16_ag_idx_imm1:%[0-9]+]]:acc1024 = VLDA_UPS_S64_D16_ag_idx_imm [[COPY7]], [[COPY]], -128, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<16 x s16>))
    ; CHECK-NEXT: $crupssign = MOV_scalar_imm10_pseudo 0
    ; CHECK-NEXT: $crupssign = COPY [[COPY3]]
    ; CHECK-NEXT: [[COPY8:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S64_D16_ag_idx_imm2:%[0-9]+]]:acc1024 = VLDA_UPS_S64_D16_ag_idx_imm [[COPY8]], [[COPY]], 96, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<16 x s16>))
    ; CHECK-NEXT: $crupssign = MOV_scalar_imm10_pseudo 0
    ; CHECK-NEXT: $crupssign = COPY [[COPY3]]
    ; CHECK-NEXT: [[COPY9:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S64_D16_ag_idx2:%[0-9]+]]:acc1024 = VLDA_UPS_S64_D16_ag_idx [[COPY9]], [[COPY]], [[MOV_PD_imm10_pseudo1]], implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<16 x s16>))
    ; CHECK-NEXT: $crupssign = MOV_scalar_imm10_pseudo 0
    ; CHECK-NEXT: PseudoRET implicit $lr, implicit [[VLDA_UPS_S64_D16_ag_idx]], implicit [[VLDA_UPS_S64_D16_ag_idx_imm]], implicit [[VLDA_UPS_S64_D16_ag_idx1]], implicit [[VLDA_UPS_S64_D16_ag_idx_imm1]], implicit [[VLDA_UPS_S64_D16_ag_idx_imm2]], implicit [[VLDA_UPS_S64_D16_ag_idx2]]
    %0:ptrregbank(p0) = COPY $p0
    %7:modregbank(s20) = COPY $m0
    %8:modregbank(s20) = G_CONSTANT i20 0
    %9:modregbank(s20) = G_CONSTANT i20 16
    %10:modregbank(s20) = G_CONSTANT i20 -128
    %11:modregbank(s20) = G_CONSTANT i20 96
    %12:modregbank(s20) = G_CONSTANT i20 128
    %101:gprregbank(s32) = COPY $r1
    %102:gprregbank(s32) = COPY $r1
    %25:vregbank(<16 x s16>) = G_AIE_OFFSET_LOAD %0, %7 :: (load (<16 x s16>))
    %103:accregbank(<16 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc64.v16.I256.ups), %25:vregbank(<16 x s16>), %101:gprregbank(s32), %102:gprregbank(s32)
    %26:vregbank(<16 x s16>) = G_AIE_OFFSET_LOAD %0, %8 :: (load (<16 x s16>))
    %104:accregbank(<16 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc64.v16.I256.ups), %26:vregbank(<16 x s16>), %101:gprregbank(s32), %102:gprregbank(s32)
    %27:vregbank(<16 x s16>) = G_AIE_OFFSET_LOAD %0, %9 :: (load (<16 x s16>))
    %105:accregbank(<16 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc64.v16.I256.ups), %27:vregbank(<16 x s16>), %101:gprregbank(s32), %102:gprregbank(s32)
    %28:vregbank(<16 x s16>) = G_AIE_OFFSET_LOAD %0, %10 :: (load (<16 x s16>))
    %106:accregbank(<16 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc64.v16.I256.ups), %28:vregbank(<16 x s16>), %101:gprregbank(s32), %102:gprregbank(s32)
    %29:vregbank(<16 x s16>) = G_AIE_OFFSET_LOAD %0, %11 :: (load (<16 x s16>))
    %107:accregbank(<16 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc64.v16.I256.ups), %29:vregbank(<16 x s16>), %101:gprregbank(s32), %102:gprregbank(s32)
    %30:vregbank(<16 x s16>) = G_AIE_OFFSET_LOAD %0, %12 :: (load (<16 x s16>))
    %108:accregbank(<16 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc64.v16.I256.ups), %30:vregbank(<16 x s16>), %101:gprregbank(s32), %102:gprregbank(s32)
    PseudoRET implicit $lr, implicit %103, implicit %104, implicit %105, implicit %106, implicit %107, implicit %108
...


---
name:            VLDA_UPS_S64_S16
alignment:       16
legalized:       true
regBankSelected: true
body:             |
  bb.1.entry:
    liveins: $p0
    ; CHECK-LABEL: name: VLDA_UPS_S64_S16
    ; CHECK: liveins: $p0
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: [[COPY:%[0-9]+]]:ep = COPY $p0
    ; CHECK-NEXT: [[COPY1:%[0-9]+]]:edj = COPY $m0
    ; CHECK-NEXT: [[MOV_PD_imm10_pseudo:%[0-9]+]]:edj = MOV_PD_imm10_pseudo 16
    ; CHECK-NEXT: [[MOV_PD_imm10_pseudo1:%[0-9]+]]:edj = MOV_PD_imm10_pseudo 128
    ; CHECK-NEXT: [[COPY2:%[0-9]+]]:er = COPY $r1
    ; CHECK-NEXT: [[COPY3:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S64_S16_ag_idx:%[0-9]+]]:acc1024 = VLDA_UPS_S64_S16_ag_idx [[COPY3]], [[COPY]], [[COPY1]], implicit-def $srups_of, implicit $crsat :: (load (<16 x s16>))
    ; CHECK-NEXT: [[COPY4:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S64_S16_ag_idx_imm:%[0-9]+]]:acc1024 = VLDA_UPS_S64_S16_ag_idx_imm [[COPY4]], [[COPY]], 0, implicit-def $srups_of, implicit $crsat :: (load (<16 x s16>))
    ; CHECK-NEXT: [[COPY5:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S64_S16_ag_idx1:%[0-9]+]]:acc1024 = VLDA_UPS_S64_S16_ag_idx [[COPY5]], [[COPY]], [[MOV_PD_imm10_pseudo]], implicit-def $srups_of, implicit $crsat :: (load (<16 x s16>))
    ; CHECK-NEXT: [[COPY6:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S64_S16_ag_idx_imm1:%[0-9]+]]:acc1024 = VLDA_UPS_S64_S16_ag_idx_imm [[COPY6]], [[COPY]], -128, implicit-def $srups_of, implicit $crsat :: (load (<16 x s16>))
    ; CHECK-NEXT: [[COPY7:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S64_S16_ag_idx_imm2:%[0-9]+]]:acc1024 = VLDA_UPS_S64_S16_ag_idx_imm [[COPY7]], [[COPY]], 96, implicit-def $srups_of, implicit $crsat :: (load (<16 x s16>))
    ; CHECK-NEXT: [[COPY8:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S64_S16_ag_idx2:%[0-9]+]]:acc1024 = VLDA_UPS_S64_S16_ag_idx [[COPY8]], [[COPY]], [[MOV_PD_imm10_pseudo1]], implicit-def $srups_of, implicit $crsat :: (load (<16 x s16>))
    ; CHECK-NEXT: PseudoRET implicit $lr, implicit [[VLDA_UPS_S64_S16_ag_idx]], implicit [[VLDA_UPS_S64_S16_ag_idx_imm]], implicit [[VLDA_UPS_S64_S16_ag_idx1]], implicit [[VLDA_UPS_S64_S16_ag_idx_imm1]], implicit [[VLDA_UPS_S64_S16_ag_idx_imm2]], implicit [[VLDA_UPS_S64_S16_ag_idx2]]
    %0:ptrregbank(p0) = COPY $p0
    %7:modregbank(s20) = COPY $m0
    %8:modregbank(s20) = G_CONSTANT i20 0
    %9:modregbank(s20) = G_CONSTANT i20 16
    %10:modregbank(s20) = G_CONSTANT i20 -128
    %11:modregbank(s20) = G_CONSTANT i20 96
    %12:modregbank(s20) = G_CONSTANT i20 128
    %101:gprregbank(s32) = COPY $r1
    %102:gprregbank(s32) = G_CONSTANT i32 1
    %25:vregbank(<16 x s16>) = G_AIE_OFFSET_LOAD %0, %7 :: (load (<16 x s16>))
    %103:accregbank(<16 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc64.v16.I256.ups), %25:vregbank(<16 x s16>), %101:gprregbank(s32), %102:gprregbank(s32)
    %26:vregbank(<16 x s16>) = G_AIE_OFFSET_LOAD %0, %8 :: (load (<16 x s16>))
    %104:accregbank(<16 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc64.v16.I256.ups), %26:vregbank(<16 x s16>), %101:gprregbank(s32), %102:gprregbank(s32)
    %27:vregbank(<16 x s16>) = G_AIE_OFFSET_LOAD %0, %9 :: (load (<16 x s16>))
    %105:accregbank(<16 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc64.v16.I256.ups), %27:vregbank(<16 x s16>), %101:gprregbank(s32), %102:gprregbank(s32)
    %28:vregbank(<16 x s16>) = G_AIE_OFFSET_LOAD %0, %10 :: (load (<16 x s16>))
    %106:accregbank(<16 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc64.v16.I256.ups), %28:vregbank(<16 x s16>), %101:gprregbank(s32), %102:gprregbank(s32)
    %29:vregbank(<16 x s16>) = G_AIE_OFFSET_LOAD %0, %11 :: (load (<16 x s16>))
    %107:accregbank(<16 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc64.v16.I256.ups), %29:vregbank(<16 x s16>), %101:gprregbank(s32), %102:gprregbank(s32)
    %30:vregbank(<16 x s16>) = G_AIE_OFFSET_LOAD %0, %12 :: (load (<16 x s16>))
    %108:accregbank(<16 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc64.v16.I256.ups), %30:vregbank(<16 x s16>), %101:gprregbank(s32), %102:gprregbank(s32)
    PseudoRET implicit $lr, implicit %103, implicit %104, implicit %105, implicit %106, implicit %107, implicit %108
...

---
name:            VLDA_UPS_S64_D32_512_bits
legalized:       true
regBankSelected: true
tracksRegLiveness: true
body: |
  bb.0:
    liveins: $p0, $m0, $r1, $amll0
    ; CHECK-LABEL: name: VLDA_UPS_S64_D32_512_bits
    ; CHECK: liveins: $p0, $m0, $r1, $amll0
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: [[COPY:%[0-9]+]]:ep = COPY $p0
    ; CHECK-NEXT: [[COPY1:%[0-9]+]]:em = COPY $m0
    ; CHECK-NEXT: [[COPY2:%[0-9]+]]:er = COPY $r1
    ; CHECK-NEXT: [[PADD_mod_pseudo:%[0-9]+]]:ep = PADD_mod_pseudo [[COPY]], [[COPY1]]
    ; CHECK-NEXT: [[COPY3:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S64_D32_ag_idx_imm:%[0-9]+]]:acc512 = VLDA_UPS_S64_D32_ag_idx_imm [[COPY3]], [[PADD_mod_pseudo]], 32, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<8 x s32>) from stack - 32, basealign 64)
    ; CHECK-NEXT: [[COPY4:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S64_D32_ag_idx_imm1:%[0-9]+]]:acc512 = VLDA_UPS_S64_D32_ag_idx_imm [[COPY4]], [[PADD_mod_pseudo]], 0, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<8 x s32>) from stack - 64, align 64)
    ; CHECK-NEXT: [[REG_SEQUENCE:%[0-9]+]]:acc1024 = REG_SEQUENCE [[VLDA_UPS_S64_D32_ag_idx_imm1]], %subreg.sub_512_lo, [[VLDA_UPS_S64_D32_ag_idx_imm]], %subreg.sub_512_hi
    ; CHECK-NEXT: [[COPY5:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S64_D32_ag_idx_imm2:%[0-9]+]]:acc512 = VLDA_UPS_S64_D32_ag_idx_imm [[COPY5]], [[COPY]], 32, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<8 x s32>) from stack - 32, basealign 64)
    ; CHECK-NEXT: [[COPY6:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S64_D32_ag_idx_imm3:%[0-9]+]]:acc512 = VLDA_UPS_S64_D32_ag_idx_imm [[COPY6]], [[COPY]], 0, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<8 x s32>) from stack - 64, align 64)
    ; CHECK-NEXT: [[REG_SEQUENCE1:%[0-9]+]]:acc1024 = REG_SEQUENCE [[VLDA_UPS_S64_D32_ag_idx_imm3]], %subreg.sub_512_lo, [[VLDA_UPS_S64_D32_ag_idx_imm2]], %subreg.sub_512_hi
    ; CHECK-NEXT: [[PADD_imm_pseudo:%[0-9]+]]:ep = PADD_imm_pseudo [[COPY]], 16
    ; CHECK-NEXT: [[COPY7:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S64_D32_ag_idx_imm4:%[0-9]+]]:acc512 = VLDA_UPS_S64_D32_ag_idx_imm [[COPY7]], [[PADD_imm_pseudo]], 32, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<8 x s32>) from stack - 32, basealign 64)
    ; CHECK-NEXT: [[COPY8:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S64_D32_ag_idx_imm5:%[0-9]+]]:acc512 = VLDA_UPS_S64_D32_ag_idx_imm [[COPY8]], [[PADD_imm_pseudo]], 0, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<8 x s32>) from stack - 64, align 64)
    ; CHECK-NEXT: [[REG_SEQUENCE2:%[0-9]+]]:acc1024 = REG_SEQUENCE [[VLDA_UPS_S64_D32_ag_idx_imm5]], %subreg.sub_512_lo, [[VLDA_UPS_S64_D32_ag_idx_imm4]], %subreg.sub_512_hi
    ; CHECK-NEXT: [[COPY9:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S64_D32_ag_idx_imm6:%[0-9]+]]:acc512 = VLDA_UPS_S64_D32_ag_idx_imm [[COPY9]], [[COPY]], -96, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<8 x s32>) from stack - 32, basealign 64)
    ; CHECK-NEXT: [[COPY10:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S64_D32_ag_idx_imm7:%[0-9]+]]:acc512 = VLDA_UPS_S64_D32_ag_idx_imm [[COPY10]], [[COPY]], -128, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<8 x s32>) from stack - 64, align 64)
    ; CHECK-NEXT: [[REG_SEQUENCE3:%[0-9]+]]:acc1024 = REG_SEQUENCE [[VLDA_UPS_S64_D32_ag_idx_imm7]], %subreg.sub_512_lo, [[VLDA_UPS_S64_D32_ag_idx_imm6]], %subreg.sub_512_hi
    ; CHECK-NEXT: [[COPY11:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S64_D32_ag_idx_imm8:%[0-9]+]]:acc512 = VLDA_UPS_S64_D32_ag_idx_imm [[COPY11]], [[COPY]], 96, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<8 x s32>) from stack - 32, basealign 64)
    ; CHECK-NEXT: [[COPY12:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S64_D32_ag_idx_imm9:%[0-9]+]]:acc512 = VLDA_UPS_S64_D32_ag_idx_imm [[COPY12]], [[COPY]], 64, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<8 x s32>) from stack - 64, align 64)
    ; CHECK-NEXT: [[REG_SEQUENCE4:%[0-9]+]]:acc1024 = REG_SEQUENCE [[VLDA_UPS_S64_D32_ag_idx_imm9]], %subreg.sub_512_lo, [[VLDA_UPS_S64_D32_ag_idx_imm8]], %subreg.sub_512_hi
    ; CHECK-NEXT: [[PADD_imm_pseudo1:%[0-9]+]]:ep = PADD_imm_pseudo [[COPY]], 96
    ; CHECK-NEXT: [[COPY13:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S64_D32_ag_idx_imm10:%[0-9]+]]:acc512 = VLDA_UPS_S64_D32_ag_idx_imm [[COPY13]], [[PADD_imm_pseudo1]], 32, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<8 x s32>) from stack - 32, basealign 64)
    ; CHECK-NEXT: [[COPY14:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S64_D32_ag_idx_imm11:%[0-9]+]]:acc512 = VLDA_UPS_S64_D32_ag_idx_imm [[COPY14]], [[PADD_imm_pseudo1]], 0, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<8 x s32>) from stack - 64, align 64)
    ; CHECK-NEXT: [[REG_SEQUENCE5:%[0-9]+]]:acc1024 = REG_SEQUENCE [[VLDA_UPS_S64_D32_ag_idx_imm11]], %subreg.sub_512_lo, [[VLDA_UPS_S64_D32_ag_idx_imm10]], %subreg.sub_512_hi
    ; CHECK-NEXT: PseudoRET implicit $lr, implicit [[REG_SEQUENCE]], implicit [[REG_SEQUENCE1]], implicit [[REG_SEQUENCE2]], implicit [[REG_SEQUENCE3]], implicit [[REG_SEQUENCE4]], implicit [[REG_SEQUENCE5]]
    %0:ptrregbank(p0) = COPY $p0
    %7:modregbank(s20) = COPY $m0
    %8:modregbank(s20) = G_CONSTANT i20 0
    %9:modregbank(s20) = G_CONSTANT i20 16
    %10:modregbank(s20) = G_CONSTANT i20 -128
    %11:modregbank(s20) = G_CONSTANT i20 64
    %12:modregbank(s20) = G_CONSTANT i20 96
    %101:gprregbank(s32) = COPY $r1
    %102:gprregbank(s32) = G_CONSTANT i32 0
    %25:vregbank(<16 x s32>) = G_AIE_OFFSET_LOAD %0, %7 :: (load (<16 x s32>) from stack - 64)
    %103:accregbank(<16 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc64.v16.I512.ups), %25:vregbank(<16 x s32>), %101:gprregbank(s32), %102:gprregbank(s32)
    %26:vregbank(<16 x s32>) = G_AIE_OFFSET_LOAD %0, %8 :: (load (<16 x s32>) from stack - 64)
    %104:accregbank(<16 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc64.v16.I512.ups), %26:vregbank(<16 x s32>), %101:gprregbank(s32), %102:gprregbank(s32)
    %27:vregbank(<16 x s32>) = G_AIE_OFFSET_LOAD %0, %9 :: (load (<16 x s32>) from stack - 64)
    %105:accregbank(<16 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc64.v16.I512.ups), %27:vregbank(<16 x s32>), %101:gprregbank(s32), %102:gprregbank(s32)
    %28:vregbank(<16 x s32>) = G_AIE_OFFSET_LOAD %0, %10 :: (load (<16 x s32>) from stack - 64)
    %106:accregbank(<16 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc64.v16.I512.ups), %28:vregbank(<16 x s32>), %101:gprregbank(s32), %102:gprregbank(s32)
    %29:vregbank(<16 x s32>) = G_AIE_OFFSET_LOAD %0, %11 :: (load (<16 x s32>) from stack - 64)
    %107:accregbank(<16 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc64.v16.I512.ups), %29:vregbank(<16 x s32>), %101:gprregbank(s32), %102:gprregbank(s32)
    %30:vregbank(<16 x s32>) = G_AIE_OFFSET_LOAD %0, %12 :: (load (<16 x s32>) from stack - 64)
    %108:accregbank(<16 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc64.v16.I512.ups), %30:vregbank(<16 x s32>), %101:gprregbank(s32), %102:gprregbank(s32)
    PseudoRET implicit $lr, implicit %103, implicit %104, implicit %105, implicit %106, implicit %107, implicit %108
...

---
name:            VLDA_UPS_S64_D32_512_bits_dyn
legalized:       true
regBankSelected: true
tracksRegLiveness: true
body: |
  bb.0:
    liveins: $p0, $m0, $r1, $amll0
    ; CHECK-LABEL: name: VLDA_UPS_S64_D32_512_bits_dyn
    ; CHECK: liveins: $p0, $m0, $r1, $amll0
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: [[COPY:%[0-9]+]]:ep = COPY $p0
    ; CHECK-NEXT: [[COPY1:%[0-9]+]]:em = COPY $m0
    ; CHECK-NEXT: [[COPY2:%[0-9]+]]:er = COPY $r1
    ; CHECK-NEXT: [[COPY3:%[0-9]+]]:er = COPY $r1
    ; CHECK-NEXT: [[PADD_mod_pseudo:%[0-9]+]]:ep = PADD_mod_pseudo [[COPY]], [[COPY1]]
    ; CHECK-NEXT: $crupssign = COPY [[COPY3]]
    ; CHECK-NEXT: [[COPY4:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S64_D32_ag_idx_imm:%[0-9]+]]:acc512 = VLDA_UPS_S64_D32_ag_idx_imm [[COPY4]], [[PADD_mod_pseudo]], 32, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<8 x s32>) from stack - 32, basealign 64)
    ; CHECK-NEXT: [[COPY5:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S64_D32_ag_idx_imm1:%[0-9]+]]:acc512 = VLDA_UPS_S64_D32_ag_idx_imm [[COPY5]], [[PADD_mod_pseudo]], 0, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<8 x s32>) from stack - 64, align 64)
    ; CHECK-NEXT: $crupssign = MOV_scalar_imm10_pseudo 0
    ; CHECK-NEXT: [[REG_SEQUENCE:%[0-9]+]]:acc1024 = REG_SEQUENCE [[VLDA_UPS_S64_D32_ag_idx_imm1]], %subreg.sub_512_lo, [[VLDA_UPS_S64_D32_ag_idx_imm]], %subreg.sub_512_hi
    ; CHECK-NEXT: $crupssign = COPY [[COPY3]]
    ; CHECK-NEXT: [[COPY6:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S64_D32_ag_idx_imm2:%[0-9]+]]:acc512 = VLDA_UPS_S64_D32_ag_idx_imm [[COPY6]], [[COPY]], 32, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<8 x s32>) from stack - 32, basealign 64)
    ; CHECK-NEXT: [[COPY7:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S64_D32_ag_idx_imm3:%[0-9]+]]:acc512 = VLDA_UPS_S64_D32_ag_idx_imm [[COPY7]], [[COPY]], 0, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<8 x s32>) from stack - 64, align 64)
    ; CHECK-NEXT: $crupssign = MOV_scalar_imm10_pseudo 0
    ; CHECK-NEXT: [[REG_SEQUENCE1:%[0-9]+]]:acc1024 = REG_SEQUENCE [[VLDA_UPS_S64_D32_ag_idx_imm3]], %subreg.sub_512_lo, [[VLDA_UPS_S64_D32_ag_idx_imm2]], %subreg.sub_512_hi
    ; CHECK-NEXT: [[PADD_imm_pseudo:%[0-9]+]]:ep = PADD_imm_pseudo [[COPY]], 16
    ; CHECK-NEXT: $crupssign = COPY [[COPY3]]
    ; CHECK-NEXT: [[COPY8:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S64_D32_ag_idx_imm4:%[0-9]+]]:acc512 = VLDA_UPS_S64_D32_ag_idx_imm [[COPY8]], [[PADD_imm_pseudo]], 32, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<8 x s32>) from stack - 32, basealign 64)
    ; CHECK-NEXT: [[COPY9:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S64_D32_ag_idx_imm5:%[0-9]+]]:acc512 = VLDA_UPS_S64_D32_ag_idx_imm [[COPY9]], [[PADD_imm_pseudo]], 0, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<8 x s32>) from stack - 64, align 64)
    ; CHECK-NEXT: $crupssign = MOV_scalar_imm10_pseudo 0
    ; CHECK-NEXT: [[REG_SEQUENCE2:%[0-9]+]]:acc1024 = REG_SEQUENCE [[VLDA_UPS_S64_D32_ag_idx_imm5]], %subreg.sub_512_lo, [[VLDA_UPS_S64_D32_ag_idx_imm4]], %subreg.sub_512_hi
    ; CHECK-NEXT: $crupssign = COPY [[COPY3]]
    ; CHECK-NEXT: [[COPY10:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S64_D32_ag_idx_imm6:%[0-9]+]]:acc512 = VLDA_UPS_S64_D32_ag_idx_imm [[COPY10]], [[COPY]], -96, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<8 x s32>) from stack - 32, basealign 64)
    ; CHECK-NEXT: [[COPY11:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S64_D32_ag_idx_imm7:%[0-9]+]]:acc512 = VLDA_UPS_S64_D32_ag_idx_imm [[COPY11]], [[COPY]], -128, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<8 x s32>) from stack - 64, align 64)
    ; CHECK-NEXT: $crupssign = MOV_scalar_imm10_pseudo 0
    ; CHECK-NEXT: [[REG_SEQUENCE3:%[0-9]+]]:acc1024 = REG_SEQUENCE [[VLDA_UPS_S64_D32_ag_idx_imm7]], %subreg.sub_512_lo, [[VLDA_UPS_S64_D32_ag_idx_imm6]], %subreg.sub_512_hi
    ; CHECK-NEXT: $crupssign = COPY [[COPY3]]
    ; CHECK-NEXT: [[COPY12:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S64_D32_ag_idx_imm8:%[0-9]+]]:acc512 = VLDA_UPS_S64_D32_ag_idx_imm [[COPY12]], [[COPY]], 96, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<8 x s32>) from stack - 32, basealign 64)
    ; CHECK-NEXT: [[COPY13:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S64_D32_ag_idx_imm9:%[0-9]+]]:acc512 = VLDA_UPS_S64_D32_ag_idx_imm [[COPY13]], [[COPY]], 64, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<8 x s32>) from stack - 64, align 64)
    ; CHECK-NEXT: $crupssign = MOV_scalar_imm10_pseudo 0
    ; CHECK-NEXT: [[REG_SEQUENCE4:%[0-9]+]]:acc1024 = REG_SEQUENCE [[VLDA_UPS_S64_D32_ag_idx_imm9]], %subreg.sub_512_lo, [[VLDA_UPS_S64_D32_ag_idx_imm8]], %subreg.sub_512_hi
    ; CHECK-NEXT: [[PADD_imm_pseudo1:%[0-9]+]]:ep = PADD_imm_pseudo [[COPY]], 96
    ; CHECK-NEXT: $crupssign = COPY [[COPY3]]
    ; CHECK-NEXT: [[COPY14:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S64_D32_ag_idx_imm10:%[0-9]+]]:acc512 = VLDA_UPS_S64_D32_ag_idx_imm [[COPY14]], [[PADD_imm_pseudo1]], 32, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<8 x s32>) from stack - 32, basealign 64)
    ; CHECK-NEXT: [[COPY15:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S64_D32_ag_idx_imm11:%[0-9]+]]:acc512 = VLDA_UPS_S64_D32_ag_idx_imm [[COPY15]], [[PADD_imm_pseudo1]], 0, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<8 x s32>) from stack - 64, align 64)
    ; CHECK-NEXT: $crupssign = MOV_scalar_imm10_pseudo 0
    ; CHECK-NEXT: [[REG_SEQUENCE5:%[0-9]+]]:acc1024 = REG_SEQUENCE [[VLDA_UPS_S64_D32_ag_idx_imm11]], %subreg.sub_512_lo, [[VLDA_UPS_S64_D32_ag_idx_imm10]], %subreg.sub_512_hi
    ; CHECK-NEXT: PseudoRET implicit $lr, implicit [[REG_SEQUENCE]], implicit [[REG_SEQUENCE1]], implicit [[REG_SEQUENCE2]], implicit [[REG_SEQUENCE3]], implicit [[REG_SEQUENCE4]], implicit [[REG_SEQUENCE5]]
    %0:ptrregbank(p0) = COPY $p0
    %7:modregbank(s20) = COPY $m0
    %8:modregbank(s20) = G_CONSTANT i20 0
    %9:modregbank(s20) = G_CONSTANT i20 16
    %10:modregbank(s20) = G_CONSTANT i20 -128
    %11:modregbank(s20) = G_CONSTANT i20 64
    %12:modregbank(s20) = G_CONSTANT i20 96
    %101:gprregbank(s32) = COPY $r1
    %102:gprregbank(s32) = COPY $r1
    %25:vregbank(<16 x s32>) = G_AIE_OFFSET_LOAD %0, %7 :: (load (<16 x s32>) from stack - 64)
    %103:accregbank(<16 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc64.v16.I512.ups), %25:vregbank(<16 x s32>), %101:gprregbank(s32), %102:gprregbank(s32)
    %26:vregbank(<16 x s32>) = G_AIE_OFFSET_LOAD %0, %8 :: (load (<16 x s32>) from stack - 64)
    %104:accregbank(<16 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc64.v16.I512.ups), %26:vregbank(<16 x s32>), %101:gprregbank(s32), %102:gprregbank(s32)
    %27:vregbank(<16 x s32>) = G_AIE_OFFSET_LOAD %0, %9 :: (load (<16 x s32>) from stack - 64)
    %105:accregbank(<16 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc64.v16.I512.ups), %27:vregbank(<16 x s32>), %101:gprregbank(s32), %102:gprregbank(s32)
    %28:vregbank(<16 x s32>) = G_AIE_OFFSET_LOAD %0, %10 :: (load (<16 x s32>) from stack - 64)
    %106:accregbank(<16 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc64.v16.I512.ups), %28:vregbank(<16 x s32>), %101:gprregbank(s32), %102:gprregbank(s32)
    %29:vregbank(<16 x s32>) = G_AIE_OFFSET_LOAD %0, %11 :: (load (<16 x s32>) from stack - 64)
    %107:accregbank(<16 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc64.v16.I512.ups), %29:vregbank(<16 x s32>), %101:gprregbank(s32), %102:gprregbank(s32)
    %30:vregbank(<16 x s32>) = G_AIE_OFFSET_LOAD %0, %12 :: (load (<16 x s32>) from stack - 64)
    %108:accregbank(<16 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc64.v16.I512.ups), %30:vregbank(<16 x s32>), %101:gprregbank(s32), %102:gprregbank(s32)
    PseudoRET implicit $lr, implicit %103, implicit %104, implicit %105, implicit %106, implicit %107, implicit %108
...

---
name:            VLDA_UPS_S64_S32_512_bits
legalized:       true
regBankSelected: true
tracksRegLiveness: true
body: |
  bb.0:
    liveins: $p0, $m0, $r1, $amll0
    ; CHECK-LABEL: name: VLDA_UPS_S64_S32_512_bits
    ; CHECK: liveins: $p0, $m0, $r1, $amll0
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: [[COPY:%[0-9]+]]:ep = COPY $p0
    ; CHECK-NEXT: [[COPY1:%[0-9]+]]:em = COPY $m0
    ; CHECK-NEXT: [[COPY2:%[0-9]+]]:er = COPY $r1
    ; CHECK-NEXT: [[PADD_mod_pseudo:%[0-9]+]]:ep = PADD_mod_pseudo [[COPY]], [[COPY1]]
    ; CHECK-NEXT: [[COPY3:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S64_S32_ag_idx_imm:%[0-9]+]]:acc512 = VLDA_UPS_S64_S32_ag_idx_imm [[COPY3]], [[PADD_mod_pseudo]], 32, implicit-def $srups_of, implicit $crsat :: (load (<8 x s32>) from stack - 32, basealign 64)
    ; CHECK-NEXT: [[COPY4:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S64_S32_ag_idx_imm1:%[0-9]+]]:acc512 = VLDA_UPS_S64_S32_ag_idx_imm [[COPY4]], [[PADD_mod_pseudo]], 0, implicit-def $srups_of, implicit $crsat :: (load (<8 x s32>) from stack - 64, align 64)
    ; CHECK-NEXT: [[REG_SEQUENCE:%[0-9]+]]:acc1024 = REG_SEQUENCE [[VLDA_UPS_S64_S32_ag_idx_imm1]], %subreg.sub_512_lo, [[VLDA_UPS_S64_S32_ag_idx_imm]], %subreg.sub_512_hi
    ; CHECK-NEXT: [[COPY5:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S64_S32_ag_idx_imm2:%[0-9]+]]:acc512 = VLDA_UPS_S64_S32_ag_idx_imm [[COPY5]], [[COPY]], 32, implicit-def $srups_of, implicit $crsat :: (load (<8 x s32>) from stack - 32, basealign 64)
    ; CHECK-NEXT: [[COPY6:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S64_S32_ag_idx_imm3:%[0-9]+]]:acc512 = VLDA_UPS_S64_S32_ag_idx_imm [[COPY6]], [[COPY]], 0, implicit-def $srups_of, implicit $crsat :: (load (<8 x s32>) from stack - 64, align 64)
    ; CHECK-NEXT: [[REG_SEQUENCE1:%[0-9]+]]:acc1024 = REG_SEQUENCE [[VLDA_UPS_S64_S32_ag_idx_imm3]], %subreg.sub_512_lo, [[VLDA_UPS_S64_S32_ag_idx_imm2]], %subreg.sub_512_hi
    ; CHECK-NEXT: [[PADD_imm_pseudo:%[0-9]+]]:ep = PADD_imm_pseudo [[COPY]], 16
    ; CHECK-NEXT: [[COPY7:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S64_S32_ag_idx_imm4:%[0-9]+]]:acc512 = VLDA_UPS_S64_S32_ag_idx_imm [[COPY7]], [[PADD_imm_pseudo]], 32, implicit-def $srups_of, implicit $crsat :: (load (<8 x s32>) from stack - 32, basealign 64)
    ; CHECK-NEXT: [[COPY8:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S64_S32_ag_idx_imm5:%[0-9]+]]:acc512 = VLDA_UPS_S64_S32_ag_idx_imm [[COPY8]], [[PADD_imm_pseudo]], 0, implicit-def $srups_of, implicit $crsat :: (load (<8 x s32>) from stack - 64, align 64)
    ; CHECK-NEXT: [[REG_SEQUENCE2:%[0-9]+]]:acc1024 = REG_SEQUENCE [[VLDA_UPS_S64_S32_ag_idx_imm5]], %subreg.sub_512_lo, [[VLDA_UPS_S64_S32_ag_idx_imm4]], %subreg.sub_512_hi
    ; CHECK-NEXT: [[COPY9:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S64_S32_ag_idx_imm6:%[0-9]+]]:acc512 = VLDA_UPS_S64_S32_ag_idx_imm [[COPY9]], [[COPY]], -96, implicit-def $srups_of, implicit $crsat :: (load (<8 x s32>) from stack - 32, basealign 64)
    ; CHECK-NEXT: [[COPY10:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S64_S32_ag_idx_imm7:%[0-9]+]]:acc512 = VLDA_UPS_S64_S32_ag_idx_imm [[COPY10]], [[COPY]], -128, implicit-def $srups_of, implicit $crsat :: (load (<8 x s32>) from stack - 64, align 64)
    ; CHECK-NEXT: [[REG_SEQUENCE3:%[0-9]+]]:acc1024 = REG_SEQUENCE [[VLDA_UPS_S64_S32_ag_idx_imm7]], %subreg.sub_512_lo, [[VLDA_UPS_S64_S32_ag_idx_imm6]], %subreg.sub_512_hi
    ; CHECK-NEXT: [[COPY11:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S64_S32_ag_idx_imm8:%[0-9]+]]:acc512 = VLDA_UPS_S64_S32_ag_idx_imm [[COPY11]], [[COPY]], 96, implicit-def $srups_of, implicit $crsat :: (load (<8 x s32>) from stack - 32, basealign 64)
    ; CHECK-NEXT: [[COPY12:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S64_S32_ag_idx_imm9:%[0-9]+]]:acc512 = VLDA_UPS_S64_S32_ag_idx_imm [[COPY12]], [[COPY]], 64, implicit-def $srups_of, implicit $crsat :: (load (<8 x s32>) from stack - 64, align 64)
    ; CHECK-NEXT: [[REG_SEQUENCE4:%[0-9]+]]:acc1024 = REG_SEQUENCE [[VLDA_UPS_S64_S32_ag_idx_imm9]], %subreg.sub_512_lo, [[VLDA_UPS_S64_S32_ag_idx_imm8]], %subreg.sub_512_hi
    ; CHECK-NEXT: [[PADD_imm_pseudo1:%[0-9]+]]:ep = PADD_imm_pseudo [[COPY]], 96
    ; CHECK-NEXT: [[COPY13:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S64_S32_ag_idx_imm10:%[0-9]+]]:acc512 = VLDA_UPS_S64_S32_ag_idx_imm [[COPY13]], [[PADD_imm_pseudo1]], 32, implicit-def $srups_of, implicit $crsat :: (load (<8 x s32>) from stack - 32, basealign 64)
    ; CHECK-NEXT: [[COPY14:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S64_S32_ag_idx_imm11:%[0-9]+]]:acc512 = VLDA_UPS_S64_S32_ag_idx_imm [[COPY14]], [[PADD_imm_pseudo1]], 0, implicit-def $srups_of, implicit $crsat :: (load (<8 x s32>) from stack - 64, align 64)
    ; CHECK-NEXT: [[REG_SEQUENCE5:%[0-9]+]]:acc1024 = REG_SEQUENCE [[VLDA_UPS_S64_S32_ag_idx_imm11]], %subreg.sub_512_lo, [[VLDA_UPS_S64_S32_ag_idx_imm10]], %subreg.sub_512_hi
    ; CHECK-NEXT: PseudoRET implicit $lr, implicit [[REG_SEQUENCE]], implicit [[REG_SEQUENCE1]], implicit [[REG_SEQUENCE2]], implicit [[REG_SEQUENCE3]], implicit [[REG_SEQUENCE4]], implicit [[REG_SEQUENCE5]]
    %0:ptrregbank(p0) = COPY $p0
    %7:modregbank(s20) = COPY $m0
    %8:modregbank(s20) = G_CONSTANT i20 0
    %9:modregbank(s20) = G_CONSTANT i20 16
    %10:modregbank(s20) = G_CONSTANT i20 -128
    %11:modregbank(s20) = G_CONSTANT i20 64
    %12:modregbank(s20) = G_CONSTANT i20 96
    %101:gprregbank(s32) = COPY $r1
    %102:gprregbank(s32) = G_CONSTANT i32 1
    %25:vregbank(<16 x s32>) = G_AIE_OFFSET_LOAD %0, %7 :: (load (<16 x s32>) from stack - 64)
    %103:accregbank(<16 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc64.v16.I512.ups), %25:vregbank(<16 x s32>), %101:gprregbank(s32), %102:gprregbank(s32)
    %26:vregbank(<16 x s32>) = G_AIE_OFFSET_LOAD %0, %8 :: (load (<16 x s32>) from stack - 64)
    %104:accregbank(<16 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc64.v16.I512.ups), %26:vregbank(<16 x s32>), %101:gprregbank(s32), %102:gprregbank(s32)
    %27:vregbank(<16 x s32>) = G_AIE_OFFSET_LOAD %0, %9 :: (load (<16 x s32>) from stack - 64)
    %105:accregbank(<16 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc64.v16.I512.ups), %27:vregbank(<16 x s32>), %101:gprregbank(s32), %102:gprregbank(s32)
    %28:vregbank(<16 x s32>) = G_AIE_OFFSET_LOAD %0, %10 :: (load (<16 x s32>) from stack - 64)
    %106:accregbank(<16 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc64.v16.I512.ups), %28:vregbank(<16 x s32>), %101:gprregbank(s32), %102:gprregbank(s32)
    %29:vregbank(<16 x s32>) = G_AIE_OFFSET_LOAD %0, %11 :: (load (<16 x s32>) from stack - 64)
    %107:accregbank(<16 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc64.v16.I512.ups), %29:vregbank(<16 x s32>), %101:gprregbank(s32), %102:gprregbank(s32)
    %30:vregbank(<16 x s32>) = G_AIE_OFFSET_LOAD %0, %12 :: (load (<16 x s32>) from stack - 64)
    %108:accregbank(<16 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc64.v16.I512.ups), %30:vregbank(<16 x s32>), %101:gprregbank(s32), %102:gprregbank(s32)
    PseudoRET implicit $lr, implicit %103, implicit %104, implicit %105, implicit %106, implicit %107, implicit %108
...

---
name:            VLDA_UPS_S32_D16_512_bits
legalized:       true
regBankSelected: true
tracksRegLiveness: true
body: |
  bb.0:
    liveins: $p0, $m0, $r1, $amll0
    ; CHECK-LABEL: name: VLDA_UPS_S32_D16_512_bits
    ; CHECK: liveins: $p0, $m0, $r1, $amll0
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: [[COPY:%[0-9]+]]:ep = COPY $p0
    ; CHECK-NEXT: [[COPY1:%[0-9]+]]:em = COPY $m0
    ; CHECK-NEXT: [[COPY2:%[0-9]+]]:er = COPY $r1
    ; CHECK-NEXT: [[PADD_mod_pseudo:%[0-9]+]]:ep = PADD_mod_pseudo [[COPY]], [[COPY1]]
    ; CHECK-NEXT: [[COPY3:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S32_D16_ag_idx_imm:%[0-9]+]]:acc512 = VLDA_UPS_S32_D16_ag_idx_imm [[COPY3]], [[PADD_mod_pseudo]], 32, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<16 x s16>) from stack - 32, basealign 64)
    ; CHECK-NEXT: [[COPY4:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S32_D16_ag_idx_imm1:%[0-9]+]]:acc512 = VLDA_UPS_S32_D16_ag_idx_imm [[COPY4]], [[PADD_mod_pseudo]], 0, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<16 x s16>) from stack - 64, align 64)
    ; CHECK-NEXT: [[REG_SEQUENCE:%[0-9]+]]:acc1024 = REG_SEQUENCE [[VLDA_UPS_S32_D16_ag_idx_imm1]], %subreg.sub_512_lo, [[VLDA_UPS_S32_D16_ag_idx_imm]], %subreg.sub_512_hi
    ; CHECK-NEXT: [[COPY5:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S32_D16_ag_idx_imm2:%[0-9]+]]:acc512 = VLDA_UPS_S32_D16_ag_idx_imm [[COPY5]], [[COPY]], 32, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<16 x s16>) from stack - 32, basealign 64)
    ; CHECK-NEXT: [[COPY6:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S32_D16_ag_idx_imm3:%[0-9]+]]:acc512 = VLDA_UPS_S32_D16_ag_idx_imm [[COPY6]], [[COPY]], 0, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<16 x s16>) from stack - 64, align 64)
    ; CHECK-NEXT: [[REG_SEQUENCE1:%[0-9]+]]:acc1024 = REG_SEQUENCE [[VLDA_UPS_S32_D16_ag_idx_imm3]], %subreg.sub_512_lo, [[VLDA_UPS_S32_D16_ag_idx_imm2]], %subreg.sub_512_hi
    ; CHECK-NEXT: [[PADD_imm_pseudo:%[0-9]+]]:ep = PADD_imm_pseudo [[COPY]], 16
    ; CHECK-NEXT: [[COPY7:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S32_D16_ag_idx_imm4:%[0-9]+]]:acc512 = VLDA_UPS_S32_D16_ag_idx_imm [[COPY7]], [[PADD_imm_pseudo]], 32, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<16 x s16>) from stack - 32, basealign 64)
    ; CHECK-NEXT: [[COPY8:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S32_D16_ag_idx_imm5:%[0-9]+]]:acc512 = VLDA_UPS_S32_D16_ag_idx_imm [[COPY8]], [[PADD_imm_pseudo]], 0, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<16 x s16>) from stack - 64, align 64)
    ; CHECK-NEXT: [[REG_SEQUENCE2:%[0-9]+]]:acc1024 = REG_SEQUENCE [[VLDA_UPS_S32_D16_ag_idx_imm5]], %subreg.sub_512_lo, [[VLDA_UPS_S32_D16_ag_idx_imm4]], %subreg.sub_512_hi
    ; CHECK-NEXT: [[COPY9:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S32_D16_ag_idx_imm6:%[0-9]+]]:acc512 = VLDA_UPS_S32_D16_ag_idx_imm [[COPY9]], [[COPY]], -96, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<16 x s16>) from stack - 32, basealign 64)
    ; CHECK-NEXT: [[COPY10:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S32_D16_ag_idx_imm7:%[0-9]+]]:acc512 = VLDA_UPS_S32_D16_ag_idx_imm [[COPY10]], [[COPY]], -128, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<16 x s16>) from stack - 64, align 64)
    ; CHECK-NEXT: [[REG_SEQUENCE3:%[0-9]+]]:acc1024 = REG_SEQUENCE [[VLDA_UPS_S32_D16_ag_idx_imm7]], %subreg.sub_512_lo, [[VLDA_UPS_S32_D16_ag_idx_imm6]], %subreg.sub_512_hi
    ; CHECK-NEXT: [[COPY11:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S32_D16_ag_idx_imm8:%[0-9]+]]:acc512 = VLDA_UPS_S32_D16_ag_idx_imm [[COPY11]], [[COPY]], 96, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<16 x s16>) from stack - 32, basealign 64)
    ; CHECK-NEXT: [[COPY12:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S32_D16_ag_idx_imm9:%[0-9]+]]:acc512 = VLDA_UPS_S32_D16_ag_idx_imm [[COPY12]], [[COPY]], 64, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<16 x s16>) from stack - 64, align 64)
    ; CHECK-NEXT: [[REG_SEQUENCE4:%[0-9]+]]:acc1024 = REG_SEQUENCE [[VLDA_UPS_S32_D16_ag_idx_imm9]], %subreg.sub_512_lo, [[VLDA_UPS_S32_D16_ag_idx_imm8]], %subreg.sub_512_hi
    ; CHECK-NEXT: [[PADD_imm_pseudo1:%[0-9]+]]:ep = PADD_imm_pseudo [[COPY]], 96
    ; CHECK-NEXT: [[COPY13:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S32_D16_ag_idx_imm10:%[0-9]+]]:acc512 = VLDA_UPS_S32_D16_ag_idx_imm [[COPY13]], [[PADD_imm_pseudo1]], 32, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<16 x s16>) from stack - 32, basealign 64)
    ; CHECK-NEXT: [[COPY14:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S32_D16_ag_idx_imm11:%[0-9]+]]:acc512 = VLDA_UPS_S32_D16_ag_idx_imm [[COPY14]], [[PADD_imm_pseudo1]], 0, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<16 x s16>) from stack - 64, align 64)
    ; CHECK-NEXT: [[REG_SEQUENCE5:%[0-9]+]]:acc1024 = REG_SEQUENCE [[VLDA_UPS_S32_D16_ag_idx_imm11]], %subreg.sub_512_lo, [[VLDA_UPS_S32_D16_ag_idx_imm10]], %subreg.sub_512_hi
    ; CHECK-NEXT: PseudoRET implicit $lr, implicit [[REG_SEQUENCE]], implicit [[REG_SEQUENCE1]], implicit [[REG_SEQUENCE2]], implicit [[REG_SEQUENCE3]], implicit [[REG_SEQUENCE4]], implicit [[REG_SEQUENCE5]]
    %0:ptrregbank(p0) = COPY $p0
    %7:modregbank(s20) = COPY $m0
    %8:modregbank(s20) = G_CONSTANT i20 0
    %9:modregbank(s20) = G_CONSTANT i20 16
    %10:modregbank(s20) = G_CONSTANT i20 -128
    %11:modregbank(s20) = G_CONSTANT i20 64
    %12:modregbank(s20) = G_CONSTANT i20 96
    %101:gprregbank(s32) = COPY $r1
    %102:gprregbank(s32) = G_CONSTANT i32 0
    %25:vregbank(<32 x s16>) = G_AIE_OFFSET_LOAD %0, %7 :: (load (<32 x s16>) from stack - 64)
    %103:accregbank(<16 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc32.v32.I512.ups), %25:vregbank(<32 x s16>), %101:gprregbank(s32), %102:gprregbank(s32)
    %26:vregbank(<32 x s16>) = G_AIE_OFFSET_LOAD %0, %8 :: (load (<32 x s16>) from stack - 64)
    %104:accregbank(<16 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc32.v32.I512.ups), %26:vregbank(<32 x s16>), %101:gprregbank(s32), %102:gprregbank(s32)
    %27:vregbank(<32 x s16>) = G_AIE_OFFSET_LOAD %0, %9 :: (load (<32 x s16>) from stack - 64)
    %105:accregbank(<16 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc32.v32.I512.ups), %27:vregbank(<32 x s16>), %101:gprregbank(s32), %102:gprregbank(s32)
    %28:vregbank(<32 x s16>) = G_AIE_OFFSET_LOAD %0, %10 :: (load (<32 x s16>) from stack - 64)
    %106:accregbank(<16 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc32.v32.I512.ups), %28:vregbank(<32 x s16>), %101:gprregbank(s32), %102:gprregbank(s32)
    %29:vregbank(<32 x s16>) = G_AIE_OFFSET_LOAD %0, %11 :: (load (<32 x s16>) from stack - 64)
    %107:accregbank(<16 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc32.v32.I512.ups), %29:vregbank(<32 x s16>), %101:gprregbank(s32), %102:gprregbank(s32)
    %30:vregbank(<32 x s16>) = G_AIE_OFFSET_LOAD %0, %12 :: (load (<32 x s16>) from stack - 64)
    %108:accregbank(<16 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc32.v32.I512.ups), %30:vregbank(<32 x s16>), %101:gprregbank(s32), %102:gprregbank(s32)
    PseudoRET implicit $lr, implicit %103, implicit %104, implicit %105, implicit %106, implicit %107, implicit %108
...

---
name:            VLDA_UPS_S32_D16_512_bits_dyn
legalized:       true
regBankSelected: true
tracksRegLiveness: true
body: |
  bb.0:
    liveins: $p0, $m0, $r1, $amll0
    ; CHECK-LABEL: name: VLDA_UPS_S32_D16_512_bits_dyn
    ; CHECK: liveins: $p0, $m0, $r1, $amll0
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: [[COPY:%[0-9]+]]:ep = COPY $p0
    ; CHECK-NEXT: [[COPY1:%[0-9]+]]:em = COPY $m0
    ; CHECK-NEXT: [[COPY2:%[0-9]+]]:er = COPY $r1
    ; CHECK-NEXT: [[COPY3:%[0-9]+]]:er = COPY $r1
    ; CHECK-NEXT: [[PADD_mod_pseudo:%[0-9]+]]:ep = PADD_mod_pseudo [[COPY]], [[COPY1]]
    ; CHECK-NEXT: $crupssign = COPY [[COPY3]]
    ; CHECK-NEXT: [[COPY4:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S32_D16_ag_idx_imm:%[0-9]+]]:acc512 = VLDA_UPS_S32_D16_ag_idx_imm [[COPY4]], [[PADD_mod_pseudo]], 32, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<16 x s16>) from stack - 32, basealign 64)
    ; CHECK-NEXT: [[COPY5:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S32_D16_ag_idx_imm1:%[0-9]+]]:acc512 = VLDA_UPS_S32_D16_ag_idx_imm [[COPY5]], [[PADD_mod_pseudo]], 0, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<16 x s16>) from stack - 64, align 64)
    ; CHECK-NEXT: $crupssign = MOV_scalar_imm10_pseudo 0
    ; CHECK-NEXT: [[REG_SEQUENCE:%[0-9]+]]:acc1024 = REG_SEQUENCE [[VLDA_UPS_S32_D16_ag_idx_imm1]], %subreg.sub_512_lo, [[VLDA_UPS_S32_D16_ag_idx_imm]], %subreg.sub_512_hi
    ; CHECK-NEXT: $crupssign = COPY [[COPY3]]
    ; CHECK-NEXT: [[COPY6:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S32_D16_ag_idx_imm2:%[0-9]+]]:acc512 = VLDA_UPS_S32_D16_ag_idx_imm [[COPY6]], [[COPY]], 32, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<16 x s16>) from stack - 32, basealign 64)
    ; CHECK-NEXT: [[COPY7:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S32_D16_ag_idx_imm3:%[0-9]+]]:acc512 = VLDA_UPS_S32_D16_ag_idx_imm [[COPY7]], [[COPY]], 0, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<16 x s16>) from stack - 64, align 64)
    ; CHECK-NEXT: $crupssign = MOV_scalar_imm10_pseudo 0
    ; CHECK-NEXT: [[REG_SEQUENCE1:%[0-9]+]]:acc1024 = REG_SEQUENCE [[VLDA_UPS_S32_D16_ag_idx_imm3]], %subreg.sub_512_lo, [[VLDA_UPS_S32_D16_ag_idx_imm2]], %subreg.sub_512_hi
    ; CHECK-NEXT: [[PADD_imm_pseudo:%[0-9]+]]:ep = PADD_imm_pseudo [[COPY]], 16
    ; CHECK-NEXT: $crupssign = COPY [[COPY3]]
    ; CHECK-NEXT: [[COPY8:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S32_D16_ag_idx_imm4:%[0-9]+]]:acc512 = VLDA_UPS_S32_D16_ag_idx_imm [[COPY8]], [[PADD_imm_pseudo]], 32, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<16 x s16>) from stack - 32, basealign 64)
    ; CHECK-NEXT: [[COPY9:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S32_D16_ag_idx_imm5:%[0-9]+]]:acc512 = VLDA_UPS_S32_D16_ag_idx_imm [[COPY9]], [[PADD_imm_pseudo]], 0, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<16 x s16>) from stack - 64, align 64)
    ; CHECK-NEXT: $crupssign = MOV_scalar_imm10_pseudo 0
    ; CHECK-NEXT: [[REG_SEQUENCE2:%[0-9]+]]:acc1024 = REG_SEQUENCE [[VLDA_UPS_S32_D16_ag_idx_imm5]], %subreg.sub_512_lo, [[VLDA_UPS_S32_D16_ag_idx_imm4]], %subreg.sub_512_hi
    ; CHECK-NEXT: $crupssign = COPY [[COPY3]]
    ; CHECK-NEXT: [[COPY10:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S32_D16_ag_idx_imm6:%[0-9]+]]:acc512 = VLDA_UPS_S32_D16_ag_idx_imm [[COPY10]], [[COPY]], -96, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<16 x s16>) from stack - 32, basealign 64)
    ; CHECK-NEXT: [[COPY11:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S32_D16_ag_idx_imm7:%[0-9]+]]:acc512 = VLDA_UPS_S32_D16_ag_idx_imm [[COPY11]], [[COPY]], -128, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<16 x s16>) from stack - 64, align 64)
    ; CHECK-NEXT: $crupssign = MOV_scalar_imm10_pseudo 0
    ; CHECK-NEXT: [[REG_SEQUENCE3:%[0-9]+]]:acc1024 = REG_SEQUENCE [[VLDA_UPS_S32_D16_ag_idx_imm7]], %subreg.sub_512_lo, [[VLDA_UPS_S32_D16_ag_idx_imm6]], %subreg.sub_512_hi
    ; CHECK-NEXT: $crupssign = COPY [[COPY3]]
    ; CHECK-NEXT: [[COPY12:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S32_D16_ag_idx_imm8:%[0-9]+]]:acc512 = VLDA_UPS_S32_D16_ag_idx_imm [[COPY12]], [[COPY]], 96, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<16 x s16>) from stack - 32, basealign 64)
    ; CHECK-NEXT: [[COPY13:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S32_D16_ag_idx_imm9:%[0-9]+]]:acc512 = VLDA_UPS_S32_D16_ag_idx_imm [[COPY13]], [[COPY]], 64, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<16 x s16>) from stack - 64, align 64)
    ; CHECK-NEXT: $crupssign = MOV_scalar_imm10_pseudo 0
    ; CHECK-NEXT: [[REG_SEQUENCE4:%[0-9]+]]:acc1024 = REG_SEQUENCE [[VLDA_UPS_S32_D16_ag_idx_imm9]], %subreg.sub_512_lo, [[VLDA_UPS_S32_D16_ag_idx_imm8]], %subreg.sub_512_hi
    ; CHECK-NEXT: [[PADD_imm_pseudo1:%[0-9]+]]:ep = PADD_imm_pseudo [[COPY]], 96
    ; CHECK-NEXT: $crupssign = COPY [[COPY3]]
    ; CHECK-NEXT: [[COPY14:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S32_D16_ag_idx_imm10:%[0-9]+]]:acc512 = VLDA_UPS_S32_D16_ag_idx_imm [[COPY14]], [[PADD_imm_pseudo1]], 32, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<16 x s16>) from stack - 32, basealign 64)
    ; CHECK-NEXT: [[COPY15:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S32_D16_ag_idx_imm11:%[0-9]+]]:acc512 = VLDA_UPS_S32_D16_ag_idx_imm [[COPY15]], [[PADD_imm_pseudo1]], 0, implicit-def $srups_of, implicit $crsat, implicit $crupssign :: (load (<16 x s16>) from stack - 64, align 64)
    ; CHECK-NEXT: $crupssign = MOV_scalar_imm10_pseudo 0
    ; CHECK-NEXT: [[REG_SEQUENCE5:%[0-9]+]]:acc1024 = REG_SEQUENCE [[VLDA_UPS_S32_D16_ag_idx_imm11]], %subreg.sub_512_lo, [[VLDA_UPS_S32_D16_ag_idx_imm10]], %subreg.sub_512_hi
    ; CHECK-NEXT: PseudoRET implicit $lr, implicit [[REG_SEQUENCE]], implicit [[REG_SEQUENCE1]], implicit [[REG_SEQUENCE2]], implicit [[REG_SEQUENCE3]], implicit [[REG_SEQUENCE4]], implicit [[REG_SEQUENCE5]]
    %0:ptrregbank(p0) = COPY $p0
    %7:modregbank(s20) = COPY $m0
    %8:modregbank(s20) = G_CONSTANT i20 0
    %9:modregbank(s20) = G_CONSTANT i20 16
    %10:modregbank(s20) = G_CONSTANT i20 -128
    %11:modregbank(s20) = G_CONSTANT i20 64
    %12:modregbank(s20) = G_CONSTANT i20 96
    %101:gprregbank(s32) = COPY $r1
    %102:gprregbank(s32) = COPY $r1
    %25:vregbank(<32 x s16>) = G_AIE_OFFSET_LOAD %0, %7 :: (load (<32 x s16>) from stack - 64)
    %103:accregbank(<16 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc32.v32.I512.ups), %25:vregbank(<32 x s16>), %101:gprregbank(s32), %102:gprregbank(s32)
    %26:vregbank(<32 x s16>) = G_AIE_OFFSET_LOAD %0, %8 :: (load (<32 x s16>) from stack - 64)
    %104:accregbank(<16 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc32.v32.I512.ups), %26:vregbank(<32 x s16>), %101:gprregbank(s32), %102:gprregbank(s32)
    %27:vregbank(<32 x s16>) = G_AIE_OFFSET_LOAD %0, %9 :: (load (<32 x s16>) from stack - 64)
    %105:accregbank(<16 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc32.v32.I512.ups), %27:vregbank(<32 x s16>), %101:gprregbank(s32), %102:gprregbank(s32)
    %28:vregbank(<32 x s16>) = G_AIE_OFFSET_LOAD %0, %10 :: (load (<32 x s16>) from stack - 64)
    %106:accregbank(<16 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc32.v32.I512.ups), %28:vregbank(<32 x s16>), %101:gprregbank(s32), %102:gprregbank(s32)
    %29:vregbank(<32 x s16>) = G_AIE_OFFSET_LOAD %0, %11 :: (load (<32 x s16>) from stack - 64)
    %107:accregbank(<16 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc32.v32.I512.ups), %29:vregbank(<32 x s16>), %101:gprregbank(s32), %102:gprregbank(s32)
    %30:vregbank(<32 x s16>) = G_AIE_OFFSET_LOAD %0, %12 :: (load (<32 x s16>) from stack - 64)
    %108:accregbank(<16 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc32.v32.I512.ups), %30:vregbank(<32 x s16>), %101:gprregbank(s32), %102:gprregbank(s32)
    PseudoRET implicit $lr, implicit %103, implicit %104, implicit %105, implicit %106, implicit %107, implicit %108
...

---
name:            VLDA_UPS_S32_S16_512_bits
legalized:       true
regBankSelected: true
tracksRegLiveness: true
body: |
  bb.0:
    liveins: $p0, $m0, $r1, $amll0
    ; CHECK-LABEL: name: VLDA_UPS_S32_S16_512_bits
    ; CHECK: liveins: $p0, $m0, $r1, $amll0
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: [[COPY:%[0-9]+]]:ep = COPY $p0
    ; CHECK-NEXT: [[COPY1:%[0-9]+]]:em = COPY $m0
    ; CHECK-NEXT: [[COPY2:%[0-9]+]]:er = COPY $r1
    ; CHECK-NEXT: [[PADD_mod_pseudo:%[0-9]+]]:ep = PADD_mod_pseudo [[COPY]], [[COPY1]]
    ; CHECK-NEXT: [[COPY3:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S32_S16_ag_idx_imm:%[0-9]+]]:acc512 = VLDA_UPS_S32_S16_ag_idx_imm [[COPY3]], [[PADD_mod_pseudo]], 32, implicit-def $srups_of, implicit $crsat :: (load (<16 x s16>) from stack - 32, basealign 64)
    ; CHECK-NEXT: [[COPY4:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S32_S16_ag_idx_imm1:%[0-9]+]]:acc512 = VLDA_UPS_S32_S16_ag_idx_imm [[COPY4]], [[PADD_mod_pseudo]], 0, implicit-def $srups_of, implicit $crsat :: (load (<16 x s16>) from stack - 64, align 64)
    ; CHECK-NEXT: [[REG_SEQUENCE:%[0-9]+]]:acc1024 = REG_SEQUENCE [[VLDA_UPS_S32_S16_ag_idx_imm1]], %subreg.sub_512_lo, [[VLDA_UPS_S32_S16_ag_idx_imm]], %subreg.sub_512_hi
    ; CHECK-NEXT: [[COPY5:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S32_S16_ag_idx_imm2:%[0-9]+]]:acc512 = VLDA_UPS_S32_S16_ag_idx_imm [[COPY5]], [[COPY]], 32, implicit-def $srups_of, implicit $crsat :: (load (<16 x s16>) from stack - 32, basealign 64)
    ; CHECK-NEXT: [[COPY6:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S32_S16_ag_idx_imm3:%[0-9]+]]:acc512 = VLDA_UPS_S32_S16_ag_idx_imm [[COPY6]], [[COPY]], 0, implicit-def $srups_of, implicit $crsat :: (load (<16 x s16>) from stack - 64, align 64)
    ; CHECK-NEXT: [[REG_SEQUENCE1:%[0-9]+]]:acc1024 = REG_SEQUENCE [[VLDA_UPS_S32_S16_ag_idx_imm3]], %subreg.sub_512_lo, [[VLDA_UPS_S32_S16_ag_idx_imm2]], %subreg.sub_512_hi
    ; CHECK-NEXT: [[PADD_imm_pseudo:%[0-9]+]]:ep = PADD_imm_pseudo [[COPY]], 16
    ; CHECK-NEXT: [[COPY7:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S32_S16_ag_idx_imm4:%[0-9]+]]:acc512 = VLDA_UPS_S32_S16_ag_idx_imm [[COPY7]], [[PADD_imm_pseudo]], 32, implicit-def $srups_of, implicit $crsat :: (load (<16 x s16>) from stack - 32, basealign 64)
    ; CHECK-NEXT: [[COPY8:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S32_S16_ag_idx_imm5:%[0-9]+]]:acc512 = VLDA_UPS_S32_S16_ag_idx_imm [[COPY8]], [[PADD_imm_pseudo]], 0, implicit-def $srups_of, implicit $crsat :: (load (<16 x s16>) from stack - 64, align 64)
    ; CHECK-NEXT: [[REG_SEQUENCE2:%[0-9]+]]:acc1024 = REG_SEQUENCE [[VLDA_UPS_S32_S16_ag_idx_imm5]], %subreg.sub_512_lo, [[VLDA_UPS_S32_S16_ag_idx_imm4]], %subreg.sub_512_hi
    ; CHECK-NEXT: [[COPY9:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S32_S16_ag_idx_imm6:%[0-9]+]]:acc512 = VLDA_UPS_S32_S16_ag_idx_imm [[COPY9]], [[COPY]], -96, implicit-def $srups_of, implicit $crsat :: (load (<16 x s16>) from stack - 32, basealign 64)
    ; CHECK-NEXT: [[COPY10:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S32_S16_ag_idx_imm7:%[0-9]+]]:acc512 = VLDA_UPS_S32_S16_ag_idx_imm [[COPY10]], [[COPY]], -128, implicit-def $srups_of, implicit $crsat :: (load (<16 x s16>) from stack - 64, align 64)
    ; CHECK-NEXT: [[REG_SEQUENCE3:%[0-9]+]]:acc1024 = REG_SEQUENCE [[VLDA_UPS_S32_S16_ag_idx_imm7]], %subreg.sub_512_lo, [[VLDA_UPS_S32_S16_ag_idx_imm6]], %subreg.sub_512_hi
    ; CHECK-NEXT: [[COPY11:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S32_S16_ag_idx_imm8:%[0-9]+]]:acc512 = VLDA_UPS_S32_S16_ag_idx_imm [[COPY11]], [[COPY]], 96, implicit-def $srups_of, implicit $crsat :: (load (<16 x s16>) from stack - 32, basealign 64)
    ; CHECK-NEXT: [[COPY12:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S32_S16_ag_idx_imm9:%[0-9]+]]:acc512 = VLDA_UPS_S32_S16_ag_idx_imm [[COPY12]], [[COPY]], 64, implicit-def $srups_of, implicit $crsat :: (load (<16 x s16>) from stack - 64, align 64)
    ; CHECK-NEXT: [[REG_SEQUENCE4:%[0-9]+]]:acc1024 = REG_SEQUENCE [[VLDA_UPS_S32_S16_ag_idx_imm9]], %subreg.sub_512_lo, [[VLDA_UPS_S32_S16_ag_idx_imm8]], %subreg.sub_512_hi
    ; CHECK-NEXT: [[PADD_imm_pseudo1:%[0-9]+]]:ep = PADD_imm_pseudo [[COPY]], 96
    ; CHECK-NEXT: [[COPY13:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S32_S16_ag_idx_imm10:%[0-9]+]]:acc512 = VLDA_UPS_S32_S16_ag_idx_imm [[COPY13]], [[PADD_imm_pseudo1]], 32, implicit-def $srups_of, implicit $crsat :: (load (<16 x s16>) from stack - 32, basealign 64)
    ; CHECK-NEXT: [[COPY14:%[0-9]+]]:mss = COPY [[COPY2]]
    ; CHECK-NEXT: [[VLDA_UPS_S32_S16_ag_idx_imm11:%[0-9]+]]:acc512 = VLDA_UPS_S32_S16_ag_idx_imm [[COPY14]], [[PADD_imm_pseudo1]], 0, implicit-def $srups_of, implicit $crsat :: (load (<16 x s16>) from stack - 64, align 64)
    ; CHECK-NEXT: [[REG_SEQUENCE5:%[0-9]+]]:acc1024 = REG_SEQUENCE [[VLDA_UPS_S32_S16_ag_idx_imm11]], %subreg.sub_512_lo, [[VLDA_UPS_S32_S16_ag_idx_imm10]], %subreg.sub_512_hi
    ; CHECK-NEXT: PseudoRET implicit $lr, implicit [[REG_SEQUENCE]], implicit [[REG_SEQUENCE1]], implicit [[REG_SEQUENCE2]], implicit [[REG_SEQUENCE3]], implicit [[REG_SEQUENCE4]], implicit [[REG_SEQUENCE5]]
    %0:ptrregbank(p0) = COPY $p0
    %7:modregbank(s20) = COPY $m0
    %8:modregbank(s20) = G_CONSTANT i20 0
    %9:modregbank(s20) = G_CONSTANT i20 16
    %10:modregbank(s20) = G_CONSTANT i20 -128
    %11:modregbank(s20) = G_CONSTANT i20 64
    %12:modregbank(s20) = G_CONSTANT i20 96
    %101:gprregbank(s32) = COPY $r1
    %102:gprregbank(s32) = G_CONSTANT i32 1
    %25:vregbank(<32 x s16>) = G_AIE_OFFSET_LOAD %0, %7 :: (load (<32 x s16>) from stack - 64)
    %103:accregbank(<16 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc32.v32.I512.ups), %25:vregbank(<32 x s16>), %101:gprregbank(s32), %102:gprregbank(s32)
    %26:vregbank(<32 x s16>) = G_AIE_OFFSET_LOAD %0, %8 :: (load (<32 x s16>) from stack - 64)
    %104:accregbank(<16 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc32.v32.I512.ups), %26:vregbank(<32 x s16>), %101:gprregbank(s32), %102:gprregbank(s32)
    %27:vregbank(<32 x s16>) = G_AIE_OFFSET_LOAD %0, %9 :: (load (<32 x s16>) from stack - 64)
    %105:accregbank(<16 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc32.v32.I512.ups), %27:vregbank(<32 x s16>), %101:gprregbank(s32), %102:gprregbank(s32)
    %28:vregbank(<32 x s16>) = G_AIE_OFFSET_LOAD %0, %10 :: (load (<32 x s16>) from stack - 64)
    %106:accregbank(<16 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc32.v32.I512.ups), %28:vregbank(<32 x s16>), %101:gprregbank(s32), %102:gprregbank(s32)
    %29:vregbank(<32 x s16>) = G_AIE_OFFSET_LOAD %0, %11 :: (load (<32 x s16>) from stack - 64)
    %107:accregbank(<16 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc32.v32.I512.ups), %29:vregbank(<32 x s16>), %101:gprregbank(s32), %102:gprregbank(s32)
    %30:vregbank(<32 x s16>) = G_AIE_OFFSET_LOAD %0, %12 :: (load (<32 x s16>) from stack - 64)
    %108:accregbank(<16 x s64>) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.aie2.acc32.v32.I512.ups), %30:vregbank(<32 x s16>), %101:gprregbank(s32), %102:gprregbank(s32)
    PseudoRET implicit $lr, implicit %103, implicit %104, implicit %105, implicit %106, implicit %107, implicit %108
...
