# NOTE: Assertions have been autogenerated by utils/update_mir_test_checks.py UTC_ARGS: --version 4
#
# This file is licensed under the Apache License v2.0 with LLVM Exceptions.
# See https://llvm.org/LICENSE.txt for license information.
# SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
#
# (c) Copyright 2023-2024 Advanced Micro Devices, Inc. or its affiliates
# RUN: llc -mtriple aie2 -run-pass=aie2-prelegalizer-combiner %s \
# RUN:         -verify-machineinstrs -o - | FileCheck %s --check-prefix=ENABLED-COMBINE
# RUN: llc -mtriple aie2 -run-pass=aie2-prelegalizer-combiner \
# RUN:        -aie-combine-vec-shift-by-zero=false %s \
# RUN:        -verify-machineinstrs -o - | FileCheck %s --check-prefix=DISABLED-COMBINE

# The goal of this set of tests is to test the pre-legalizer combiner that converges
# shift-by-zero vector operations into register copies.

---
name:            test_v64int8
legalized:       false
body:             |
  bb.1.entry:
    liveins: $x2

    ; ENABLED-COMBINE-LABEL: name: test_v64int8
    ; ENABLED-COMBINE: liveins: $x2
    ; ENABLED-COMBINE-NEXT: {{  $}}
    ; ENABLED-COMBINE-NEXT: [[COPY:%[0-9]+]]:_(<16 x s32>) = COPY $x2
    ; ENABLED-COMBINE-NEXT: $x0 = COPY [[COPY]](<16 x s32>)
    ; ENABLED-COMBINE-NEXT: PseudoRET implicit $lr, implicit $x0
    ;
    ; DISABLED-COMBINE-LABEL: name: test_v64int8
    ; DISABLED-COMBINE: liveins: $x2
    ; DISABLED-COMBINE-NEXT: {{  $}}
    ; DISABLED-COMBINE-NEXT: [[COPY:%[0-9]+]]:_(<16 x s32>) = COPY $x2
    ; DISABLED-COMBINE-NEXT: [[C:%[0-9]+]]:_(s32) = G_CONSTANT i32 0
    ; DISABLED-COMBINE-NEXT: [[INT:%[0-9]+]]:_(<64 x s8>) = G_INTRINSIC intrinsic(@llvm.aie2.v64int8)
    ; DISABLED-COMBINE-NEXT: [[BITCAST:%[0-9]+]]:_(<16 x s32>) = G_BITCAST [[INT]](<64 x s8>)
    ; DISABLED-COMBINE-NEXT: [[INT1:%[0-9]+]]:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vshift.I512.I512), [[COPY]](<16 x s32>), [[BITCAST]](<16 x s32>), [[C]](s32), [[C]](s32)
    ; DISABLED-COMBINE-NEXT: $x0 = COPY [[INT1]](<16 x s32>)
    ; DISABLED-COMBINE-NEXT: PseudoRET implicit $lr, implicit $x0
    %1:_(<16 x s32>) = COPY $x2
    %4:_(s32) = G_CONSTANT i32 0
    %2:_(<64 x s8>) = G_INTRINSIC intrinsic(@llvm.aie2.v64int8)
    %3:_(<16 x s32>) = G_BITCAST %2(<64 x s8>)
    %0:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vshift.I512.I512), %1(<16 x s32>), %3(<16 x s32>), %4(s32), %4(s32)
    $x0 = COPY %0(<16 x s32>)
    PseudoRET implicit $lr, implicit $x0

...

---
name:            test_v32int16
legalized:       false
body:             |
  bb.1.entry:
    liveins: $x2

    ; ENABLED-COMBINE-LABEL: name: test_v32int16
    ; ENABLED-COMBINE: liveins: $x2
    ; ENABLED-COMBINE-NEXT: {{  $}}
    ; ENABLED-COMBINE-NEXT: [[COPY:%[0-9]+]]:_(<16 x s32>) = COPY $x2
    ; ENABLED-COMBINE-NEXT: $x0 = COPY [[COPY]](<16 x s32>)
    ; ENABLED-COMBINE-NEXT: PseudoRET implicit $lr, implicit $x0
    ;
    ; DISABLED-COMBINE-LABEL: name: test_v32int16
    ; DISABLED-COMBINE: liveins: $x2
    ; DISABLED-COMBINE-NEXT: {{  $}}
    ; DISABLED-COMBINE-NEXT: [[COPY:%[0-9]+]]:_(<16 x s32>) = COPY $x2
    ; DISABLED-COMBINE-NEXT: [[C:%[0-9]+]]:_(s32) = G_CONSTANT i32 0
    ; DISABLED-COMBINE-NEXT: [[INT:%[0-9]+]]:_(<32 x s16>) = G_INTRINSIC intrinsic(@llvm.aie2.v32int16)
    ; DISABLED-COMBINE-NEXT: [[BITCAST:%[0-9]+]]:_(<16 x s32>) = G_BITCAST [[INT]](<32 x s16>)
    ; DISABLED-COMBINE-NEXT: [[INT1:%[0-9]+]]:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vshift.I512.I512), [[COPY]](<16 x s32>), [[BITCAST]](<16 x s32>), [[C]](s32), [[C]](s32)
    ; DISABLED-COMBINE-NEXT: $x0 = COPY [[INT1]](<16 x s32>)
    ; DISABLED-COMBINE-NEXT: PseudoRET implicit $lr, implicit $x0
    %1:_(<16 x s32>) = COPY $x2
    %4:_(s32) = G_CONSTANT i32 0
    %2:_(<32 x s16>) = G_INTRINSIC intrinsic(@llvm.aie2.v32int16)
    %3:_(<16 x s32>) = G_BITCAST %2(<32 x s16>)
    %0:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vshift.I512.I512), %1(<16 x s32>), %3(<16 x s32>), %4(s32), %4(s32)
    $x0 = COPY %0(<16 x s32>)
    PseudoRET implicit $lr, implicit $x0

...

---
name:            test3_v16int32
legalized:       false
body:             |
  bb.1.entry:
    liveins: $x2

    ; ENABLED-COMBINE-LABEL: name: test3_v16int32
    ; ENABLED-COMBINE: liveins: $x2
    ; ENABLED-COMBINE-NEXT: {{  $}}
    ; ENABLED-COMBINE-NEXT: [[COPY:%[0-9]+]]:_(<16 x s32>) = COPY $x2
    ; ENABLED-COMBINE-NEXT: $x0 = COPY [[COPY]](<16 x s32>)
    ; ENABLED-COMBINE-NEXT: PseudoRET implicit $lr, implicit $x0
    ;
    ; DISABLED-COMBINE-LABEL: name: test3_v16int32
    ; DISABLED-COMBINE: liveins: $x2
    ; DISABLED-COMBINE-NEXT: {{  $}}
    ; DISABLED-COMBINE-NEXT: [[COPY:%[0-9]+]]:_(<16 x s32>) = COPY $x2
    ; DISABLED-COMBINE-NEXT: [[C:%[0-9]+]]:_(s32) = G_CONSTANT i32 0
    ; DISABLED-COMBINE-NEXT: [[INT:%[0-9]+]]:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.v16int32)
    ; DISABLED-COMBINE-NEXT: [[INT1:%[0-9]+]]:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vshift.I512.I512), [[COPY]](<16 x s32>), [[INT]](<16 x s32>), [[C]](s32), [[C]](s32)
    ; DISABLED-COMBINE-NEXT: $x0 = COPY [[INT1]](<16 x s32>)
    ; DISABLED-COMBINE-NEXT: PseudoRET implicit $lr, implicit $x0
    %1:_(<16 x s32>) = COPY $x2
    %3:_(s32) = G_CONSTANT i32 0
    %2:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.v16int32)
    %0:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vshift.I512.I512), %1(<16 x s32>), %2(<16 x s32>), %3(s32), %3(s32)
    $x0 = COPY %0(<16 x s32>)
    PseudoRET implicit $lr, implicit $x0

...
