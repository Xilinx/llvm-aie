# NOTE: Assertions have been autogenerated by utils/update_mir_test_checks.py
#
# This file is licensed under the Apache License v2.0 with LLVM Exceptions.
# See https://llvm.org/LICENSE.txt for license information.
# SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
#
# (c) Copyright 2023-2024 Advanced Micro Devices, Inc. or its affiliates
# RUN: llc -mtriple aie2 -run-pass=legalizer %s -verify-machineinstrs -o - | FileCheck %s

# v2s32 is legalized to G_UNMERGE_VALUES and G_BUILD_VECTOR.
---
name:            v2s32_const_idx
legalized:       false
body:             |
  bb.0.entry:
    liveins: $l0, $r0
    ; CHECK-LABEL: name: v2s32_const_idx
    ; CHECK: liveins: $l0, $r0
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: [[COPY:%[0-9]+]]:_(<2 x s32>) = COPY $l0
    ; CHECK-NEXT: [[COPY1:%[0-9]+]]:_(s32) = COPY $r0
    ; CHECK-NEXT: [[UV:%[0-9]+]]:_(s32), [[UV1:%[0-9]+]]:_(s32) = G_UNMERGE_VALUES [[COPY]](<2 x s32>)
    ; CHECK-NEXT: [[BUILD_VECTOR:%[0-9]+]]:_(<2 x s32>) = G_BUILD_VECTOR [[COPY1]](s32), [[UV1]](s32)
    ; CHECK-NEXT: [[UV2:%[0-9]+]]:_(s32), [[UV3:%[0-9]+]]:_(s32) = G_UNMERGE_VALUES [[COPY]](<2 x s32>)
    ; CHECK-NEXT: [[BUILD_VECTOR1:%[0-9]+]]:_(<2 x s32>) = G_BUILD_VECTOR [[UV2]](s32), [[COPY1]](s32)
    ; CHECK-NEXT: $l3 = COPY [[BUILD_VECTOR]](<2 x s32>)
    ; CHECK-NEXT: $l5 = COPY [[BUILD_VECTOR1]](<2 x s32>)
    %0:_(<2 x s32>) = COPY $l0
    %1:_(s32) = COPY $r0
    %2:_(s32) = G_CONSTANT i32 0
    %3:_(<2 x s32>) = G_INSERT_VECTOR_ELT %0(<2 x s32>), %1(s32), %2(s32)
    %4:_(s32) = G_CONSTANT i32 1
    %5:_(<2 x s32>) = G_INSERT_VECTOR_ELT %0(<2 x s32>), %1(s32), %4(s32)
    $l3 = COPY %3(<2 x s32>)
    $l5 = COPY %5(<2 x s32>)
...

---
name:            v2s32_dyn_idx
legalized:       false
body:             |
  bb.0.entry:
    liveins: $l0, $r0, $r1
    ; CHECK-LABEL: name: v2s32_dyn_idx
    ; CHECK: liveins: $l0, $r0, $r1
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: [[COPY:%[0-9]+]]:_(<2 x s32>) = COPY $l0
    ; CHECK-NEXT: [[COPY1:%[0-9]+]]:_(s32) = COPY $r0
    ; CHECK-NEXT: [[COPY2:%[0-9]+]]:_(s32) = COPY $r1
    ; CHECK-NEXT: [[UV:%[0-9]+]]:_(s32), [[UV1:%[0-9]+]]:_(s32) = G_UNMERGE_VALUES [[COPY]](<2 x s32>)
    ; CHECK-NEXT: [[SELECT:%[0-9]+]]:_(s32) = G_SELECT [[COPY2]](s32), [[UV]], [[COPY1]]
    ; CHECK-NEXT: [[SELECT1:%[0-9]+]]:_(s32) = G_SELECT [[COPY2]](s32), [[COPY1]], [[UV1]]
    ; CHECK-NEXT: [[BUILD_VECTOR:%[0-9]+]]:_(<2 x s32>) = G_BUILD_VECTOR [[SELECT]](s32), [[SELECT1]](s32)
    ; CHECK-NEXT: $l3 = COPY [[BUILD_VECTOR]](<2 x s32>)
    %0:_(<2 x s32>) = COPY $l0
    %1:_(s32) = COPY $r0
    %2:_(s32) = COPY $r1
    %3:_(<2 x s32>) = G_INSERT_VECTOR_ELT %0(<2 x s32>), %1(s32), %2(s32)
    $l3 = COPY %3(<2 x s32>)
...

# 64-bit idx for INSERT_VECTOR_ELT is narrowed to 32-bit.
---
name:            v8s32_64bit_idx
legalized:       false
body:             |
  bb.0.entry:
    liveins: $wl0, $r0
    ; CHECK-LABEL: name: v8s32_64bit_idx
    ; CHECK: liveins: $wl0, $r0
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: [[COPY:%[0-9]+]]:_(<8 x s32>) = COPY $wl0
    ; CHECK-NEXT: [[COPY1:%[0-9]+]]:_(s32) = COPY $r0
    ; CHECK-NEXT: [[C:%[0-9]+]]:_(s32) = G_CONSTANT i32 0
    ; CHECK-NEXT: [[AIE_INSERT_VECTOR_ELT:%[0-9]+]]:_(<8 x s32>) = G_AIE_INSERT_VECTOR_ELT [[COPY]], [[COPY1]](s32), [[C]](s32)
    ; CHECK-NEXT: $wl3 = COPY [[AIE_INSERT_VECTOR_ELT]](<8 x s32>)
    %0:_(<8 x s32>) = COPY $wl0
    %1:_(s32) = COPY $r0
    %2:_(s64) = G_CONSTANT i64 0
    %3:_(<8 x s32>) = G_INSERT_VECTOR_ELT %0(<8 x s32>), %1(s32), %2(s64)
    $wl3 = COPY %3(<8 x s32>)
...

---
name:            v8s32_64bit_non_const_idx
legalized:       false
body:             |
  bb.0.entry:
    liveins: $wl0, $r0
    ; CHECK-LABEL: name: v8s32_64bit_non_const_idx
    ; CHECK: liveins: $wl0, $r0
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: [[COPY:%[0-9]+]]:_(<8 x s32>) = COPY $wl0
    ; CHECK-NEXT: [[COPY1:%[0-9]+]]:_(s32) = COPY $r0
    ; CHECK-NEXT: [[COPY2:%[0-9]+]]:_(s64) = COPY $l0
    ; CHECK-NEXT: [[TRUNC:%[0-9]+]]:_(s32) = G_TRUNC [[COPY2]](s64)
    ; CHECK-NEXT: [[AIE_INSERT_VECTOR_ELT:%[0-9]+]]:_(<8 x s32>) = G_AIE_INSERT_VECTOR_ELT [[COPY]], [[COPY1]](s32), [[TRUNC]](s32)
    ; CHECK-NEXT: $wl3 = COPY [[AIE_INSERT_VECTOR_ELT]](<8 x s32>)
    %0:_(<8 x s32>) = COPY $wl0
    %1:_(s32) = COPY $r0
    %2:_(s64) = COPY $l0
    %3:_(<8 x s32>) = G_INSERT_VECTOR_ELT %0(<8 x s32>), %1(s32), %2(s64)
    $wl3 = COPY %3(<8 x s32>)
...

---
name:            256bit_s8_s16_s32_val
legalized:       false
body:             |
  bb.0.entry:
    liveins: $wl0, $r0
    ; CHECK-LABEL: name: 256bit_s8_s16_s32_val
    ; CHECK: liveins: $wl0, $r0
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: [[COPY:%[0-9]+]]:_(<32 x s8>) = COPY $wl0
    ; CHECK-NEXT: [[COPY1:%[0-9]+]]:_(s32) = COPY $r0
    ; CHECK-NEXT: [[C:%[0-9]+]]:_(s32) = G_CONSTANT i32 23
    ; CHECK-NEXT: [[AIE_INSERT_VECTOR_ELT:%[0-9]+]]:_(<32 x s8>) = G_AIE_INSERT_VECTOR_ELT [[COPY]], [[COPY1]](s32), [[C]](s32)
    ; CHECK-NEXT: [[AIE_INSERT_VECTOR_ELT1:%[0-9]+]]:_(<32 x s8>) = G_AIE_INSERT_VECTOR_ELT [[COPY]], [[COPY1]](s32), [[C]](s32)
    ; CHECK-NEXT: [[AIE_INSERT_VECTOR_ELT2:%[0-9]+]]:_(<32 x s8>) = G_AIE_INSERT_VECTOR_ELT [[COPY]], [[COPY1]](s32), [[C]](s32)
    ; CHECK-NEXT: $wl3 = COPY [[AIE_INSERT_VECTOR_ELT]](<32 x s8>)
    ; CHECK-NEXT: $wh6 = COPY [[AIE_INSERT_VECTOR_ELT1]](<32 x s8>)
    ; CHECK-NEXT: $wl7 = COPY [[AIE_INSERT_VECTOR_ELT2]](<32 x s8>)
    %0:_(<32 x s8>) = COPY $wl0
    %1:_(s32) = COPY $r0
    %2:_(s32) = G_CONSTANT i32 23
    %3:_(s8) = G_TRUNC %1:_(s32)
    %4:_(<32 x s8>) = G_INSERT_VECTOR_ELT %0(<32 x s8>), %3:_(s8), %2(s32)
    %5:_(s16) = G_TRUNC %1:_(s32)
    %6:_(<32 x s8>) = G_INSERT_VECTOR_ELT %0(<32 x s8>), %5:_(s16), %2(s32)
    %7:_(<32 x s8>) = G_INSERT_VECTOR_ELT %0(<32 x s8>), %1:_(s32), %2(s32)
    $wl3 = COPY %4(<32 x s8>)
    $wh6 = COPY %6(<32 x s8>)
    $wl7 = COPY %7(<32 x s8>)
...

---
name:            512bit_s8_s16_s32_val
legalized:       false
body:             |
  bb.0.entry:
    liveins: $x0, $r0
    ; CHECK-LABEL: name: 512bit_s8_s16_s32_val
    ; CHECK: liveins: $x0, $r0
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: [[COPY:%[0-9]+]]:_(<32 x s16>) = COPY $x0
    ; CHECK-NEXT: [[COPY1:%[0-9]+]]:_(s32) = COPY $r0
    ; CHECK-NEXT: [[C:%[0-9]+]]:_(s32) = G_CONSTANT i32 5
    ; CHECK-NEXT: [[AIE_INSERT_VECTOR_ELT:%[0-9]+]]:_(<32 x s16>) = G_AIE_INSERT_VECTOR_ELT [[COPY]], [[COPY1]](s32), [[C]](s32)
    ; CHECK-NEXT: [[AIE_INSERT_VECTOR_ELT1:%[0-9]+]]:_(<32 x s16>) = G_AIE_INSERT_VECTOR_ELT [[COPY]], [[COPY1]](s32), [[C]](s32)
    ; CHECK-NEXT: [[AIE_INSERT_VECTOR_ELT2:%[0-9]+]]:_(<32 x s16>) = G_AIE_INSERT_VECTOR_ELT [[COPY]], [[COPY1]](s32), [[C]](s32)
    ; CHECK-NEXT: $x2 = COPY [[AIE_INSERT_VECTOR_ELT]](<32 x s16>)
    ; CHECK-NEXT: $x3 = COPY [[AIE_INSERT_VECTOR_ELT1]](<32 x s16>)
    ; CHECK-NEXT: $x4 = COPY [[AIE_INSERT_VECTOR_ELT2]](<32 x s16>)
    %0:_(<32 x s16>) = COPY $x0
    %1:_(s32) = COPY $r0
    %2:_(s32) = G_CONSTANT i32 5
    %3:_(s8) = G_TRUNC %1:_(s32)
    %4:_(<32 x s16>) = G_INSERT_VECTOR_ELT %0(<32 x s16>), %3(s8), %2(s32)
    %5:_(s16) = G_TRUNC %1:_(s32)
    %6:_(<32 x s16>) = G_INSERT_VECTOR_ELT %0(<32 x s16>), %5(s16), %2(s32)
    %7:_(<32 x s16>) = G_INSERT_VECTOR_ELT %0(<32 x s16>), %1(s32), %2(s32)
    $x2 = COPY %4(<32 x s16>)
    $x3 = COPY %6(<32 x s16>)
    $x4 = COPY %7(<32 x s16>)
...

---
name:            1024bit_s8_s16_s32_val
legalized:       false
body:             |
  bb.0.entry:
    liveins: $y2, $r0
    ; CHECK-LABEL: name: 1024bit_s8_s16_s32_val
    ; CHECK: liveins: $y2, $r0
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: [[COPY:%[0-9]+]]:_(<128 x s8>) = COPY $y2
    ; CHECK-NEXT: [[COPY1:%[0-9]+]]:_(s32) = COPY $r0
    ; CHECK-NEXT: [[C:%[0-9]+]]:_(s32) = G_CONSTANT i32 72
    ; CHECK-NEXT: [[AIE_INSERT_VECTOR_ELT:%[0-9]+]]:_(<128 x s8>) = G_AIE_INSERT_VECTOR_ELT [[COPY]], [[COPY1]](s32), [[C]](s32)
    ; CHECK-NEXT: [[AIE_INSERT_VECTOR_ELT1:%[0-9]+]]:_(<128 x s8>) = G_AIE_INSERT_VECTOR_ELT [[COPY]], [[COPY1]](s32), [[C]](s32)
    ; CHECK-NEXT: [[AIE_INSERT_VECTOR_ELT2:%[0-9]+]]:_(<128 x s8>) = G_AIE_INSERT_VECTOR_ELT [[COPY]], [[COPY1]](s32), [[C]](s32)
    ; CHECK-NEXT: $y3 = COPY [[AIE_INSERT_VECTOR_ELT]](<128 x s8>)
    ; CHECK-NEXT: $y4 = COPY [[AIE_INSERT_VECTOR_ELT1]](<128 x s8>)
    ; CHECK-NEXT: $y5 = COPY [[AIE_INSERT_VECTOR_ELT2]](<128 x s8>)
    %0:_(<128 x s8>) = COPY $y2
    %1:_(s32) = COPY $r0
    %2:_(s32) = G_CONSTANT i32 72
    %3:_(s8) = G_TRUNC %1:_(s32)
    %4:_(<128 x s8>) = G_INSERT_VECTOR_ELT %0(<128 x s8>), %3(s8), %2(s32)
    %5:_(s16) = G_TRUNC %1:_(s32)
    %6:_(<128 x s8>) = G_INSERT_VECTOR_ELT %0(<128 x s8>), %5(s16), %2(s32)
    %7:_(<128 x s8>) = G_INSERT_VECTOR_ELT %0(<128 x s8>), %1(s32), %2(s32)
    $y3 = COPY %4(<128 x s8>)
    $y4 = COPY %6(<128 x s8>)
    $y5 = COPY %7(<128 x s8>)
...
