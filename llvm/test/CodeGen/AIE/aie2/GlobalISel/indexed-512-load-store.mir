# NOTE: Assertions have been autogenerated by utils/update_mir_test_checks.py
#
# This file is licensed under the Apache License v2.0 with LLVM Exceptions.
# See https://llvm.org/LICENSE.txt for license information.
# SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
#
# (c) Copyright 2023-2024 Advanced Micro Devices, Inc. or its affiliates
# RUN: llc -mtriple aie2 -start-before=aie2-postlegalizer-custom-combiner -stop-after=instruction-select %s -verify-machineinstrs -o - | FileCheck %s

# The way we currently select 512-bit offset memory operantions when the offset
# does not fit in the immediate range of the instruction leads to unnecessarily
# selecting an identical PADD twice in certain situations.
# For simpler tests CSE picks up on these cases and removes one copy of the PADD
# further down the compilation pipeline, but in both examples below the
# duplicated PADD reaches the final assembly.
# TODO: This can and should be avoided!

---
name:            load_offset_not_32_step
legalized:       true
tracksRegLiveness: true
body:             |
  bb.0:
    liveins: $p0
    ; CHECK-LABEL: name: load_offset_not_32_step
    ; CHECK: liveins: $p0
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: [[COPY:%[0-9]+]]:ep = COPY $p0
    ; CHECK-NEXT: [[MOV_RLC_imm10_pseudo:%[0-9]+]]:er = MOV_RLC_imm10_pseudo 24
    ; CHECK-NEXT: [[COPY1:%[0-9]+]]:edj = COPY [[MOV_RLC_imm10_pseudo]]
    ; CHECK-NEXT: [[COPY2:%[0-9]+]]:em = COPY [[COPY1]]
    ; CHECK-NEXT: [[PADD_mod_pseudo:%[0-9]+]]:ep = PADD_mod_pseudo [[COPY]], [[COPY2]]
    ; CHECK-NEXT: [[MOV_PD_imm10_pseudo:%[0-9]+]]:edj = MOV_PD_imm10_pseudo 56
    ; CHECK-NEXT: [[VLD_idx_pseudo:%[0-9]+]]:ewh = VLD_idx_pseudo [[COPY]], [[MOV_PD_imm10_pseudo]] :: (load (<16 x s16>) from unknown-address + 32)
    ; CHECK-NEXT: [[VLD_idx_pseudo1:%[0-9]+]]:ewl = VLD_idx_pseudo [[COPY]], [[COPY1]] :: (load (<16 x s16>), align 64)
    ; CHECK-NEXT: [[REG_SEQUENCE:%[0-9]+]]:vec512 = REG_SEQUENCE [[VLD_idx_pseudo1]], %subreg.sub_256_lo, [[VLD_idx_pseudo]], %subreg.sub_256_hi
    ; CHECK-NEXT: [[MOV_PD_imm10_pseudo1:%[0-9]+]]:edj = MOV_PD_imm10_pseudo 48
    ; CHECK-NEXT: [[VLD_idx_pseudo2:%[0-9]+]]:vec256 = VLD_idx_pseudo [[COPY]], [[MOV_PD_imm10_pseudo1]] :: (load (<16 x s16>))
    ; CHECK-NEXT: PseudoRET implicit $lr, implicit [[PADD_mod_pseudo]], implicit [[REG_SEQUENCE]], implicit [[VLD_idx_pseudo2]]
    %0:_(p0) = COPY $p0
    %1:_(s32) = G_CONSTANT i32 24
    %2:_(s20) = G_TRUNC %1
    %3:_(p0) = G_PTR_ADD %0, %2
    %4:_(<32 x s16>) = G_LOAD %3(p0) :: (load (<32 x s16>))
    %5:_(p0) = G_PTR_ADD %3, %2
    %6:_(<16 x s16>) = G_LOAD %5(p0) :: (load (<16 x s16>))
    PseudoRET implicit $lr, implicit %3, implicit %4, implicit %6
...

---
name:            load_offset_is_32_step
legalized:       true
tracksRegLiveness: true
body:             |
  bb.0:
    liveins: $p0
    ; CHECK-LABEL: name: load_offset_is_32_step
    ; CHECK: liveins: $p0
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: [[COPY:%[0-9]+]]:ep = COPY $p0
    ; CHECK-NEXT: [[MOV_RLC_imm10_pseudo:%[0-9]+]]:er = MOV_RLC_imm10_pseudo 64
    ; CHECK-NEXT: [[COPY1:%[0-9]+]]:em = COPY [[MOV_RLC_imm10_pseudo]]
    ; CHECK-NEXT: [[PADD_mod_pseudo:%[0-9]+]]:ep = PADD_mod_pseudo [[COPY]], [[COPY1]]
    ; CHECK-NEXT: [[VLD_idx_imm_3x32_pseudo:%[0-9]+]]:ewh = VLD_idx_imm_3x32_pseudo [[COPY]], 96 :: (load (<16 x s16>) from unknown-address + 32)
    ; CHECK-NEXT: [[VLD_idx_imm_3x32_pseudo1:%[0-9]+]]:ewl = VLD_idx_imm_3x32_pseudo [[COPY]], 64 :: (load (<16 x s16>), align 64)
    ; CHECK-NEXT: [[REG_SEQUENCE:%[0-9]+]]:vec512 = REG_SEQUENCE [[VLD_idx_imm_3x32_pseudo1]], %subreg.sub_256_lo, [[VLD_idx_imm_3x32_pseudo]], %subreg.sub_256_hi
    ; CHECK-NEXT: [[VLDA_dmw_lda_w_ag_idx_imm:%[0-9]+]]:vec256 = VLDA_dmw_lda_w_ag_idx_imm [[COPY]], 128 :: (load (<16 x s16>))
    ; CHECK-NEXT: PseudoRET implicit $lr, implicit [[PADD_mod_pseudo]], implicit [[REG_SEQUENCE]], implicit [[VLDA_dmw_lda_w_ag_idx_imm]]
    %0:_(p0) = COPY $p0
    %1:_(s32) = G_CONSTANT i32 64
    %2:_(s20) = G_TRUNC %1
    %3:_(p0) = G_PTR_ADD %0, %2
    %4:_(<32 x s16>) = G_LOAD %3(p0) :: (load (<32 x s16>))
    %5:_(p0) = G_PTR_ADD %3, %2
    %6:_(<16 x s16>) = G_LOAD %5(p0) :: (load (<16 x s16>))
    PseudoRET implicit $lr, implicit %3, implicit %4, implicit %6
...

---
name:            store_offset_not_32_step
legalized:       true
tracksRegLiveness: true
body:             |
  bb.0:
    liveins: $p0, $x0, $wl2
    ; CHECK-LABEL: name: store_offset_not_32_step
    ; CHECK: liveins: $p0, $x0, $wl2
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: [[COPY:%[0-9]+]]:ep = COPY $p0
    ; CHECK-NEXT: [[COPY1:%[0-9]+]]:vec512 = COPY $x0
    ; CHECK-NEXT: [[COPY2:%[0-9]+]]:vec256 = COPY $wl2
    ; CHECK-NEXT: [[MOV_RLC_imm10_pseudo:%[0-9]+]]:er = MOV_RLC_imm10_pseudo 24
    ; CHECK-NEXT: [[COPY3:%[0-9]+]]:edj = COPY [[MOV_RLC_imm10_pseudo]]
    ; CHECK-NEXT: [[COPY4:%[0-9]+]]:em = COPY [[COPY3]]
    ; CHECK-NEXT: [[PADD_mod_pseudo:%[0-9]+]]:ep = PADD_mod_pseudo [[COPY]], [[COPY4]]
    ; CHECK-NEXT: [[COPY5:%[0-9]+]]:vec256 = COPY [[COPY1]].sub_256_lo
    ; CHECK-NEXT: [[COPY6:%[0-9]+]]:vec256 = COPY [[COPY1]].sub_256_hi
    ; CHECK-NEXT: [[MOV_PD_imm10_pseudo:%[0-9]+]]:edj = MOV_PD_imm10_pseudo 56
    ; CHECK-NEXT: VST_dmw_sts_w_ag_idx [[COPY6]], [[COPY]], [[MOV_PD_imm10_pseudo]] :: (store (<16 x s16>) into unknown-address + 32)
    ; CHECK-NEXT: VST_dmw_sts_w_ag_idx [[COPY5]], [[COPY]], [[COPY3]] :: (store (<16 x s16>), align 64)
    ; CHECK-NEXT: [[MOV_PD_imm10_pseudo1:%[0-9]+]]:edj = MOV_PD_imm10_pseudo 48
    ; CHECK-NEXT: VST_dmw_sts_w_ag_idx [[COPY2]], [[COPY]], [[MOV_PD_imm10_pseudo1]] :: (store (<16 x s16>))
    ; CHECK-NEXT: PseudoRET implicit $lr, implicit [[PADD_mod_pseudo]]
    %0:_(p0) = COPY $p0
    %1:_(<32 x s16>) = COPY $x0
    %2:_(<16 x s16>) = COPY $wl2
    %3:_(s32) = G_CONSTANT i32 24
    %4:_(s20) = G_TRUNC %3
    %5:_(p0) = G_PTR_ADD %0, %4
    G_STORE %1, %5(p0) :: (store (<32 x s16>))
    %6:_(p0) = G_PTR_ADD %5, %4
    G_STORE %2, %6(p0) :: (store (<16 x s16>))
    PseudoRET implicit $lr, implicit %5
...

---
name:            store_offset_is_32_step
legalized:       true
tracksRegLiveness: true
body:             |
  bb.0:
    liveins: $p0, $x0, $wl2
    ; CHECK-LABEL: name: store_offset_is_32_step
    ; CHECK: liveins: $p0, $x0, $wl2
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: [[COPY:%[0-9]+]]:ep = COPY $p0
    ; CHECK-NEXT: [[COPY1:%[0-9]+]]:vec512 = COPY $x0
    ; CHECK-NEXT: [[COPY2:%[0-9]+]]:vec256 = COPY $wl2
    ; CHECK-NEXT: [[MOV_RLC_imm10_pseudo:%[0-9]+]]:er = MOV_RLC_imm10_pseudo 64
    ; CHECK-NEXT: [[COPY3:%[0-9]+]]:em = COPY [[MOV_RLC_imm10_pseudo]]
    ; CHECK-NEXT: [[PADD_mod_pseudo:%[0-9]+]]:ep = PADD_mod_pseudo [[COPY]], [[COPY3]]
    ; CHECK-NEXT: [[COPY4:%[0-9]+]]:vec256 = COPY [[COPY1]].sub_256_lo
    ; CHECK-NEXT: [[COPY5:%[0-9]+]]:vec256 = COPY [[COPY1]].sub_256_hi
    ; CHECK-NEXT: VST_dmw_sts_w_ag_idx_imm [[COPY5]], [[COPY]], 96 :: (store (<16 x s16>) into unknown-address + 32)
    ; CHECK-NEXT: VST_dmw_sts_w_ag_idx_imm [[COPY4]], [[COPY]], 64 :: (store (<16 x s16>), align 64)
    ; CHECK-NEXT: VST_dmw_sts_w_ag_idx_imm [[COPY2]], [[COPY]], 128 :: (store (<16 x s16>))
    ; CHECK-NEXT: PseudoRET implicit $lr, implicit [[PADD_mod_pseudo]]
    %0:_(p0) = COPY $p0
    %1:_(<32 x s16>) = COPY $x0
    %2:_(<16 x s16>) = COPY $wl2
    %3:_(s32) = G_CONSTANT i32 64
    %4:_(s20) = G_TRUNC %3
    %5:_(p0) = G_PTR_ADD %0, %4
    G_STORE %1, %5(p0) :: (store (<32 x s16>))
    %6:_(p0) = G_PTR_ADD %5, %4
    G_STORE %2, %6(p0) :: (store (<16 x s16>))
    PseudoRET implicit $lr, implicit %5
...
