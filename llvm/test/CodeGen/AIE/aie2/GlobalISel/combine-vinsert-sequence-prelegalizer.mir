# NOTE: Assertions have been autogenerated by utils/update_mir_test_checks.py
#
# This file is licensed under the Apache License v2.0 with LLVM Exceptions.
# See https://llvm.org/LICENSE.txt for license information.
# SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
#
# (c) Copyright 2024 Advanced Micro Devices, Inc. or its affiliates
# RUN: llc -mtriple aie2 -run-pass=aie2-prelegalizer-combiner %s -o - | FileCheck %s

---
name:            vinsert8-I128
legalized:       false
body:             |
  bb.1.entry:
    ; CHECK-LABEL: name: vinsert8-I128
    ; CHECK: [[C:%[0-9]+]]:_(s32) = G_CONSTANT i32 0
    ; CHECK-NEXT: [[C1:%[0-9]+]]:_(s32) = G_CONSTANT i32 1
    ; CHECK-NEXT: [[C2:%[0-9]+]]:_(s32) = G_CONSTANT i32 2
    ; CHECK-NEXT: [[C3:%[0-9]+]]:_(s32) = G_CONSTANT i32 3
    ; CHECK-NEXT: [[C4:%[0-9]+]]:_(s32) = G_CONSTANT i32 4
    ; CHECK-NEXT: [[C5:%[0-9]+]]:_(s32) = G_CONSTANT i32 5
    ; CHECK-NEXT: [[C6:%[0-9]+]]:_(s32) = G_CONSTANT i32 6
    ; CHECK-NEXT: [[C7:%[0-9]+]]:_(s32) = G_CONSTANT i32 7
    ; CHECK-NEXT: [[C8:%[0-9]+]]:_(s32) = G_CONSTANT i32 8
    ; CHECK-NEXT: [[C9:%[0-9]+]]:_(s32) = G_CONSTANT i32 9
    ; CHECK-NEXT: [[C10:%[0-9]+]]:_(s32) = G_CONSTANT i32 10
    ; CHECK-NEXT: [[C11:%[0-9]+]]:_(s32) = G_CONSTANT i32 11
    ; CHECK-NEXT: [[C12:%[0-9]+]]:_(s32) = G_CONSTANT i32 12
    ; CHECK-NEXT: [[C13:%[0-9]+]]:_(s32) = G_CONSTANT i32 13
    ; CHECK-NEXT: [[C14:%[0-9]+]]:_(s32) = G_CONSTANT i32 14
    ; CHECK-NEXT: [[C15:%[0-9]+]]:_(s32) = G_CONSTANT i32 15
    ; CHECK-NEXT: [[BUILD_VECTOR_TRUNC:%[0-9]+]]:_(<16 x s8>) = G_BUILD_VECTOR_TRUNC [[C15]](s32), [[C13]](s32), [[C11]](s32), [[C9]](s32), [[C7]](s32), [[C5]](s32), [[C3]](s32), [[C1]](s32), [[C]](s32), [[C2]](s32), [[C4]](s32), [[C6]](s32), [[C8]](s32), [[C10]](s32), [[C12]](s32), [[C14]](s32)
    ; CHECK-NEXT: [[AIE_PAD_VECTOR_UNDEF:%[0-9]+]]:_(<16 x s32>) = G_AIE_PAD_VECTOR_UNDEF [[BUILD_VECTOR_TRUNC]](<16 x s8>)
    ; CHECK-NEXT: PseudoRET implicit $lr, implicit [[AIE_PAD_VECTOR_UNDEF]](<16 x s32>)
    %0:_(<16 x s8>) = G_INTRINSIC intrinsic(@llvm.aie2.v16int8)
    %100:_(<4 x s32>) = G_BITCAST %0(<16 x s8>)
    %101:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.set.I512.I128), %100(<4 x s32>)
    %1:_(s32) = G_CONSTANT i32 0
    %2:_(s32) = G_CONSTANT i32 1
    %3:_(s32) = G_CONSTANT i32 2
    %4:_(s32) = G_CONSTANT i32 3
    %5:_(s32) = G_CONSTANT i32 4
    %6:_(s32) = G_CONSTANT i32 5
    %7:_(s32) = G_CONSTANT i32 6
    %8:_(s32) = G_CONSTANT i32 7
    %9:_(s32) = G_CONSTANT i32 8
    %10:_(s32) = G_CONSTANT i32 9
    %11:_(s32) = G_CONSTANT i32 10
    %12:_(s32) = G_CONSTANT i32 11
    %13:_(s32) = G_CONSTANT i32 12
    %14:_(s32) = G_CONSTANT i32 13
    %15:_(s32) = G_CONSTANT i32 14
    %16:_(s32) = G_CONSTANT i32 15
    %17:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %101(<16 x s32>), %1(s32), %16(s32)
    %18:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %17(<16 x s32>), %2(s32), %14(s32)
    %19:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %18(<16 x s32>), %3(s32), %12(s32)
    %20:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %19(<16 x s32>), %4(s32), %10(s32)
    %21:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %20(<16 x s32>), %5(s32), %8(s32)
    %22:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %21(<16 x s32>), %6(s32), %6(s32)
    %23:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %22(<16 x s32>), %7(s32), %4(s32)
    %24:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %23(<16 x s32>), %8(s32), %2(s32)
    %25:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %24(<16 x s32>), %9(s32), %1(s32)
    %26:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %25(<16 x s32>), %10(s32), %3(s32)
    %27:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %26(<16 x s32>), %11(s32), %5(s32)
    %28:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %27(<16 x s32>), %12(s32), %7(s32)
    %29:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %28(<16 x s32>), %13(s32), %9(s32)
    %30:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %29(<16 x s32>), %14(s32), %11(s32)
    %31:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %30(<16 x s32>), %15(s32), %13(s32)
    %32:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %31(<16 x s32>), %16(s32), %15(s32)
    PseudoRET implicit $lr, implicit %32
...

---
name:            vinsert8-I256
legalized:       false
body:             |
  bb.1.entry:
    liveins: $wl0
    ; CHECK-LABEL: name: vinsert8-I256
    ; CHECK: liveins: $wl0
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: [[C:%[0-9]+]]:_(s32) = G_CONSTANT i32 0
    ; CHECK-NEXT: [[C1:%[0-9]+]]:_(s32) = G_CONSTANT i32 1
    ; CHECK-NEXT: [[C2:%[0-9]+]]:_(s32) = G_CONSTANT i32 2
    ; CHECK-NEXT: [[C3:%[0-9]+]]:_(s32) = G_CONSTANT i32 3
    ; CHECK-NEXT: [[C4:%[0-9]+]]:_(s32) = G_CONSTANT i32 4
    ; CHECK-NEXT: [[C5:%[0-9]+]]:_(s32) = G_CONSTANT i32 5
    ; CHECK-NEXT: [[C6:%[0-9]+]]:_(s32) = G_CONSTANT i32 6
    ; CHECK-NEXT: [[C7:%[0-9]+]]:_(s32) = G_CONSTANT i32 7
    ; CHECK-NEXT: [[C8:%[0-9]+]]:_(s32) = G_CONSTANT i32 8
    ; CHECK-NEXT: [[C9:%[0-9]+]]:_(s32) = G_CONSTANT i32 9
    ; CHECK-NEXT: [[C10:%[0-9]+]]:_(s32) = G_CONSTANT i32 10
    ; CHECK-NEXT: [[C11:%[0-9]+]]:_(s32) = G_CONSTANT i32 11
    ; CHECK-NEXT: [[C12:%[0-9]+]]:_(s32) = G_CONSTANT i32 12
    ; CHECK-NEXT: [[C13:%[0-9]+]]:_(s32) = G_CONSTANT i32 13
    ; CHECK-NEXT: [[C14:%[0-9]+]]:_(s32) = G_CONSTANT i32 14
    ; CHECK-NEXT: [[C15:%[0-9]+]]:_(s32) = G_CONSTANT i32 15
    ; CHECK-NEXT: [[BUILD_VECTOR_TRUNC:%[0-9]+]]:_(<32 x s8>) = G_BUILD_VECTOR_TRUNC [[C15]](s32), [[C13]](s32), [[C11]](s32), [[C9]](s32), [[C7]](s32), [[C5]](s32), [[C3]](s32), [[C1]](s32), [[C]](s32), [[C2]](s32), [[C4]](s32), [[C6]](s32), [[C8]](s32), [[C10]](s32), [[C12]](s32), [[C14]](s32), [[C]](s32), [[C1]](s32), [[C2]](s32), [[C3]](s32), [[C4]](s32), [[C5]](s32), [[C6]](s32), [[C7]](s32), [[C8]](s32), [[C9]](s32), [[C10]](s32), [[C11]](s32), [[C12]](s32), [[C13]](s32), [[C14]](s32), [[C15]](s32)
    ; CHECK-NEXT: [[AIE_PAD_VECTOR_UNDEF:%[0-9]+]]:_(<16 x s32>) = G_AIE_PAD_VECTOR_UNDEF [[BUILD_VECTOR_TRUNC]](<32 x s8>)
    ; CHECK-NEXT: PseudoRET implicit $lr, implicit [[AIE_PAD_VECTOR_UNDEF]](<16 x s32>)
    %0:_(<8 x s32>) = COPY $wl0
    %1:_(s32) = G_CONSTANT i32 0
    %2:_(s32) = G_CONSTANT i32 1
    %3:_(s32) = G_CONSTANT i32 2
    %4:_(s32) = G_CONSTANT i32 3
    %5:_(s32) = G_CONSTANT i32 4
    %6:_(s32) = G_CONSTANT i32 5
    %7:_(s32) = G_CONSTANT i32 6
    %8:_(s32) = G_CONSTANT i32 7
    %9:_(s32) = G_CONSTANT i32 8
    %10:_(s32) = G_CONSTANT i32 9
    %11:_(s32) = G_CONSTANT i32 10
    %12:_(s32) = G_CONSTANT i32 11
    %13:_(s32) = G_CONSTANT i32 12
    %14:_(s32) = G_CONSTANT i32 13
    %15:_(s32) = G_CONSTANT i32 14
    %16:_(s32) = G_CONSTANT i32 15
    %102:_(s32) = G_CONSTANT i32 16
    %103:_(s32) = G_CONSTANT i32 17
    %104:_(s32) = G_CONSTANT i32 18
    %105:_(s32) = G_CONSTANT i32 19
    %106:_(s32) = G_CONSTANT i32 20
    %107:_(s32) = G_CONSTANT i32 21
    %108:_(s32) = G_CONSTANT i32 22
    %109:_(s32) = G_CONSTANT i32 23
    %110:_(s32) = G_CONSTANT i32 24
    %111:_(s32) = G_CONSTANT i32 25
    %112:_(s32) = G_CONSTANT i32 26
    %113:_(s32) = G_CONSTANT i32 27
    %114:_(s32) = G_CONSTANT i32 28
    %115:_(s32) = G_CONSTANT i32 29
    %116:_(s32) = G_CONSTANT i32 30
    %117:_(s32) = G_CONSTANT i32 31
    %118:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.set.I512.I256), %0(<8 x s32>), %1(s32)
    %17:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %118(<16 x s32>), %117(s32), %16(s32)
    %18:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %17(<16 x s32>), %115(s32), %14(s32)
    %19:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %18(<16 x s32>), %113(s32), %12(s32)
    %20:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %19(<16 x s32>), %111(s32), %10(s32)
    %21:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %20(<16 x s32>), %109(s32), %8(s32)
    %22:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %21(<16 x s32>), %107(s32), %6(s32)
    %23:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %22(<16 x s32>), %105(s32), %4(s32)
    %24:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %23(<16 x s32>), %103(s32), %2(s32)
    %25:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %24(<16 x s32>), %102(s32), %1(s32)
    %26:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %25(<16 x s32>), %104(s32), %3(s32)
    %27:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %26(<16 x s32>), %106(s32), %5(s32)
    %28:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %27(<16 x s32>), %108(s32), %7(s32)
    %29:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %28(<16 x s32>), %110(s32), %9(s32)
    %30:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %29(<16 x s32>), %112(s32), %11(s32)
    %31:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %30(<16 x s32>), %114(s32), %13(s32)
    %32:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %31(<16 x s32>), %116(s32), %15(s32)
    %33:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %32(<16 x s32>), %1(s32), %16(s32)
    %34:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %33(<16 x s32>), %2(s32), %14(s32)
    %35:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %34(<16 x s32>), %3(s32), %12(s32)
    %36:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %35(<16 x s32>), %4(s32), %10(s32)
    %37:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %36(<16 x s32>), %5(s32), %8(s32)
    %38:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %37(<16 x s32>), %6(s32), %6(s32)
    %39:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %38(<16 x s32>), %7(s32), %4(s32)
    %40:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %39(<16 x s32>), %8(s32), %2(s32)
    %41:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %40(<16 x s32>), %9(s32), %1(s32)
    %42:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %41(<16 x s32>), %10(s32), %3(s32)
    %43:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %42(<16 x s32>), %11(s32), %5(s32)
    %44:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %43(<16 x s32>), %12(s32), %7(s32)
    %45:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %44(<16 x s32>), %13(s32), %9(s32)
    %46:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %45(<16 x s32>), %14(s32), %11(s32)
    %47:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %46(<16 x s32>), %15(s32), %13(s32)
    %48:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %47(<16 x s32>), %16(s32), %15(s32)
    PseudoRET implicit $lr, implicit %48
...

---
name:            vinsert8-I128_idx_miss
legalized:       false
body:             |
  bb.1.entry:
    ; CHECK-LABEL: name: vinsert8-I128_idx_miss
    ; CHECK: [[INT:%[0-9]+]]:_(<16 x s8>) = G_INTRINSIC intrinsic(@llvm.aie2.v16int8)
    ; CHECK-NEXT: [[BITCAST:%[0-9]+]]:_(<4 x s32>) = G_BITCAST [[INT]](<16 x s8>)
    ; CHECK-NEXT: [[INT1:%[0-9]+]]:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.set.I512.I128), [[BITCAST]](<4 x s32>)
    ; CHECK-NEXT: [[C:%[0-9]+]]:_(s32) = G_CONSTANT i32 0
    ; CHECK-NEXT: [[C1:%[0-9]+]]:_(s32) = G_CONSTANT i32 1
    ; CHECK-NEXT: [[C2:%[0-9]+]]:_(s32) = G_CONSTANT i32 2
    ; CHECK-NEXT: [[C3:%[0-9]+]]:_(s32) = G_CONSTANT i32 3
    ; CHECK-NEXT: [[C4:%[0-9]+]]:_(s32) = G_CONSTANT i32 4
    ; CHECK-NEXT: [[C5:%[0-9]+]]:_(s32) = G_CONSTANT i32 5
    ; CHECK-NEXT: [[C6:%[0-9]+]]:_(s32) = G_CONSTANT i32 6
    ; CHECK-NEXT: [[C7:%[0-9]+]]:_(s32) = G_CONSTANT i32 7
    ; CHECK-NEXT: [[C8:%[0-9]+]]:_(s32) = G_CONSTANT i32 8
    ; CHECK-NEXT: [[C9:%[0-9]+]]:_(s32) = G_CONSTANT i32 9
    ; CHECK-NEXT: [[C10:%[0-9]+]]:_(s32) = G_CONSTANT i32 10
    ; CHECK-NEXT: [[C11:%[0-9]+]]:_(s32) = G_CONSTANT i32 11
    ; CHECK-NEXT: [[C12:%[0-9]+]]:_(s32) = G_CONSTANT i32 12
    ; CHECK-NEXT: [[C13:%[0-9]+]]:_(s32) = G_CONSTANT i32 13
    ; CHECK-NEXT: [[C14:%[0-9]+]]:_(s32) = G_CONSTANT i32 14
    ; CHECK-NEXT: [[C15:%[0-9]+]]:_(s32) = G_CONSTANT i32 15
    ; CHECK-NEXT: [[INT2:%[0-9]+]]:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), [[INT1]](<16 x s32>), [[C]](s32), [[C15]](s32)
    ; CHECK-NEXT: [[INT3:%[0-9]+]]:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), [[INT2]](<16 x s32>), [[C1]](s32), [[C13]](s32)
    ; CHECK-NEXT: [[INT4:%[0-9]+]]:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), [[INT3]](<16 x s32>), [[C2]](s32), [[C11]](s32)
    ; CHECK-NEXT: [[INT5:%[0-9]+]]:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), [[INT4]](<16 x s32>), [[C3]](s32), [[C9]](s32)
    ; CHECK-NEXT: [[INT6:%[0-9]+]]:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), [[INT5]](<16 x s32>), [[C4]](s32), [[C7]](s32)
    ; CHECK-NEXT: [[INT7:%[0-9]+]]:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), [[INT6]](<16 x s32>), [[C5]](s32), [[C5]](s32)
    ; CHECK-NEXT: [[INT8:%[0-9]+]]:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), [[INT7]](<16 x s32>), [[C6]](s32), [[C3]](s32)
    ; CHECK-NEXT: [[INT9:%[0-9]+]]:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), [[INT8]](<16 x s32>), [[C7]](s32), [[C1]](s32)
    ; CHECK-NEXT: [[INT10:%[0-9]+]]:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), [[INT9]](<16 x s32>), [[C8]](s32), [[C]](s32)
    ; CHECK-NEXT: [[INT11:%[0-9]+]]:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), [[INT10]](<16 x s32>), [[C10]](s32), [[C4]](s32)
    ; CHECK-NEXT: [[INT12:%[0-9]+]]:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), [[INT11]](<16 x s32>), [[C11]](s32), [[C6]](s32)
    ; CHECK-NEXT: [[INT13:%[0-9]+]]:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), [[INT12]](<16 x s32>), [[C12]](s32), [[C8]](s32)
    ; CHECK-NEXT: [[INT14:%[0-9]+]]:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), [[INT13]](<16 x s32>), [[C13]](s32), [[C10]](s32)
    ; CHECK-NEXT: [[INT15:%[0-9]+]]:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), [[INT14]](<16 x s32>), [[C14]](s32), [[C12]](s32)
    ; CHECK-NEXT: [[INT16:%[0-9]+]]:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), [[INT15]](<16 x s32>), [[C15]](s32), [[C14]](s32)
    ; CHECK-NEXT: PseudoRET implicit $lr, implicit [[INT16]](<16 x s32>)
    %0:_(<16 x s8>) = G_INTRINSIC intrinsic(@llvm.aie2.v16int8)
    %100:_(<4 x s32>) = G_BITCAST %0(<16 x s8>)
    %101:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.set.I512.I128), %100(<4 x s32>)
    %1:_(s32) = G_CONSTANT i32 0
    %2:_(s32) = G_CONSTANT i32 1
    %3:_(s32) = G_CONSTANT i32 2
    %4:_(s32) = G_CONSTANT i32 3
    %5:_(s32) = G_CONSTANT i32 4
    %6:_(s32) = G_CONSTANT i32 5
    %7:_(s32) = G_CONSTANT i32 6
    %8:_(s32) = G_CONSTANT i32 7
    %9:_(s32) = G_CONSTANT i32 8
    %10:_(s32) = G_CONSTANT i32 9
    %11:_(s32) = G_CONSTANT i32 10
    %12:_(s32) = G_CONSTANT i32 11
    %13:_(s32) = G_CONSTANT i32 12
    %14:_(s32) = G_CONSTANT i32 13
    %15:_(s32) = G_CONSTANT i32 14
    %16:_(s32) = G_CONSTANT i32 15
    %17:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %101(<16 x s32>), %1(s32), %16(s32)
    %18:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %17(<16 x s32>), %2(s32), %14(s32)
    %19:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %18(<16 x s32>), %3(s32), %12(s32)
    %20:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %19(<16 x s32>), %4(s32), %10(s32)
    %21:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %20(<16 x s32>), %5(s32), %8(s32)
    %22:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %21(<16 x s32>), %6(s32), %6(s32)
    %23:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %22(<16 x s32>), %7(s32), %4(s32)
    %24:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %23(<16 x s32>), %8(s32), %2(s32)
    %25:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %24(<16 x s32>), %9(s32), %1(s32)
    %27:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %25(<16 x s32>), %11(s32), %5(s32)
    %28:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %27(<16 x s32>), %12(s32), %7(s32)
    %29:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %28(<16 x s32>), %13(s32), %9(s32)
    %30:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %29(<16 x s32>), %14(s32), %11(s32)
    %31:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %30(<16 x s32>), %15(s32), %13(s32)
    %32:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %31(<16 x s32>), %16(s32), %15(s32)
    PseudoRET implicit $lr, implicit %32
...

---
name:            vinsert8-I128_idx_multiple
legalized:       false
body:             |
  bb.1.entry:
    ; CHECK-LABEL: name: vinsert8-I128_idx_multiple
    ; CHECK: [[INT:%[0-9]+]]:_(<16 x s8>) = G_INTRINSIC intrinsic(@llvm.aie2.v16int8)
    ; CHECK-NEXT: [[BITCAST:%[0-9]+]]:_(<4 x s32>) = G_BITCAST [[INT]](<16 x s8>)
    ; CHECK-NEXT: [[INT1:%[0-9]+]]:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.set.I512.I128), [[BITCAST]](<4 x s32>)
    ; CHECK-NEXT: [[C:%[0-9]+]]:_(s32) = G_CONSTANT i32 0
    ; CHECK-NEXT: [[C1:%[0-9]+]]:_(s32) = G_CONSTANT i32 1
    ; CHECK-NEXT: [[C2:%[0-9]+]]:_(s32) = G_CONSTANT i32 2
    ; CHECK-NEXT: [[C3:%[0-9]+]]:_(s32) = G_CONSTANT i32 3
    ; CHECK-NEXT: [[C4:%[0-9]+]]:_(s32) = G_CONSTANT i32 4
    ; CHECK-NEXT: [[C5:%[0-9]+]]:_(s32) = G_CONSTANT i32 5
    ; CHECK-NEXT: [[C6:%[0-9]+]]:_(s32) = G_CONSTANT i32 6
    ; CHECK-NEXT: [[C7:%[0-9]+]]:_(s32) = G_CONSTANT i32 7
    ; CHECK-NEXT: [[C8:%[0-9]+]]:_(s32) = G_CONSTANT i32 8
    ; CHECK-NEXT: [[C9:%[0-9]+]]:_(s32) = G_CONSTANT i32 9
    ; CHECK-NEXT: [[C10:%[0-9]+]]:_(s32) = G_CONSTANT i32 10
    ; CHECK-NEXT: [[C11:%[0-9]+]]:_(s32) = G_CONSTANT i32 11
    ; CHECK-NEXT: [[C12:%[0-9]+]]:_(s32) = G_CONSTANT i32 12
    ; CHECK-NEXT: [[C13:%[0-9]+]]:_(s32) = G_CONSTANT i32 13
    ; CHECK-NEXT: [[C14:%[0-9]+]]:_(s32) = G_CONSTANT i32 14
    ; CHECK-NEXT: [[C15:%[0-9]+]]:_(s32) = G_CONSTANT i32 15
    ; CHECK-NEXT: [[INT2:%[0-9]+]]:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), [[INT1]](<16 x s32>), [[C]](s32), [[C15]](s32)
    ; CHECK-NEXT: [[INT3:%[0-9]+]]:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), [[INT2]](<16 x s32>), [[C1]](s32), [[C13]](s32)
    ; CHECK-NEXT: [[INT4:%[0-9]+]]:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), [[INT3]](<16 x s32>), [[C2]](s32), [[C11]](s32)
    ; CHECK-NEXT: [[INT5:%[0-9]+]]:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), [[INT4]](<16 x s32>), [[C3]](s32), [[C9]](s32)
    ; CHECK-NEXT: [[INT6:%[0-9]+]]:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), [[INT5]](<16 x s32>), [[C4]](s32), [[C7]](s32)
    ; CHECK-NEXT: [[INT7:%[0-9]+]]:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), [[INT6]](<16 x s32>), [[C5]](s32), [[C5]](s32)
    ; CHECK-NEXT: [[INT8:%[0-9]+]]:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), [[INT7]](<16 x s32>), [[C6]](s32), [[C3]](s32)
    ; CHECK-NEXT: [[INT9:%[0-9]+]]:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), [[INT8]](<16 x s32>), [[C7]](s32), [[C1]](s32)
    ; CHECK-NEXT: [[INT10:%[0-9]+]]:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), [[INT9]](<16 x s32>), [[C8]](s32), [[C]](s32)
    ; CHECK-NEXT: [[INT11:%[0-9]+]]:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), [[INT10]](<16 x s32>), [[C9]](s32), [[C2]](s32)
    ; CHECK-NEXT: [[INT12:%[0-9]+]]:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), [[INT11]](<16 x s32>), [[C10]](s32), [[C4]](s32)
    ; CHECK-NEXT: [[INT13:%[0-9]+]]:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), [[INT12]](<16 x s32>), [[C11]](s32), [[C6]](s32)
    ; CHECK-NEXT: [[INT14:%[0-9]+]]:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), [[INT13]](<16 x s32>), [[C12]](s32), [[C8]](s32)
    ; CHECK-NEXT: [[INT15:%[0-9]+]]:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), [[INT14]](<16 x s32>), [[C8]](s32), [[C15]](s32)
    ; CHECK-NEXT: [[INT16:%[0-9]+]]:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), [[INT15]](<16 x s32>), [[C13]](s32), [[C10]](s32)
    ; CHECK-NEXT: [[INT17:%[0-9]+]]:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), [[INT16]](<16 x s32>), [[C14]](s32), [[C12]](s32)
    ; CHECK-NEXT: [[INT18:%[0-9]+]]:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), [[INT17]](<16 x s32>), [[C15]](s32), [[C14]](s32)
    ; CHECK-NEXT: PseudoRET implicit $lr, implicit [[INT18]](<16 x s32>)
    %0:_(<16 x s8>) = G_INTRINSIC intrinsic(@llvm.aie2.v16int8)
    %100:_(<4 x s32>) = G_BITCAST %0(<16 x s8>)
    %101:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.set.I512.I128), %100(<4 x s32>)
    %1:_(s32) = G_CONSTANT i32 0
    %2:_(s32) = G_CONSTANT i32 1
    %3:_(s32) = G_CONSTANT i32 2
    %4:_(s32) = G_CONSTANT i32 3
    %5:_(s32) = G_CONSTANT i32 4
    %6:_(s32) = G_CONSTANT i32 5
    %7:_(s32) = G_CONSTANT i32 6
    %8:_(s32) = G_CONSTANT i32 7
    %9:_(s32) = G_CONSTANT i32 8
    %10:_(s32) = G_CONSTANT i32 9
    %11:_(s32) = G_CONSTANT i32 10
    %12:_(s32) = G_CONSTANT i32 11
    %13:_(s32) = G_CONSTANT i32 12
    %14:_(s32) = G_CONSTANT i32 13
    %15:_(s32) = G_CONSTANT i32 14
    %16:_(s32) = G_CONSTANT i32 15
    %17:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %101(<16 x s32>), %1(s32), %16(s32)
    %18:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %17(<16 x s32>), %2(s32), %14(s32)
    %19:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %18(<16 x s32>), %3(s32), %12(s32)
    %20:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %19(<16 x s32>), %4(s32), %10(s32)
    %21:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %20(<16 x s32>), %5(s32), %8(s32)
    %22:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %21(<16 x s32>), %6(s32), %6(s32)
    %23:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %22(<16 x s32>), %7(s32), %4(s32)
    %24:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %23(<16 x s32>), %8(s32), %2(s32)
    %25:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %24(<16 x s32>), %9(s32), %1(s32)
    %26:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %25(<16 x s32>), %10(s32), %3(s32)
    %27:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %26(<16 x s32>), %11(s32), %5(s32)
    %28:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %27(<16 x s32>), %12(s32), %7(s32)
    %29:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %28(<16 x s32>), %13(s32), %9(s32)
    %125:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %29(<16 x s32>), %9(s32), %16(s32)
    %30:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %125(<16 x s32>), %14(s32), %11(s32)
    %31:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %30(<16 x s32>), %15(s32), %13(s32)
    %32:_(<16 x s32>) = G_INTRINSIC intrinsic(@llvm.aie2.vinsert8.I512), %31(<16 x s32>), %16(s32), %15(s32)
    PseudoRET implicit $lr, implicit %32
...
