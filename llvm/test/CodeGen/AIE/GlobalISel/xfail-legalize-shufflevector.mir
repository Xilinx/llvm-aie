# NOTE: Assertions have been autogenerated by utils/update_mir_test_checks.py
#
# This file is licensed under the Apache License v2.0 with LLVM Exceptions.
# See https://llvm.org/LICENSE.txt for license information.
# SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
#
# (c) Copyright 2023-2024 Advanced Micro Devices, Inc. or its affiliates
# RUN: not --crash llc -mtriple aie -run-pass=legalizer %s -verify-machineinstrs -o - 2>&1
# RUN: not --crash llc -mtriple aie2 -run-pass=legalizer %s -verify-machineinstrs -o - 2>&1

---
name:            f_1x16
body:             |
  bb.0:
    %1:_(s16) = G_IMPLICIT_DEF
    %2:_(p0) = G_IMPLICIT_DEF
    %0:_(s16) = G_SHUFFLE_VECTOR %1(s16), %1, shufflemask(undef)
    G_STORE %0(s16), %2(p0) :: (store (s16) into `<1 x i16>* undef`)
    PseudoRET implicit $lr
...

---
name:            f_64x1
body:             |
  bb.0:
    %1:_(<64 x s1>) = G_IMPLICIT_DEF
    %2:_(p0) = G_IMPLICIT_DEF
    %0:_(<64 x s1>) = G_SHUFFLE_VECTOR %1(<64 x s1>), %1, shufflemask(undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef)
    G_STORE %0(<64 x s1>), %2(p0) :: (store (<64 x s1>) into `<64 x i1>* undef`, align 2)
    PseudoRET implicit $lr
...

---
name:            f_32x8
body:             |
  bb.0:
    %1:_(<32 x s8>) = G_IMPLICIT_DEF
    %2:_(p0) = G_IMPLICIT_DEF
    %0:_(<32 x s8>) = G_SHUFFLE_VECTOR %1(<32 x s8>), %1, shufflemask(undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef)
    G_STORE %0(<32 x s8>), %2(p0) :: (store (<32 x s8>) into `<32 x i8>* undef`, align 2)
    PseudoRET implicit $lr
...

---
name:            f_16x16
body:             |
  bb.0:
    %1:_(<16 x s16>) = G_IMPLICIT_DEF
    %2:_(p0) = G_IMPLICIT_DEF
    %0:_(<16 x s16>) = G_SHUFFLE_VECTOR %1(<16 x s16>), %1, shufflemask(undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef)
    G_STORE %0(<16 x s16>), %2(p0) :: (store (<16 x s16>) into `<16 x i16>* undef`, align 2)
    PseudoRET implicit $lr
...

---
name:            f_32x16
body:             |
  bb.0:
    %1:_(<32 x s16>) = G_IMPLICIT_DEF
    %2:_(p0) = G_IMPLICIT_DEF
    %0:_(<32 x s16>) = G_SHUFFLE_VECTOR %1(<32 x s16>), %1, shufflemask(undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef, undef)
    G_STORE %0(<32 x s16>), %2(p0) :: (store (<32 x s16>) into `<32 x i16>* undef`, align 2)
    PseudoRET implicit $lr
...

---
name:            f_2x64
body:             |
  bb.0:
    %1:_(<2 x s64>) = G_IMPLICIT_DEF
    %2:_(p0) = G_IMPLICIT_DEF
    %0:_(<2 x s64>) = G_SHUFFLE_VECTOR %1(<2 x s64>), %1, shufflemask(undef, undef)
    G_STORE %0(<2 x s64>), %2(p0) :: (store (<2 x s64>) into `<2 x i64>* undef`, align 2)
    PseudoRET implicit $lr
...
