// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py
//===- aie2-intrinsics.cpp --------------------------------------*- C++ -*-===//
//
// This file is licensed under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
// (c) Copyright 2023-2024 Advanced Micro Devices, Inc. or its affiliates
//
//===----------------------------------------------------------------------===//
// RUN: %clang -O0 %s --target=aie2 -nostdlibinc -S -emit-llvm -o - | FileCheck %s

// CHECK-LABEL: @_Z21test_mac_4x2_2x4_confiiiiii(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR_I:%.*]] = alloca <16 x i32>, align 32
// CHECK-NEXT:    [[SGN_X_ADDR_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[B_ADDR_I:%.*]] = alloca <32 x i16>, align 32
// CHECK-NEXT:    [[SGN_Y_ADDR_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[ACC1_ADDR_I:%.*]] = alloca <16 x i64>, align 32
// CHECK-NEXT:    [[ZERO_ACC1_ADDR_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[SHIFT16_ADDR_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[SUB_MUL_ADDR_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[SUB_ACC1_ADDR_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[CONF_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[SGN_X_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[SGN_Y_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[ZERO_ACC1_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[SHIFT16_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[SUB_MUL_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[SUB_ACC1_ADDR:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[A:%.*]] = alloca <16 x i32>, align 32
// CHECK-NEXT:    [[B:%.*]] = alloca <32 x i16>, align 32
// CHECK-NEXT:    [[ACC1:%.*]] = alloca <16 x i64>, align 32
// CHECK-NEXT:    store i32 [[SGN_X:%.*]], ptr [[SGN_X_ADDR]], align 4
// CHECK-NEXT:    store i32 [[SGN_Y:%.*]], ptr [[SGN_Y_ADDR]], align 4
// CHECK-NEXT:    store i32 [[ZERO_ACC1:%.*]], ptr [[ZERO_ACC1_ADDR]], align 4
// CHECK-NEXT:    store i32 [[SHIFT16:%.*]], ptr [[SHIFT16_ADDR]], align 4
// CHECK-NEXT:    store i32 [[SUB_MUL:%.*]], ptr [[SUB_MUL_ADDR]], align 4
// CHECK-NEXT:    store i32 [[SUB_ACC1:%.*]], ptr [[SUB_ACC1_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = call noundef <16 x i32> @llvm.aie2.v16int32()
// CHECK-NEXT:    store <16 x i32> [[TMP0]], ptr [[A]], align 32
// CHECK-NEXT:    [[TMP1:%.*]] = call noundef <32 x i16> @llvm.aie2.v32int16()
// CHECK-NEXT:    store <32 x i16> [[TMP1]], ptr [[B]], align 32
// CHECK-NEXT:    [[TMP2:%.*]] = call noundef <16 x i64> @llvm.aie2.v16acc64()
// CHECK-NEXT:    store <16 x i64> [[TMP2]], ptr [[ACC1]], align 32
// CHECK-NEXT:    [[TMP3:%.*]] = load <16 x i32>, ptr [[A]], align 32
// CHECK-NEXT:    [[TMP4:%.*]] = load i32, ptr [[SGN_X_ADDR]], align 4
// CHECK-NEXT:    [[TMP5:%.*]] = load <32 x i16>, ptr [[B]], align 32
// CHECK-NEXT:    [[TMP6:%.*]] = load i32, ptr [[SGN_Y_ADDR]], align 4
// CHECK-NEXT:    [[TMP7:%.*]] = load <16 x i64>, ptr [[ACC1]], align 32
// CHECK-NEXT:    [[TMP8:%.*]] = load i32, ptr [[ZERO_ACC1_ADDR]], align 4
// CHECK-NEXT:    [[TMP9:%.*]] = load i32, ptr [[SHIFT16_ADDR]], align 4
// CHECK-NEXT:    [[TMP10:%.*]] = load i32, ptr [[SUB_MUL_ADDR]], align 4
// CHECK-NEXT:    [[TMP11:%.*]] = load i32, ptr [[SUB_ACC1_ADDR]], align 4
// CHECK-NEXT:    store <16 x i32> [[TMP3]], ptr [[A_ADDR_I]], align 32
// CHECK-NEXT:    store i32 [[TMP4]], ptr [[SGN_X_ADDR_I]], align 4
// CHECK-NEXT:    store <32 x i16> [[TMP5]], ptr [[B_ADDR_I]], align 32
// CHECK-NEXT:    store i32 [[TMP6]], ptr [[SGN_Y_ADDR_I]], align 4
// CHECK-NEXT:    store <16 x i64> [[TMP7]], ptr [[ACC1_ADDR_I]], align 32
// CHECK-NEXT:    store i32 [[TMP8]], ptr [[ZERO_ACC1_ADDR_I]], align 4
// CHECK-NEXT:    store i32 [[TMP9]], ptr [[SHIFT16_ADDR_I]], align 4
// CHECK-NEXT:    store i32 [[TMP10]], ptr [[SUB_MUL_ADDR_I]], align 4
// CHECK-NEXT:    store i32 [[TMP11]], ptr [[SUB_ACC1_ADDR_I]], align 4
// CHECK-NEXT:    [[TMP12:%.*]] = load i32, ptr [[SGN_X_ADDR_I]], align 4
// CHECK-NEXT:    [[TMP13:%.*]] = load i32, ptr [[SGN_Y_ADDR_I]], align 4
// CHECK-NEXT:    [[TMP14:%.*]] = load i32, ptr [[ZERO_ACC1_ADDR_I]], align 4
// CHECK-NEXT:    [[TMP15:%.*]] = load i32, ptr [[SHIFT16_ADDR_I]], align 4
// CHECK-NEXT:    [[TMP16:%.*]] = load i32, ptr [[SUB_MUL_ADDR_I]], align 4
// CHECK-NEXT:    [[TMP17:%.*]] = load i32, ptr [[SUB_ACC1_ADDR_I]], align 4
// CHECK-NEXT:    [[CALL_I:%.*]] = call noundef i32 @_ZL21aiev2_compute_controliiiiiiiiiii(i32 noundef [[TMP12]], i32 noundef [[TMP13]], i32 noundef 1, i32 noundef 0, i32 noundef 0, i32 noundef [[TMP14]], i32 noundef [[TMP15]], i32 noundef [[TMP16]], i32 noundef [[TMP17]], i32 noundef 0, i32 noundef 0)
// CHECK-NEXT:    store i32 [[CALL_I]], ptr [[CONF_I]], align 4
// CHECK-NEXT:    [[TMP18:%.*]] = load <16 x i32>, ptr [[A_ADDR_I]], align 32
// CHECK-NEXT:    [[TMP19:%.*]] = bitcast <16 x i32> [[TMP18]] to <64 x i8>
// CHECK-NEXT:    [[TMP20:%.*]] = load <32 x i16>, ptr [[B_ADDR_I]], align 32
// CHECK-NEXT:    [[TMP21:%.*]] = bitcast <32 x i16> [[TMP20]] to <16 x i32>
// CHECK-NEXT:    [[TMP22:%.*]] = load <16 x i64>, ptr [[ACC1_ADDR_I]], align 32
// CHECK-NEXT:    [[TMP23:%.*]] = load i32, ptr [[CONF_I]], align 4
// CHECK-NEXT:    [[TMP24:%.*]] = call noundef <16 x i64> @llvm.aie2.I512.I512.ACC1024.acc64.mac.conf(<64 x i8> [[TMP19]], <16 x i32> [[TMP21]], <16 x i64> [[TMP22]], i32 [[TMP23]])
// CHECK-NEXT:    ret <16 x i64> [[TMP24]]
//
v16acc64 test_mac_4x2_2x4_conf(int sgn_x, int sgn_y, int zero_acc1, int shift16, int sub_mul, int sub_acc1) {
  v16int32 a = undef_v16int32();
  v32int16 b = undef_v32int16();
  v16acc64 acc1 = undef_v16acc64();
  return mac_4x2_2x4_conf(a, sgn_x, b, sgn_y, acc1,zero_acc1, shift16, sub_mul, sub_acc1);
}
// CHECK-LABEL: @_Z18test_mul_4x16_16x8v(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR_I:%.*]] = alloca <64 x i8>, align 32
// CHECK-NEXT:    [[B_ADDR_I:%.*]] = alloca <64 x i8>, align 32
// CHECK-NEXT:    [[CONF_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[A:%.*]] = alloca <64 x i8>, align 32
// CHECK-NEXT:    [[B:%.*]] = alloca <64 x i8>, align 32
// CHECK-NEXT:    [[TMP0:%.*]] = call noundef <64 x i8> @llvm.aie2.v64int8()
// CHECK-NEXT:    store <64 x i8> [[TMP0]], ptr [[A]], align 32
// CHECK-NEXT:    [[TMP1:%.*]] = call <16 x i32> @llvm.aie2.v16int32()
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <16 x i32> [[TMP1]] to <64 x i8>
// CHECK-NEXT:    store <64 x i8> [[TMP2]], ptr [[B]], align 32
// CHECK-NEXT:    [[TMP3:%.*]] = load <64 x i8>, ptr [[A]], align 32
// CHECK-NEXT:    [[TMP4:%.*]] = load <64 x i8>, ptr [[B]], align 32
// CHECK-NEXT:    store <64 x i8> [[TMP3]], ptr [[A_ADDR_I]], align 32
// CHECK-NEXT:    store <64 x i8> [[TMP4]], ptr [[B_ADDR_I]], align 32
// CHECK-NEXT:    [[CALL_I:%.*]] = call noundef i32 @_ZL21aiev2_compute_controliiiiiiiiiii(i32 noundef 0, i32 noundef 0, i32 noundef 0, i32 noundef 0, i32 noundef 0, i32 noundef 0, i32 noundef 0, i32 noundef 0, i32 noundef 0, i32 noundef 0, i32 noundef 0)
// CHECK-NEXT:    store i32 [[CALL_I]], ptr [[CONF_I]], align 4
// CHECK-NEXT:    [[TMP5:%.*]] = load <64 x i8>, ptr [[A_ADDR_I]], align 32
// CHECK-NEXT:    [[TMP6:%.*]] = load <64 x i8>, ptr [[B_ADDR_I]], align 32
// CHECK-NEXT:    [[TMP7:%.*]] = bitcast <64 x i8> [[TMP6]] to <16 x i32>
// CHECK-NEXT:    [[TMP8:%.*]] = load i32, ptr [[CONF_I]], align 4
// CHECK-NEXT:    [[TMP9:%.*]] = call noundef <16 x i64> @llvm.aie2.I512.I512.acc32.mul.conf(<64 x i8> [[TMP5]], <16 x i32> [[TMP7]], i32 [[TMP8]])
// CHECK-NEXT:    ret <16 x i64> [[TMP9]]
//
v32acc32 test_mul_4x16_16x8() {
  v64uint8 a = undef_v64uint8();
  v128uint4 b = undef_v128uint4();
  return mul_4x16_16x8(a, b);
}
// CHECK-LABEL: @_Z21test_negmsc_4x16_16x8Dv64_hDv64_DU8_Dv32_u7__acc32(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR_I:%.*]] = alloca <64 x i8>, align 32
// CHECK-NEXT:    [[B_ADDR_I:%.*]] = alloca <64 x i8>, align 32
// CHECK-NEXT:    [[ACC1_ADDR_I:%.*]] = alloca <16 x i64>, align 32
// CHECK-NEXT:    [[CONF_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <64 x i8>, align 32
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <64 x i8>, align 32
// CHECK-NEXT:    [[ACC1_ADDR:%.*]] = alloca <16 x i64>, align 32
// CHECK-NEXT:    store <64 x i8> [[A:%.*]], ptr [[A_ADDR]], align 32
// CHECK-NEXT:    store <64 x i8> [[B:%.*]], ptr [[B_ADDR]], align 32
// CHECK-NEXT:    store <16 x i64> [[ACC1:%.*]], ptr [[ACC1_ADDR]], align 32
// CHECK-NEXT:    [[TMP0:%.*]] = load <64 x i8>, ptr [[A_ADDR]], align 32
// CHECK-NEXT:    [[TMP1:%.*]] = load <64 x i8>, ptr [[B_ADDR]], align 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <16 x i64>, ptr [[ACC1_ADDR]], align 32
// CHECK-NEXT:    store <64 x i8> [[TMP0]], ptr [[A_ADDR_I]], align 32
// CHECK-NEXT:    store <64 x i8> [[TMP1]], ptr [[B_ADDR_I]], align 32
// CHECK-NEXT:    store <16 x i64> [[TMP2]], ptr [[ACC1_ADDR_I]], align 32
// CHECK-NEXT:    [[CALL_I:%.*]] = call noundef i32 @_ZL21aiev2_compute_controliiiiiiiiiii(i32 noundef 0, i32 noundef 0, i32 noundef 0, i32 noundef 0, i32 noundef 0, i32 noundef 0, i32 noundef 0, i32 noundef 0, i32 noundef 0, i32 noundef 0, i32 noundef 0)
// CHECK-NEXT:    store i32 [[CALL_I]], ptr [[CONF_I]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = load <64 x i8>, ptr [[A_ADDR_I]], align 32
// CHECK-NEXT:    [[TMP4:%.*]] = load <64 x i8>, ptr [[B_ADDR_I]], align 32
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <64 x i8> [[TMP4]] to <16 x i32>
// CHECK-NEXT:    [[TMP6:%.*]] = load <16 x i64>, ptr [[ACC1_ADDR_I]], align 32
// CHECK-NEXT:    [[TMP7:%.*]] = load i32, ptr [[CONF_I]], align 4
// CHECK-NEXT:    [[TMP8:%.*]] = call noundef <16 x i64> @llvm.aie2.I512.I512.ACC1024.acc32.negmsc.conf(<64 x i8> [[TMP3]], <16 x i32> [[TMP5]], <16 x i64> [[TMP6]], i32 [[TMP7]])
// CHECK-NEXT:    ret <16 x i64> [[TMP8]]
//
v32acc32 test_negmsc_4x16_16x8(v64uint8 a, v128uint4 b, v32acc32 acc1) {
  return negmsc_4x16_16x8(a, b , acc1);
}
// CHECK-LABEL: @_Z19test_negmsc_2x8_8x8Dv32_tDv64_hDv16_u7__acc64(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR_I:%.*]] = alloca <32 x i16>, align 32
// CHECK-NEXT:    [[B_ADDR_I:%.*]] = alloca <64 x i8>, align 32
// CHECK-NEXT:    [[ACC1_ADDR_I:%.*]] = alloca <16 x i64>, align 32
// CHECK-NEXT:    [[CONF_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x i16>, align 32
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <64 x i8>, align 32
// CHECK-NEXT:    [[ACC1_ADDR:%.*]] = alloca <16 x i64>, align 32
// CHECK-NEXT:    store <32 x i16> [[A:%.*]], ptr [[A_ADDR]], align 32
// CHECK-NEXT:    store <64 x i8> [[B:%.*]], ptr [[B_ADDR]], align 32
// CHECK-NEXT:    store <16 x i64> [[ACC1:%.*]], ptr [[ACC1_ADDR]], align 32
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x i16>, ptr [[A_ADDR]], align 32
// CHECK-NEXT:    [[TMP1:%.*]] = load <64 x i8>, ptr [[B_ADDR]], align 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <16 x i64>, ptr [[ACC1_ADDR]], align 32
// CHECK-NEXT:    store <32 x i16> [[TMP0]], ptr [[A_ADDR_I]], align 32
// CHECK-NEXT:    store <64 x i8> [[TMP1]], ptr [[B_ADDR_I]], align 32
// CHECK-NEXT:    store <16 x i64> [[TMP2]], ptr [[ACC1_ADDR_I]], align 32
// CHECK-NEXT:    [[CALL_I:%.*]] = call noundef i32 @_ZL21aiev2_compute_controliiiiiiiiiii(i32 noundef 0, i32 noundef 0, i32 noundef 1, i32 noundef 2, i32 noundef 0, i32 noundef 0, i32 noundef 0, i32 noundef 0, i32 noundef 0, i32 noundef 0, i32 noundef 0)
// CHECK-NEXT:    store i32 [[CALL_I]], ptr [[CONF_I]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = load <32 x i16>, ptr [[A_ADDR_I]], align 32
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <32 x i16> [[TMP3]] to <64 x i8>
// CHECK-NEXT:    [[TMP5:%.*]] = load <64 x i8>, ptr [[B_ADDR_I]], align 32
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast <64 x i8> [[TMP5]] to <16 x i32>
// CHECK-NEXT:    [[TMP7:%.*]] = load <16 x i64>, ptr [[ACC1_ADDR_I]], align 32
// CHECK-NEXT:    [[TMP8:%.*]] = load i32, ptr [[CONF_I]], align 4
// CHECK-NEXT:    [[TMP9:%.*]] = call noundef <16 x i64> @llvm.aie2.I512.I512.ACC1024.acc64.negmsc.conf(<64 x i8> [[TMP4]], <16 x i32> [[TMP6]], <16 x i64> [[TMP7]], i32 [[TMP8]])
// CHECK-NEXT:    ret <16 x i64> [[TMP9]]
//
v16acc64 test_negmsc_2x8_8x8(v32uint16 a, v64uint8 b, v16acc64 acc1) {
  return negmsc_2x8_8x8(a, b, acc1);
}
// CHECK-LABEL: @_Z19test_negmsc_elem_32Dv32_tS_Dv32_u7__acc32(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[A_ADDR_I:%.*]] = alloca <32 x i16>, align 32
// CHECK-NEXT:    [[B_ADDR_I:%.*]] = alloca <32 x i16>, align 32
// CHECK-NEXT:    [[ACC1_ADDR_I:%.*]] = alloca <16 x i64>, align 32
// CHECK-NEXT:    [[CONF_I:%.*]] = alloca i32, align 4
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <32 x i16>, align 32
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca <32 x i16>, align 32
// CHECK-NEXT:    [[ACC1_ADDR:%.*]] = alloca <16 x i64>, align 32
// CHECK-NEXT:    store <32 x i16> [[A:%.*]], ptr [[A_ADDR]], align 32
// CHECK-NEXT:    store <32 x i16> [[B:%.*]], ptr [[B_ADDR]], align 32
// CHECK-NEXT:    store <16 x i64> [[ACC1:%.*]], ptr [[ACC1_ADDR]], align 32
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x i16>, ptr [[A_ADDR]], align 32
// CHECK-NEXT:    [[TMP1:%.*]] = load <32 x i16>, ptr [[B_ADDR]], align 32
// CHECK-NEXT:    [[TMP2:%.*]] = load <16 x i64>, ptr [[ACC1_ADDR]], align 32
// CHECK-NEXT:    store <32 x i16> [[TMP0]], ptr [[A_ADDR_I]], align 32
// CHECK-NEXT:    store <32 x i16> [[TMP1]], ptr [[B_ADDR_I]], align 32
// CHECK-NEXT:    store <16 x i64> [[TMP2]], ptr [[ACC1_ADDR_I]], align 32
// CHECK-NEXT:    [[CALL_I:%.*]] = call noundef i32 @_ZL21aiev2_compute_controliiiiiiiiiii(i32 noundef 0, i32 noundef 0, i32 noundef 0, i32 noundef 3, i32 noundef 1, i32 noundef 0, i32 noundef 0, i32 noundef 0, i32 noundef 0, i32 noundef 0, i32 noundef 0)
// CHECK-NEXT:    store i32 [[CALL_I]], ptr [[CONF_I]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = load <32 x i16>, ptr [[A_ADDR_I]], align 32
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <32 x i16> [[TMP3]] to <64 x i8>
// CHECK-NEXT:    [[TMP5:%.*]] = load <32 x i16>, ptr [[B_ADDR_I]], align 32
// CHECK-NEXT:    [[TMP6:%.*]] = bitcast <32 x i16> [[TMP5]] to <16 x i32>
// CHECK-NEXT:    [[TMP7:%.*]] = load <16 x i64>, ptr [[ACC1_ADDR_I]], align 32
// CHECK-NEXT:    [[TMP8:%.*]] = load i32, ptr [[CONF_I]], align 4
// CHECK-NEXT:    [[TMP9:%.*]] = call noundef <16 x i64> @llvm.aie2.I512.I512.ACC1024.acc32.negmsc.conf(<64 x i8> [[TMP4]], <16 x i32> [[TMP6]], <16 x i64> [[TMP7]], i32 [[TMP8]])
// CHECK-NEXT:    ret <16 x i64> [[TMP9]]
//
v32acc32 test_negmsc_elem_32(v32uint16 a, v32uint16 b, v32acc32 acc1) {
  return negmsc_elem_32(a, b, acc1);
}
