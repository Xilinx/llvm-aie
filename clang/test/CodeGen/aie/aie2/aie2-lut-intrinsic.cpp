// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py
//===- aie2-lut-intrinsic.cpp -----------------------------------*- C++ -*-===//
//
// This file is licensed under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
// (c) Copyright 2023-2024 Advanced Micro Devices, Inc. or its affiliates
//
//===----------------------------------------------------------------------===//
// RUN: %clang -O2 %s --target=aie2 -nostdlibinc -S -emit-llvm -o - | FileCheck %s

// CHECK-LABEL: @_Z19test_load_lut_int32PKvS0_Dv16_i(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[LUT1:%.*]] to i20
// CHECK-NEXT:    [[TMP1:%.*]] = zext i20 [[TMP0]] to i32
// CHECK-NEXT:    [[TMP2:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vbroadcast32.I512(i32 [[TMP1]])
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[LUT2:%.*]] to i20
// CHECK-NEXT:    [[TMP4:%.*]] = zext i20 [[TMP3]] to i32
// CHECK-NEXT:    [[TMP5:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vbroadcast32.I512(i32 [[TMP4]])
// CHECK-NEXT:    [[TMP6:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vsel32(<16 x i32> [[TMP2]], <16 x i32> [[TMP5]], i32 52428)
// CHECK-NEXT:    [[ADD_I9:%.*]] = add <16 x i32> [[TMP6]], [[OFFSET:%.*]]
// CHECK-NEXT:    [[TMP7:%.*]] = tail call <8 x i32> @llvm.aie2.ext.I256.I512(<16 x i32> [[ADD_I9]], i32 0)
// CHECK-NEXT:    [[TMP8:%.*]] = tail call noundef <8 x i32> @llvm.aie2.load.4x64.lo(<8 x i32> [[TMP7]])
// CHECK-NEXT:    [[TMP9:%.*]] = tail call <16 x i32> @llvm.aie2.upd.I512.I256(<16 x i32> undef, <8 x i32> [[TMP8]], i32 0)
// CHECK-NEXT:    [[TMP10:%.*]] = tail call noundef <8 x i32> @llvm.aie2.load.4x64.hi(<8 x i32> [[TMP7]])
// CHECK-NEXT:    [[TMP11:%.*]] = tail call <16 x i32> @llvm.aie2.upd.I512.I256(<16 x i32> [[TMP9]], <8 x i32> [[TMP10]], i32 1)
// CHECK-NEXT:    ret <16 x i32> [[TMP11]]
//
v16int32 test_load_lut_int32(const void *lut1, const void *lut2,
                             v16int32 offset) {
  v16int32 res;
  load_lut_int32(lut1, lut2, offset, res);
  return res;
}

// CHECK-LABEL: @_Z22test_load_lut_2x_int32PKvS0_Dv16_iRS1_S2_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <16 x i32>, ptr [[V1:%.*]], align 32, !tbaa [[TBAA2:![0-9]+]]
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[LUT1:%.*]] to i20
// CHECK-NEXT:    [[TMP2:%.*]] = zext i20 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP3:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vbroadcast32.I512(i32 [[TMP2]])
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[LUT2:%.*]] to i20
// CHECK-NEXT:    [[TMP5:%.*]] = zext i20 [[TMP4]] to i32
// CHECK-NEXT:    [[TMP6:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vbroadcast32.I512(i32 [[TMP5]])
// CHECK-NEXT:    [[TMP7:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vsel32(<16 x i32> [[TMP3]], <16 x i32> [[TMP6]], i32 52428)
// CHECK-NEXT:    [[ADD_I9:%.*]] = add <16 x i32> [[TMP7]], [[OFFSET:%.*]]
// CHECK-NEXT:    [[TMP8:%.*]] = tail call <8 x i32> @llvm.aie2.ext.I256.I512(<16 x i32> [[ADD_I9]], i32 0)
// CHECK-NEXT:    [[TMP9:%.*]] = tail call noundef <8 x i32> @llvm.aie2.load.4x64.lo(<8 x i32> [[TMP8]])
// CHECK-NEXT:    [[TMP10:%.*]] = tail call <16 x i32> @llvm.aie2.upd.I512.I256(<16 x i32> [[TMP0]], <8 x i32> [[TMP9]], i32 0)
// CHECK-NEXT:    store <16 x i32> [[TMP10]], ptr [[V1]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP11:%.*]] = tail call noundef <8 x i32> @llvm.aie2.load.4x64.hi(<8 x i32> [[TMP8]])
// CHECK-NEXT:    [[TMP12:%.*]] = tail call <16 x i32> @llvm.aie2.upd.I512.I256(<16 x i32> [[TMP10]], <8 x i32> [[TMP11]], i32 1)
// CHECK-NEXT:    store <16 x i32> [[TMP12]], ptr [[V1]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP13:%.*]] = load <16 x i32>, ptr [[V2:%.*]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP14:%.*]] = tail call <8 x i32> @llvm.aie2.ext.I256.I512(<16 x i32> [[ADD_I9]], i32 1)
// CHECK-NEXT:    [[TMP15:%.*]] = tail call noundef <8 x i32> @llvm.aie2.load.4x64.lo(<8 x i32> [[TMP14]])
// CHECK-NEXT:    [[TMP16:%.*]] = tail call <16 x i32> @llvm.aie2.upd.I512.I256(<16 x i32> [[TMP13]], <8 x i32> [[TMP15]], i32 0)
// CHECK-NEXT:    store <16 x i32> [[TMP16]], ptr [[V2]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP17:%.*]] = tail call noundef <8 x i32> @llvm.aie2.load.4x64.hi(<8 x i32> [[TMP14]])
// CHECK-NEXT:    [[TMP18:%.*]] = tail call <16 x i32> @llvm.aie2.upd.I512.I256(<16 x i32> [[TMP16]], <8 x i32> [[TMP17]], i32 1)
// CHECK-NEXT:    store <16 x i32> [[TMP18]], ptr [[V2]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    ret void
//
void test_load_lut_2x_int32(const void *lut1, const void *lut2, v16int32 offset,
                            v16int32 &v1, v16int32 &v2) {
  return load_lut_2x_int32(lut1, lut2, offset, v1, v2);
}

// CHECK-LABEL: @_Z19test_load_lut_int16PKvS0_Dv16_i(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[LUT1:%.*]] to i20
// CHECK-NEXT:    [[TMP1:%.*]] = zext i20 [[TMP0]] to i32
// CHECK-NEXT:    [[TMP2:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vbroadcast32.I512(i32 [[TMP1]])
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[LUT2:%.*]] to i20
// CHECK-NEXT:    [[TMP4:%.*]] = zext i20 [[TMP3]] to i32
// CHECK-NEXT:    [[TMP5:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vbroadcast32.I512(i32 [[TMP4]])
// CHECK-NEXT:    [[TMP6:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vsel32(<16 x i32> [[TMP2]], <16 x i32> [[TMP5]], i32 52428)
// CHECK-NEXT:    [[ADD_I_I:%.*]] = add <16 x i32> [[TMP6]], [[OFFSET:%.*]]
// CHECK-NEXT:    [[TMP7:%.*]] = tail call <8 x i32> @llvm.aie2.ext.I256.I512(<16 x i32> [[ADD_I_I]], i32 0)
// CHECK-NEXT:    [[TMP8:%.*]] = tail call noundef <8 x i32> @llvm.aie2.load.4x32.lo(<8 x i32> [[TMP7]])
// CHECK-NEXT:    [[TMP9:%.*]] = tail call <16 x i32> @llvm.aie2.upd.I512.I256(<16 x i32> undef, <8 x i32> [[TMP8]], i32 0)
// CHECK-NEXT:    [[TMP10:%.*]] = tail call noundef <8 x i32> @llvm.aie2.load.4x32.hi(<8 x i32> [[TMP7]])
// CHECK-NEXT:    [[TMP11:%.*]] = tail call <16 x i32> @llvm.aie2.upd.I512.I256(<16 x i32> [[TMP9]], <8 x i32> [[TMP10]], i32 1)
// CHECK-NEXT:    [[RETVAL_0_I:%.*]] = bitcast <16 x i32> [[TMP11]] to <32 x i16>
// CHECK-NEXT:    ret <32 x i16> [[RETVAL_0_I]]
//
v32int16 test_load_lut_int16(const void *lut1, const void *lut2,
                             v16int32 offset) {
  v32int16 res;
  load_lut_int16(lut1, lut2, offset, res);
  return res;
}

// CHECK-LABEL: @_Z22test_load_lut_2x_int16PKvS0_Dv16_iRDv32_sS3_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <16 x i32>, ptr [[V1:%.*]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[LUT1:%.*]] to i20
// CHECK-NEXT:    [[TMP2:%.*]] = zext i20 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP3:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vbroadcast32.I512(i32 [[TMP2]])
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[LUT2:%.*]] to i20
// CHECK-NEXT:    [[TMP5:%.*]] = zext i20 [[TMP4]] to i32
// CHECK-NEXT:    [[TMP6:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vbroadcast32.I512(i32 [[TMP5]])
// CHECK-NEXT:    [[TMP7:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vsel32(<16 x i32> [[TMP3]], <16 x i32> [[TMP6]], i32 52428)
// CHECK-NEXT:    [[ADD_I_I:%.*]] = add <16 x i32> [[TMP7]], [[OFFSET:%.*]]
// CHECK-NEXT:    [[TMP8:%.*]] = tail call <8 x i32> @llvm.aie2.ext.I256.I512(<16 x i32> [[ADD_I_I]], i32 0)
// CHECK-NEXT:    [[TMP9:%.*]] = tail call noundef <8 x i32> @llvm.aie2.load.4x32.lo(<8 x i32> [[TMP8]])
// CHECK-NEXT:    [[TMP10:%.*]] = tail call <16 x i32> @llvm.aie2.upd.I512.I256(<16 x i32> [[TMP0]], <8 x i32> [[TMP9]], i32 0)
// CHECK-NEXT:    store <16 x i32> [[TMP10]], ptr [[V1]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP11:%.*]] = tail call noundef <8 x i32> @llvm.aie2.load.4x32.hi(<8 x i32> [[TMP8]])
// CHECK-NEXT:    [[TMP12:%.*]] = tail call <16 x i32> @llvm.aie2.upd.I512.I256(<16 x i32> [[TMP10]], <8 x i32> [[TMP11]], i32 1)
// CHECK-NEXT:    store <16 x i32> [[TMP12]], ptr [[V1]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP13:%.*]] = load <16 x i32>, ptr [[V2:%.*]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP14:%.*]] = tail call <8 x i32> @llvm.aie2.ext.I256.I512(<16 x i32> [[ADD_I_I]], i32 1)
// CHECK-NEXT:    [[TMP15:%.*]] = tail call noundef <8 x i32> @llvm.aie2.load.4x32.lo(<8 x i32> [[TMP14]])
// CHECK-NEXT:    [[TMP16:%.*]] = tail call <16 x i32> @llvm.aie2.upd.I512.I256(<16 x i32> [[TMP13]], <8 x i32> [[TMP15]], i32 0)
// CHECK-NEXT:    store <16 x i32> [[TMP16]], ptr [[V2]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP17:%.*]] = tail call noundef <8 x i32> @llvm.aie2.load.4x32.hi(<8 x i32> [[TMP14]])
// CHECK-NEXT:    [[TMP18:%.*]] = tail call <16 x i32> @llvm.aie2.upd.I512.I256(<16 x i32> [[TMP16]], <8 x i32> [[TMP17]], i32 1)
// CHECK-NEXT:    store <16 x i32> [[TMP18]], ptr [[V2]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    ret void
//
void test_load_lut_2x_int16(const void *lut1, const void *lut2, v16int32 offset,
                            v32int16 &v1, v32int16 &v2) {
  return load_lut_2x_int16(lut1, lut2, offset, v1, v2);
}

// CHECK-LABEL: @_Z18test_load_lut_int8PKvS0_Dv16_i(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[LUT1:%.*]] to i20
// CHECK-NEXT:    [[TMP1:%.*]] = zext i20 [[TMP0]] to i32
// CHECK-NEXT:    [[TMP2:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vbroadcast32.I512(i32 [[TMP1]])
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[LUT2:%.*]] to i20
// CHECK-NEXT:    [[TMP4:%.*]] = zext i20 [[TMP3]] to i32
// CHECK-NEXT:    [[TMP5:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vbroadcast32.I512(i32 [[TMP4]])
// CHECK-NEXT:    [[TMP6:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vsel32(<16 x i32> [[TMP2]], <16 x i32> [[TMP5]], i32 52428)
// CHECK-NEXT:    [[ADD_I_I:%.*]] = add <16 x i32> [[TMP6]], [[OFFSET:%.*]]
// CHECK-NEXT:    [[TMP7:%.*]] = tail call <8 x i32> @llvm.aie2.ext.I256.I512(<16 x i32> [[ADD_I_I]], i32 0)
// CHECK-NEXT:    [[TMP8:%.*]] = tail call noundef <8 x i32> @llvm.aie2.load.4x16.lo(<8 x i32> [[TMP7]])
// CHECK-NEXT:    [[TMP9:%.*]] = tail call <16 x i32> @llvm.aie2.upd.I512.I256(<16 x i32> undef, <8 x i32> [[TMP8]], i32 0)
// CHECK-NEXT:    [[TMP10:%.*]] = tail call noundef <8 x i32> @llvm.aie2.load.4x16.hi(<8 x i32> [[TMP7]])
// CHECK-NEXT:    [[TMP11:%.*]] = tail call <16 x i32> @llvm.aie2.upd.I512.I256(<16 x i32> [[TMP9]], <8 x i32> [[TMP10]], i32 1)
// CHECK-NEXT:    [[RETVAL_0_I:%.*]] = bitcast <16 x i32> [[TMP11]] to <64 x i8>
// CHECK-NEXT:    ret <64 x i8> [[RETVAL_0_I]]
//
v64int8 test_load_lut_int8(const void *lut1, const void *lut2,
                           v16int32 offset) {
  v64int8 res;
  load_lut_int8(lut1, lut2, offset, res);
  return res;
}

// CHECK-LABEL: @_Z21test_load_lut_2x_int8PKvS0_Dv16_iRDv64_aS3_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <16 x i32>, ptr [[V1:%.*]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[LUT1:%.*]] to i20
// CHECK-NEXT:    [[TMP2:%.*]] = zext i20 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP3:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vbroadcast32.I512(i32 [[TMP2]])
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[LUT2:%.*]] to i20
// CHECK-NEXT:    [[TMP5:%.*]] = zext i20 [[TMP4]] to i32
// CHECK-NEXT:    [[TMP6:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vbroadcast32.I512(i32 [[TMP5]])
// CHECK-NEXT:    [[TMP7:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vsel32(<16 x i32> [[TMP3]], <16 x i32> [[TMP6]], i32 52428)
// CHECK-NEXT:    [[ADD_I_I:%.*]] = add <16 x i32> [[TMP7]], [[OFFSET:%.*]]
// CHECK-NEXT:    [[TMP8:%.*]] = tail call <8 x i32> @llvm.aie2.ext.I256.I512(<16 x i32> [[ADD_I_I]], i32 0)
// CHECK-NEXT:    [[TMP9:%.*]] = tail call noundef <8 x i32> @llvm.aie2.load.4x16.lo(<8 x i32> [[TMP8]])
// CHECK-NEXT:    [[TMP10:%.*]] = tail call <16 x i32> @llvm.aie2.upd.I512.I256(<16 x i32> [[TMP0]], <8 x i32> [[TMP9]], i32 0)
// CHECK-NEXT:    store <16 x i32> [[TMP10]], ptr [[V1]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP11:%.*]] = tail call noundef <8 x i32> @llvm.aie2.load.4x16.hi(<8 x i32> [[TMP8]])
// CHECK-NEXT:    [[TMP12:%.*]] = tail call <16 x i32> @llvm.aie2.upd.I512.I256(<16 x i32> [[TMP10]], <8 x i32> [[TMP11]], i32 1)
// CHECK-NEXT:    store <16 x i32> [[TMP12]], ptr [[V1]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP13:%.*]] = load <16 x i32>, ptr [[V2:%.*]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP14:%.*]] = tail call <8 x i32> @llvm.aie2.ext.I256.I512(<16 x i32> [[ADD_I_I]], i32 1)
// CHECK-NEXT:    [[TMP15:%.*]] = tail call noundef <8 x i32> @llvm.aie2.load.4x16.lo(<8 x i32> [[TMP14]])
// CHECK-NEXT:    [[TMP16:%.*]] = tail call <16 x i32> @llvm.aie2.upd.I512.I256(<16 x i32> [[TMP13]], <8 x i32> [[TMP15]], i32 0)
// CHECK-NEXT:    store <16 x i32> [[TMP16]], ptr [[V2]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP17:%.*]] = tail call noundef <8 x i32> @llvm.aie2.load.4x16.hi(<8 x i32> [[TMP14]])
// CHECK-NEXT:    [[TMP18:%.*]] = tail call <16 x i32> @llvm.aie2.upd.I512.I256(<16 x i32> [[TMP16]], <8 x i32> [[TMP17]], i32 1)
// CHECK-NEXT:    store <16 x i32> [[TMP18]], ptr [[V2]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    ret void
//
void test_load_lut_2x_int8(const void *lut1, const void *lut2, v16int32 offset,
                           v64int8 &v1, v64int8 &v2) {
  return load_lut_2x_int8(lut1, lut2, offset, v1, v2);
}

// CHECK-LABEL: @_Z19test_load_lut_int32PKvS0_Dv16_j(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[LUT1:%.*]] to i20
// CHECK-NEXT:    [[TMP1:%.*]] = zext i20 [[TMP0]] to i32
// CHECK-NEXT:    [[TMP2:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vbroadcast32.I512(i32 [[TMP1]])
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[LUT2:%.*]] to i20
// CHECK-NEXT:    [[TMP4:%.*]] = zext i20 [[TMP3]] to i32
// CHECK-NEXT:    [[TMP5:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vbroadcast32.I512(i32 [[TMP4]])
// CHECK-NEXT:    [[TMP6:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vsel32(<16 x i32> [[TMP2]], <16 x i32> [[TMP5]], i32 52428)
// CHECK-NEXT:    [[ADD_I7:%.*]] = add <16 x i32> [[TMP6]], [[OFFSET:%.*]]
// CHECK-NEXT:    [[TMP7:%.*]] = tail call <8 x i32> @llvm.aie2.ext.I256.I512(<16 x i32> [[ADD_I7]], i32 0)
// CHECK-NEXT:    [[TMP8:%.*]] = tail call noundef <8 x i32> @llvm.aie2.load.4x64.lo(<8 x i32> [[TMP7]])
// CHECK-NEXT:    [[TMP9:%.*]] = tail call <16 x i32> @llvm.aie2.upd.I512.I256(<16 x i32> undef, <8 x i32> [[TMP8]], i32 0)
// CHECK-NEXT:    [[TMP10:%.*]] = tail call noundef <8 x i32> @llvm.aie2.load.4x64.hi(<8 x i32> [[TMP7]])
// CHECK-NEXT:    [[TMP11:%.*]] = tail call <16 x i32> @llvm.aie2.upd.I512.I256(<16 x i32> [[TMP9]], <8 x i32> [[TMP10]], i32 1)
// CHECK-NEXT:    ret <16 x i32> [[TMP11]]
//
v16int32 test_load_lut_int32(const void *lut1, const void *lut2,
                             v16uint32 offset) {
  v16int32 res;
  load_lut_int32(lut1, lut2, offset, res);
  return res;
}

// CHECK-LABEL: @_Z22test_load_lut_2x_int32PKvS0_Dv16_jRDv16_iS3_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <16 x i32>, ptr [[V1:%.*]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[LUT1:%.*]] to i20
// CHECK-NEXT:    [[TMP2:%.*]] = zext i20 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP3:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vbroadcast32.I512(i32 [[TMP2]])
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[LUT2:%.*]] to i20
// CHECK-NEXT:    [[TMP5:%.*]] = zext i20 [[TMP4]] to i32
// CHECK-NEXT:    [[TMP6:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vbroadcast32.I512(i32 [[TMP5]])
// CHECK-NEXT:    [[TMP7:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vsel32(<16 x i32> [[TMP3]], <16 x i32> [[TMP6]], i32 52428)
// CHECK-NEXT:    [[ADD_I7:%.*]] = add <16 x i32> [[TMP7]], [[OFFSET:%.*]]
// CHECK-NEXT:    [[TMP8:%.*]] = tail call <8 x i32> @llvm.aie2.ext.I256.I512(<16 x i32> [[ADD_I7]], i32 0)
// CHECK-NEXT:    [[TMP9:%.*]] = tail call noundef <8 x i32> @llvm.aie2.load.4x64.lo(<8 x i32> [[TMP8]])
// CHECK-NEXT:    [[TMP10:%.*]] = tail call <16 x i32> @llvm.aie2.upd.I512.I256(<16 x i32> [[TMP0]], <8 x i32> [[TMP9]], i32 0)
// CHECK-NEXT:    store <16 x i32> [[TMP10]], ptr [[V1]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP11:%.*]] = tail call noundef <8 x i32> @llvm.aie2.load.4x64.hi(<8 x i32> [[TMP8]])
// CHECK-NEXT:    [[TMP12:%.*]] = tail call <16 x i32> @llvm.aie2.upd.I512.I256(<16 x i32> [[TMP10]], <8 x i32> [[TMP11]], i32 1)
// CHECK-NEXT:    store <16 x i32> [[TMP12]], ptr [[V1]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP13:%.*]] = load <16 x i32>, ptr [[V2:%.*]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP14:%.*]] = tail call <8 x i32> @llvm.aie2.ext.I256.I512(<16 x i32> [[ADD_I7]], i32 1)
// CHECK-NEXT:    [[TMP15:%.*]] = tail call noundef <8 x i32> @llvm.aie2.load.4x64.lo(<8 x i32> [[TMP14]])
// CHECK-NEXT:    [[TMP16:%.*]] = tail call <16 x i32> @llvm.aie2.upd.I512.I256(<16 x i32> [[TMP13]], <8 x i32> [[TMP15]], i32 0)
// CHECK-NEXT:    store <16 x i32> [[TMP16]], ptr [[V2]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP17:%.*]] = tail call noundef <8 x i32> @llvm.aie2.load.4x64.hi(<8 x i32> [[TMP14]])
// CHECK-NEXT:    [[TMP18:%.*]] = tail call <16 x i32> @llvm.aie2.upd.I512.I256(<16 x i32> [[TMP16]], <8 x i32> [[TMP17]], i32 1)
// CHECK-NEXT:    store <16 x i32> [[TMP18]], ptr [[V2]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    ret void
//
void test_load_lut_2x_int32(const void *lut1, const void *lut2,
                            v16uint32 offset, v16int32 &v1, v16int32 &v2) {
  return load_lut_2x_int32(lut1, lut2, offset, v1, v2);
}

// CHECK-LABEL: @_Z19test_load_lut_int16PKvS0_Dv16_j(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[LUT1:%.*]] to i20
// CHECK-NEXT:    [[TMP1:%.*]] = zext i20 [[TMP0]] to i32
// CHECK-NEXT:    [[TMP2:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vbroadcast32.I512(i32 [[TMP1]])
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[LUT2:%.*]] to i20
// CHECK-NEXT:    [[TMP4:%.*]] = zext i20 [[TMP3]] to i32
// CHECK-NEXT:    [[TMP5:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vbroadcast32.I512(i32 [[TMP4]])
// CHECK-NEXT:    [[TMP6:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vsel32(<16 x i32> [[TMP2]], <16 x i32> [[TMP5]], i32 52428)
// CHECK-NEXT:    [[ADD_I_I:%.*]] = add <16 x i32> [[TMP6]], [[OFFSET:%.*]]
// CHECK-NEXT:    [[TMP7:%.*]] = tail call <8 x i32> @llvm.aie2.ext.I256.I512(<16 x i32> [[ADD_I_I]], i32 0)
// CHECK-NEXT:    [[TMP8:%.*]] = tail call noundef <8 x i32> @llvm.aie2.load.4x32.lo(<8 x i32> [[TMP7]])
// CHECK-NEXT:    [[TMP9:%.*]] = tail call <16 x i32> @llvm.aie2.upd.I512.I256(<16 x i32> undef, <8 x i32> [[TMP8]], i32 0)
// CHECK-NEXT:    [[TMP10:%.*]] = tail call noundef <8 x i32> @llvm.aie2.load.4x32.hi(<8 x i32> [[TMP7]])
// CHECK-NEXT:    [[TMP11:%.*]] = tail call <16 x i32> @llvm.aie2.upd.I512.I256(<16 x i32> [[TMP9]], <8 x i32> [[TMP10]], i32 1)
// CHECK-NEXT:    [[RETVAL_0_I:%.*]] = bitcast <16 x i32> [[TMP11]] to <32 x i16>
// CHECK-NEXT:    ret <32 x i16> [[RETVAL_0_I]]
//
v32int16 test_load_lut_int16(const void *lut1, const void *lut2,
                             v16uint32 offset) {
  v32int16 res;
  load_lut_int16(lut1, lut2, offset, res);
  return res;
}

// CHECK-LABEL: @_Z22test_load_lut_2x_int16PKvS0_Dv16_jRDv32_sS3_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <16 x i32>, ptr [[V1:%.*]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[LUT1:%.*]] to i20
// CHECK-NEXT:    [[TMP2:%.*]] = zext i20 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP3:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vbroadcast32.I512(i32 [[TMP2]])
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[LUT2:%.*]] to i20
// CHECK-NEXT:    [[TMP5:%.*]] = zext i20 [[TMP4]] to i32
// CHECK-NEXT:    [[TMP6:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vbroadcast32.I512(i32 [[TMP5]])
// CHECK-NEXT:    [[TMP7:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vsel32(<16 x i32> [[TMP3]], <16 x i32> [[TMP6]], i32 52428)
// CHECK-NEXT:    [[ADD_I_I:%.*]] = add <16 x i32> [[TMP7]], [[OFFSET:%.*]]
// CHECK-NEXT:    [[TMP8:%.*]] = tail call <8 x i32> @llvm.aie2.ext.I256.I512(<16 x i32> [[ADD_I_I]], i32 0)
// CHECK-NEXT:    [[TMP9:%.*]] = tail call noundef <8 x i32> @llvm.aie2.load.4x32.lo(<8 x i32> [[TMP8]])
// CHECK-NEXT:    [[TMP10:%.*]] = tail call <16 x i32> @llvm.aie2.upd.I512.I256(<16 x i32> [[TMP0]], <8 x i32> [[TMP9]], i32 0)
// CHECK-NEXT:    store <16 x i32> [[TMP10]], ptr [[V1]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP11:%.*]] = tail call noundef <8 x i32> @llvm.aie2.load.4x32.hi(<8 x i32> [[TMP8]])
// CHECK-NEXT:    [[TMP12:%.*]] = tail call <16 x i32> @llvm.aie2.upd.I512.I256(<16 x i32> [[TMP10]], <8 x i32> [[TMP11]], i32 1)
// CHECK-NEXT:    store <16 x i32> [[TMP12]], ptr [[V1]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP13:%.*]] = load <16 x i32>, ptr [[V2:%.*]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP14:%.*]] = tail call <8 x i32> @llvm.aie2.ext.I256.I512(<16 x i32> [[ADD_I_I]], i32 1)
// CHECK-NEXT:    [[TMP15:%.*]] = tail call noundef <8 x i32> @llvm.aie2.load.4x32.lo(<8 x i32> [[TMP14]])
// CHECK-NEXT:    [[TMP16:%.*]] = tail call <16 x i32> @llvm.aie2.upd.I512.I256(<16 x i32> [[TMP13]], <8 x i32> [[TMP15]], i32 0)
// CHECK-NEXT:    store <16 x i32> [[TMP16]], ptr [[V2]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP17:%.*]] = tail call noundef <8 x i32> @llvm.aie2.load.4x32.hi(<8 x i32> [[TMP14]])
// CHECK-NEXT:    [[TMP18:%.*]] = tail call <16 x i32> @llvm.aie2.upd.I512.I256(<16 x i32> [[TMP16]], <8 x i32> [[TMP17]], i32 1)
// CHECK-NEXT:    store <16 x i32> [[TMP18]], ptr [[V2]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    ret void
//
void test_load_lut_2x_int16(const void *lut1, const void *lut2,
                            v16uint32 offset, v32int16 &v1, v32int16 &v2) {
  return load_lut_2x_int16(lut1, lut2, offset, v1, v2);
}

// CHECK-LABEL: @_Z18test_load_lut_int8PKvS0_Dv16_j(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[LUT1:%.*]] to i20
// CHECK-NEXT:    [[TMP1:%.*]] = zext i20 [[TMP0]] to i32
// CHECK-NEXT:    [[TMP2:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vbroadcast32.I512(i32 [[TMP1]])
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[LUT2:%.*]] to i20
// CHECK-NEXT:    [[TMP4:%.*]] = zext i20 [[TMP3]] to i32
// CHECK-NEXT:    [[TMP5:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vbroadcast32.I512(i32 [[TMP4]])
// CHECK-NEXT:    [[TMP6:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vsel32(<16 x i32> [[TMP2]], <16 x i32> [[TMP5]], i32 52428)
// CHECK-NEXT:    [[ADD_I_I:%.*]] = add <16 x i32> [[TMP6]], [[OFFSET:%.*]]
// CHECK-NEXT:    [[TMP7:%.*]] = tail call <8 x i32> @llvm.aie2.ext.I256.I512(<16 x i32> [[ADD_I_I]], i32 0)
// CHECK-NEXT:    [[TMP8:%.*]] = tail call noundef <8 x i32> @llvm.aie2.load.4x16.lo(<8 x i32> [[TMP7]])
// CHECK-NEXT:    [[TMP9:%.*]] = tail call <16 x i32> @llvm.aie2.upd.I512.I256(<16 x i32> undef, <8 x i32> [[TMP8]], i32 0)
// CHECK-NEXT:    [[TMP10:%.*]] = tail call noundef <8 x i32> @llvm.aie2.load.4x16.hi(<8 x i32> [[TMP7]])
// CHECK-NEXT:    [[TMP11:%.*]] = tail call <16 x i32> @llvm.aie2.upd.I512.I256(<16 x i32> [[TMP9]], <8 x i32> [[TMP10]], i32 1)
// CHECK-NEXT:    [[RETVAL_0_I:%.*]] = bitcast <16 x i32> [[TMP11]] to <64 x i8>
// CHECK-NEXT:    ret <64 x i8> [[RETVAL_0_I]]
//
v64int8 test_load_lut_int8(const void *lut1, const void *lut2,
                           v16uint32 offset) {
  v64int8 res;
  load_lut_int8(lut1, lut2, offset, res);
  return res;
}

// CHECK-LABEL: @_Z21test_load_lut_2x_int8PKvS0_Dv16_jRDv64_aS3_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <16 x i32>, ptr [[V1:%.*]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[LUT1:%.*]] to i20
// CHECK-NEXT:    [[TMP2:%.*]] = zext i20 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP3:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vbroadcast32.I512(i32 [[TMP2]])
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[LUT2:%.*]] to i20
// CHECK-NEXT:    [[TMP5:%.*]] = zext i20 [[TMP4]] to i32
// CHECK-NEXT:    [[TMP6:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vbroadcast32.I512(i32 [[TMP5]])
// CHECK-NEXT:    [[TMP7:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vsel32(<16 x i32> [[TMP3]], <16 x i32> [[TMP6]], i32 52428)
// CHECK-NEXT:    [[ADD_I_I:%.*]] = add <16 x i32> [[TMP7]], [[OFFSET:%.*]]
// CHECK-NEXT:    [[TMP8:%.*]] = tail call <8 x i32> @llvm.aie2.ext.I256.I512(<16 x i32> [[ADD_I_I]], i32 0)
// CHECK-NEXT:    [[TMP9:%.*]] = tail call noundef <8 x i32> @llvm.aie2.load.4x16.lo(<8 x i32> [[TMP8]])
// CHECK-NEXT:    [[TMP10:%.*]] = tail call <16 x i32> @llvm.aie2.upd.I512.I256(<16 x i32> [[TMP0]], <8 x i32> [[TMP9]], i32 0)
// CHECK-NEXT:    store <16 x i32> [[TMP10]], ptr [[V1]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP11:%.*]] = tail call noundef <8 x i32> @llvm.aie2.load.4x16.hi(<8 x i32> [[TMP8]])
// CHECK-NEXT:    [[TMP12:%.*]] = tail call <16 x i32> @llvm.aie2.upd.I512.I256(<16 x i32> [[TMP10]], <8 x i32> [[TMP11]], i32 1)
// CHECK-NEXT:    store <16 x i32> [[TMP12]], ptr [[V1]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP13:%.*]] = load <16 x i32>, ptr [[V2:%.*]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP14:%.*]] = tail call <8 x i32> @llvm.aie2.ext.I256.I512(<16 x i32> [[ADD_I_I]], i32 1)
// CHECK-NEXT:    [[TMP15:%.*]] = tail call noundef <8 x i32> @llvm.aie2.load.4x16.lo(<8 x i32> [[TMP14]])
// CHECK-NEXT:    [[TMP16:%.*]] = tail call <16 x i32> @llvm.aie2.upd.I512.I256(<16 x i32> [[TMP13]], <8 x i32> [[TMP15]], i32 0)
// CHECK-NEXT:    store <16 x i32> [[TMP16]], ptr [[V2]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP17:%.*]] = tail call noundef <8 x i32> @llvm.aie2.load.4x16.hi(<8 x i32> [[TMP14]])
// CHECK-NEXT:    [[TMP18:%.*]] = tail call <16 x i32> @llvm.aie2.upd.I512.I256(<16 x i32> [[TMP16]], <8 x i32> [[TMP17]], i32 1)
// CHECK-NEXT:    store <16 x i32> [[TMP18]], ptr [[V2]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    ret void
//
void test_load_lut_2x_int8(const void *lut1, const void *lut2, v16uint32 offset,
                           v64int8 &v1, v64int8 &v2) {
  return load_lut_2x_int8(lut1, lut2, offset, v1, v2);
}

// CHECK-LABEL: @_Z19test_load_lut_floatPKvS0_Dv16_i(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[LUT1:%.*]] to i20
// CHECK-NEXT:    [[TMP1:%.*]] = zext i20 [[TMP0]] to i32
// CHECK-NEXT:    [[TMP2:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vbroadcast32.I512(i32 [[TMP1]])
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[LUT2:%.*]] to i20
// CHECK-NEXT:    [[TMP4:%.*]] = zext i20 [[TMP3]] to i32
// CHECK-NEXT:    [[TMP5:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vbroadcast32.I512(i32 [[TMP4]])
// CHECK-NEXT:    [[TMP6:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vsel32(<16 x i32> [[TMP2]], <16 x i32> [[TMP5]], i32 52428)
// CHECK-NEXT:    [[ADD_I10:%.*]] = add <16 x i32> [[TMP6]], [[OFFSET:%.*]]
// CHECK-NEXT:    [[TMP7:%.*]] = tail call <8 x i32> @llvm.aie2.ext.I256.I512(<16 x i32> [[ADD_I10]], i32 0)
// CHECK-NEXT:    [[TMP8:%.*]] = tail call noundef <8 x i32> @llvm.aie2.load.4x64.lo(<8 x i32> [[TMP7]])
// CHECK-NEXT:    [[TMP9:%.*]] = bitcast <8 x i32> [[TMP8]] to <16 x bfloat>
// CHECK-NEXT:    [[TMP10:%.*]] = tail call <32 x bfloat> @llvm.aie2.upd.bf512.bf256(<32 x bfloat> undef, <16 x bfloat> [[TMP9]], i32 0)
// CHECK-NEXT:    [[TMP11:%.*]] = tail call noundef <8 x i32> @llvm.aie2.load.4x64.hi(<8 x i32> [[TMP7]])
// CHECK-NEXT:    [[TMP12:%.*]] = bitcast <8 x i32> [[TMP11]] to <16 x bfloat>
// CHECK-NEXT:    [[TMP13:%.*]] = tail call <32 x bfloat> @llvm.aie2.upd.bf512.bf256(<32 x bfloat> [[TMP10]], <16 x bfloat> [[TMP12]], i32 1)
// CHECK-NEXT:    ret <32 x bfloat> [[TMP13]]
//
v32bfloat16 test_load_lut_float(const void *lut1, const void *lut2,
                                v16int32 offset) {
  v32bfloat16 res;
  load_lut_float(lut1, lut2, offset, res);
  return res;
}

// CHECK-LABEL: @_Z22test_load_lut_2x_floatPKvS0_Dv16_iRDv32_u6__bf16S3_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x bfloat>, ptr [[V1:%.*]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[LUT1:%.*]] to i20
// CHECK-NEXT:    [[TMP2:%.*]] = zext i20 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP3:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vbroadcast32.I512(i32 [[TMP2]])
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[LUT2:%.*]] to i20
// CHECK-NEXT:    [[TMP5:%.*]] = zext i20 [[TMP4]] to i32
// CHECK-NEXT:    [[TMP6:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vbroadcast32.I512(i32 [[TMP5]])
// CHECK-NEXT:    [[TMP7:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vsel32(<16 x i32> [[TMP3]], <16 x i32> [[TMP6]], i32 52428)
// CHECK-NEXT:    [[ADD_I10:%.*]] = add <16 x i32> [[TMP7]], [[OFFSET:%.*]]
// CHECK-NEXT:    [[TMP8:%.*]] = tail call <8 x i32> @llvm.aie2.ext.I256.I512(<16 x i32> [[ADD_I10]], i32 0)
// CHECK-NEXT:    [[TMP9:%.*]] = tail call noundef <8 x i32> @llvm.aie2.load.4x64.lo(<8 x i32> [[TMP8]])
// CHECK-NEXT:    [[TMP10:%.*]] = bitcast <8 x i32> [[TMP9]] to <16 x bfloat>
// CHECK-NEXT:    [[TMP11:%.*]] = tail call <32 x bfloat> @llvm.aie2.upd.bf512.bf256(<32 x bfloat> [[TMP0]], <16 x bfloat> [[TMP10]], i32 0)
// CHECK-NEXT:    store <32 x bfloat> [[TMP11]], ptr [[V1]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP12:%.*]] = tail call noundef <8 x i32> @llvm.aie2.load.4x64.hi(<8 x i32> [[TMP8]])
// CHECK-NEXT:    [[TMP13:%.*]] = bitcast <8 x i32> [[TMP12]] to <16 x bfloat>
// CHECK-NEXT:    [[TMP14:%.*]] = tail call <32 x bfloat> @llvm.aie2.upd.bf512.bf256(<32 x bfloat> [[TMP11]], <16 x bfloat> [[TMP13]], i32 1)
// CHECK-NEXT:    store <32 x bfloat> [[TMP14]], ptr [[V1]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP15:%.*]] = load <32 x bfloat>, ptr [[V2:%.*]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP16:%.*]] = tail call <8 x i32> @llvm.aie2.ext.I256.I512(<16 x i32> [[ADD_I10]], i32 1)
// CHECK-NEXT:    [[TMP17:%.*]] = tail call noundef <8 x i32> @llvm.aie2.load.4x64.lo(<8 x i32> [[TMP16]])
// CHECK-NEXT:    [[TMP18:%.*]] = bitcast <8 x i32> [[TMP17]] to <16 x bfloat>
// CHECK-NEXT:    [[TMP19:%.*]] = tail call <32 x bfloat> @llvm.aie2.upd.bf512.bf256(<32 x bfloat> [[TMP15]], <16 x bfloat> [[TMP18]], i32 0)
// CHECK-NEXT:    store <32 x bfloat> [[TMP19]], ptr [[V2]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP20:%.*]] = tail call noundef <8 x i32> @llvm.aie2.load.4x64.hi(<8 x i32> [[TMP16]])
// CHECK-NEXT:    [[TMP21:%.*]] = bitcast <8 x i32> [[TMP20]] to <16 x bfloat>
// CHECK-NEXT:    [[TMP22:%.*]] = tail call <32 x bfloat> @llvm.aie2.upd.bf512.bf256(<32 x bfloat> [[TMP19]], <16 x bfloat> [[TMP21]], i32 1)
// CHECK-NEXT:    store <32 x bfloat> [[TMP22]], ptr [[V2]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    ret void
//
void test_load_lut_2x_float(const void *lut1, const void *lut2, v16int32 offset,
                            v32bfloat16 &v1, v32bfloat16 &v2) {
  return load_lut_2x_float(lut1, lut2, offset, v1, v2);
}

// CHECK-LABEL: @_Z19test_load_lut_floatPKvS0_Dv16_j(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = ptrtoint ptr [[LUT1:%.*]] to i20
// CHECK-NEXT:    [[TMP1:%.*]] = zext i20 [[TMP0]] to i32
// CHECK-NEXT:    [[TMP2:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vbroadcast32.I512(i32 [[TMP1]])
// CHECK-NEXT:    [[TMP3:%.*]] = ptrtoint ptr [[LUT2:%.*]] to i20
// CHECK-NEXT:    [[TMP4:%.*]] = zext i20 [[TMP3]] to i32
// CHECK-NEXT:    [[TMP5:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vbroadcast32.I512(i32 [[TMP4]])
// CHECK-NEXT:    [[TMP6:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vsel32(<16 x i32> [[TMP2]], <16 x i32> [[TMP5]], i32 52428)
// CHECK-NEXT:    [[ADD_I8:%.*]] = add <16 x i32> [[TMP6]], [[OFFSET:%.*]]
// CHECK-NEXT:    [[TMP7:%.*]] = tail call <8 x i32> @llvm.aie2.ext.I256.I512(<16 x i32> [[ADD_I8]], i32 0)
// CHECK-NEXT:    [[TMP8:%.*]] = tail call noundef <8 x i32> @llvm.aie2.load.4x64.lo(<8 x i32> [[TMP7]])
// CHECK-NEXT:    [[TMP9:%.*]] = bitcast <8 x i32> [[TMP8]] to <16 x bfloat>
// CHECK-NEXT:    [[TMP10:%.*]] = tail call <32 x bfloat> @llvm.aie2.upd.bf512.bf256(<32 x bfloat> undef, <16 x bfloat> [[TMP9]], i32 0)
// CHECK-NEXT:    [[TMP11:%.*]] = tail call noundef <8 x i32> @llvm.aie2.load.4x64.hi(<8 x i32> [[TMP7]])
// CHECK-NEXT:    [[TMP12:%.*]] = bitcast <8 x i32> [[TMP11]] to <16 x bfloat>
// CHECK-NEXT:    [[TMP13:%.*]] = tail call <32 x bfloat> @llvm.aie2.upd.bf512.bf256(<32 x bfloat> [[TMP10]], <16 x bfloat> [[TMP12]], i32 1)
// CHECK-NEXT:    ret <32 x bfloat> [[TMP13]]
//
v32bfloat16 test_load_lut_float(const void *lut1, const void *lut2,
                                v16uint32 offset) {
  v32bfloat16 res;
  load_lut_float(lut1, lut2, offset, res);
  return res;
}

// CHECK-LABEL: @_Z22test_load_lut_2x_floatPKvS0_Dv16_jRDv32_u6__bf16S3_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = load <32 x bfloat>, ptr [[V1:%.*]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP1:%.*]] = ptrtoint ptr [[LUT1:%.*]] to i20
// CHECK-NEXT:    [[TMP2:%.*]] = zext i20 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP3:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vbroadcast32.I512(i32 [[TMP2]])
// CHECK-NEXT:    [[TMP4:%.*]] = ptrtoint ptr [[LUT2:%.*]] to i20
// CHECK-NEXT:    [[TMP5:%.*]] = zext i20 [[TMP4]] to i32
// CHECK-NEXT:    [[TMP6:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vbroadcast32.I512(i32 [[TMP5]])
// CHECK-NEXT:    [[TMP7:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vsel32(<16 x i32> [[TMP3]], <16 x i32> [[TMP6]], i32 52428)
// CHECK-NEXT:    [[ADD_I8:%.*]] = add <16 x i32> [[TMP7]], [[OFFSET:%.*]]
// CHECK-NEXT:    [[TMP8:%.*]] = tail call <8 x i32> @llvm.aie2.ext.I256.I512(<16 x i32> [[ADD_I8]], i32 0)
// CHECK-NEXT:    [[TMP9:%.*]] = tail call noundef <8 x i32> @llvm.aie2.load.4x64.lo(<8 x i32> [[TMP8]])
// CHECK-NEXT:    [[TMP10:%.*]] = bitcast <8 x i32> [[TMP9]] to <16 x bfloat>
// CHECK-NEXT:    [[TMP11:%.*]] = tail call <32 x bfloat> @llvm.aie2.upd.bf512.bf256(<32 x bfloat> [[TMP0]], <16 x bfloat> [[TMP10]], i32 0)
// CHECK-NEXT:    store <32 x bfloat> [[TMP11]], ptr [[V1]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP12:%.*]] = tail call noundef <8 x i32> @llvm.aie2.load.4x64.hi(<8 x i32> [[TMP8]])
// CHECK-NEXT:    [[TMP13:%.*]] = bitcast <8 x i32> [[TMP12]] to <16 x bfloat>
// CHECK-NEXT:    [[TMP14:%.*]] = tail call <32 x bfloat> @llvm.aie2.upd.bf512.bf256(<32 x bfloat> [[TMP11]], <16 x bfloat> [[TMP13]], i32 1)
// CHECK-NEXT:    store <32 x bfloat> [[TMP14]], ptr [[V1]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP15:%.*]] = load <32 x bfloat>, ptr [[V2:%.*]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP16:%.*]] = tail call <8 x i32> @llvm.aie2.ext.I256.I512(<16 x i32> [[ADD_I8]], i32 1)
// CHECK-NEXT:    [[TMP17:%.*]] = tail call noundef <8 x i32> @llvm.aie2.load.4x64.lo(<8 x i32> [[TMP16]])
// CHECK-NEXT:    [[TMP18:%.*]] = bitcast <8 x i32> [[TMP17]] to <16 x bfloat>
// CHECK-NEXT:    [[TMP19:%.*]] = tail call <32 x bfloat> @llvm.aie2.upd.bf512.bf256(<32 x bfloat> [[TMP15]], <16 x bfloat> [[TMP18]], i32 0)
// CHECK-NEXT:    store <32 x bfloat> [[TMP19]], ptr [[V2]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    [[TMP20:%.*]] = tail call noundef <8 x i32> @llvm.aie2.load.4x64.hi(<8 x i32> [[TMP16]])
// CHECK-NEXT:    [[TMP21:%.*]] = bitcast <8 x i32> [[TMP20]] to <16 x bfloat>
// CHECK-NEXT:    [[TMP22:%.*]] = tail call <32 x bfloat> @llvm.aie2.upd.bf512.bf256(<32 x bfloat> [[TMP19]], <16 x bfloat> [[TMP21]], i32 1)
// CHECK-NEXT:    store <32 x bfloat> [[TMP22]], ptr [[V2]], align 32, !tbaa [[TBAA2]]
// CHECK-NEXT:    ret void
//
void test_load_lut_2x_float(const void *lut1, const void *lut2,
                            v16uint32 offset, v32bfloat16 &v1,
                            v32bfloat16 &v2) {
  return load_lut_2x_float(lut1, lut2, offset, v1, v2);
}
