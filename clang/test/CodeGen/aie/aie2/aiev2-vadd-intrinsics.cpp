// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py
//===- aiev2-vadd-intrinsics.cpp --------------------------------*- C++ -*-===//
//
// This file is licensed under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
// (c) Copyright 2023-2024 Advanced Micro Devices, Inc. or its affiliates
//
//===----------------------------------------------------------------------===//
// RUN: %clang -O2 %s --target=aie2 -S -emit-llvm -o - | FileCheck %s

// CHECK-LABEL: @_Z17test_add_v64uint8Dv64_hS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ADD_I:%.*]] = add <64 x i8> [[B:%.*]], [[A:%.*]]
// CHECK-NEXT:    ret <64 x i8> [[ADD_I]]
//
v64uint8 test_add_v64uint8(v64uint8 a, v64uint8 b) { return add(a, b); }

// CHECK-LABEL: @_Z17test_sub_v64uint8Dv64_hS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[SUB_I:%.*]] = sub <64 x i8> [[A:%.*]], [[B:%.*]]
// CHECK-NEXT:    ret <64 x i8> [[SUB_I]]
//
v64uint8 test_sub_v64uint8(v64uint8 a, v64uint8 b) { return sub(a, b); }

// CHECK-LABEL: @_Z20test_addsub_v64uint8Dv64_hS_y(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast i64 [[AS:%.*]] to <2 x i32>
// CHECK-NEXT:    [[TMP1:%.*]] = tail call noundef <64 x i8> @llvm.aie2.vaddsub8(<64 x i8> [[A:%.*]], <64 x i8> [[B:%.*]], <2 x i32> [[TMP0]])
// CHECK-NEXT:    ret <64 x i8> [[TMP1]]
//
v64uint8 test_addsub_v64uint8(v64uint8 a, v64uint8 b, unsigned long long as) {
  return addsub(a, b, as);
}

// CHECK-LABEL: @_Z21test_neg_gtz_v64uint8Dv64_hRy(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <64 x i8>, <2 x i32> } @llvm.aie2.vneg.gtz8(<64 x i8> [[A:%.*]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 1
// CHECK-NEXT:    store <2 x i32> [[TMP1]], ptr [[CMP:%.*]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 0
// CHECK-NEXT:    ret <64 x i8> [[TMP2]]
//
v64uint8 test_neg_gtz_v64uint8(v64uint8 a, unsigned long long &cmp) {
  return neg_gtz(a, cmp);
}

// CHECK-LABEL: @_Z17test_neg_v64uint8Dv64_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <64 x i8>, <2 x i32> } @llvm.aie2.vneg.gtz8(<64 x i8> [[A:%.*]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 0
// CHECK-NEXT:    ret <64 x i8> [[TMP1]]
//
v64uint8 test_neg_v64uint8(v64uint8 a) { return neg(a); }

// CHECK-LABEL: @_Z20test_sub_lt_v64uint8Dv64_hS_Ry(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <64 x i8>, <2 x i32> } @llvm.aie2.vsub.lt8(<64 x i8> [[A:%.*]], <64 x i8> [[B:%.*]], i32 0)
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 1
// CHECK-NEXT:    store <2 x i32> [[TMP1]], ptr [[CMP:%.*]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 0
// CHECK-NEXT:    ret <64 x i8> [[TMP2]]
//
v64uint8 test_sub_lt_v64uint8(v64uint8 a, v64uint8 b, unsigned long long &cmp) {
  return sub_lt(a, b, cmp);
}

// CHECK-LABEL: @_Z20test_sub_lt_v64uint8Dv64_hS_bRy(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <64 x i8>, <2 x i32> } @llvm.aie2.vsub.lt8(<64 x i8> [[A:%.*]], <64 x i8> [[B:%.*]], i32 [[CONV_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 1
// CHECK-NEXT:    store <2 x i32> [[TMP1]], ptr [[CMP:%.*]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 0
// CHECK-NEXT:    ret <64 x i8> [[TMP2]]
//
v64uint8 test_sub_lt_v64uint8(v64uint8 a, v64uint8 b, bool sgn,
                              unsigned long long &cmp) {
  return sub_lt(a, b, sgn, cmp);
}

// CHECK-LABEL: @_Z20test_sub_ge_v64uint8Dv64_hS_Ry(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <64 x i8>, <2 x i32> } @llvm.aie2.vsub.ge8(<64 x i8> [[A:%.*]], <64 x i8> [[B:%.*]], i32 0)
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 1
// CHECK-NEXT:    store <2 x i32> [[TMP1]], ptr [[CMP:%.*]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 0
// CHECK-NEXT:    ret <64 x i8> [[TMP2]]
//
v64uint8 test_sub_ge_v64uint8(v64uint8 a, v64uint8 b, unsigned long long &cmp) {
  return sub_ge(a, b, cmp);
}

// CHECK-LABEL: @_Z20test_sub_ge_v64uint8Dv64_hS_bRy(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <64 x i8>, <2 x i32> } @llvm.aie2.vsub.ge8(<64 x i8> [[A:%.*]], <64 x i8> [[B:%.*]], i32 [[CONV_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 1
// CHECK-NEXT:    store <2 x i32> [[TMP1]], ptr [[CMP:%.*]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 0
// CHECK-NEXT:    ret <64 x i8> [[TMP2]]
//
v64uint8 test_sub_ge_v64uint8(v64uint8 a, v64uint8 b, bool sgn,
                              unsigned long long &cmp) {
  return sub_ge(a, b, sgn, cmp);
}

// CHECK-LABEL: @_Z24test_maxdiff_lt_v64uint8Dv64_hS_Ry(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <64 x i8>, <2 x i32> } @llvm.aie2.vmaxdiff.lt8(<64 x i8> [[A:%.*]], <64 x i8> [[B:%.*]], i32 0)
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 1
// CHECK-NEXT:    store <2 x i32> [[TMP1]], ptr [[CMP:%.*]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 0
// CHECK-NEXT:    ret <64 x i8> [[TMP2]]
//
v64uint8 test_maxdiff_lt_v64uint8(v64uint8 a, v64uint8 b,
                                  unsigned long long &cmp) {
  return maxdiff_lt(a, b, cmp);
}

// CHECK-LABEL: @_Z24test_maxdiff_lt_v64uint8Dv64_hS_bRy(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <64 x i8>, <2 x i32> } @llvm.aie2.vmaxdiff.lt8(<64 x i8> [[A:%.*]], <64 x i8> [[B:%.*]], i32 [[CONV_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 1
// CHECK-NEXT:    store <2 x i32> [[TMP1]], ptr [[CMP:%.*]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 0
// CHECK-NEXT:    ret <64 x i8> [[TMP2]]
//
v64uint8 test_maxdiff_lt_v64uint8(v64uint8 a, v64uint8 b, bool sgn,
                                  unsigned long long &cmp) {
  return maxdiff_lt(a, b, sgn, cmp);
}

// CHECK-LABEL: @_Z21test_maxdiff_v64uint8Dv64_hS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <64 x i8>, <2 x i32> } @llvm.aie2.vmaxdiff.lt8(<64 x i8> [[A:%.*]], <64 x i8> [[B:%.*]], i32 0)
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 0
// CHECK-NEXT:    ret <64 x i8> [[TMP1]]
//
v64uint8 test_maxdiff_v64uint8(v64uint8 a, v64uint8 b) { return maxdiff(a, b); }

// CHECK-LABEL: @_Z21test_maxdiff_v64uint8Dv64_hS_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <64 x i8>, <2 x i32> } @llvm.aie2.vmaxdiff.lt8(<64 x i8> [[A:%.*]], <64 x i8> [[B:%.*]], i32 [[CONV_I_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 0
// CHECK-NEXT:    ret <64 x i8> [[TMP1]]
//
v64uint8 test_maxdiff_v64uint8(v64uint8 a, v64uint8 b, bool sgn) {
  return maxdiff(a, b, sgn);
}

// CHECK-LABEL: @_Z20test_min_ge_v64uint8Dv64_hS_Ry(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <64 x i8>, <2 x i32> } @llvm.aie2.vmin.ge8(<64 x i8> [[A:%.*]], <64 x i8> [[B:%.*]], i32 0)
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 1
// CHECK-NEXT:    store <2 x i32> [[TMP1]], ptr [[CMP:%.*]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 0
// CHECK-NEXT:    ret <64 x i8> [[TMP2]]
//
v64uint8 test_min_ge_v64uint8(v64uint8 a, v64uint8 b, unsigned long long &cmp) {
  return min_ge(a, b, cmp);
}

// CHECK-LABEL: @_Z20test_min_ge_v64uint8Dv64_hS_bRy(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <64 x i8>, <2 x i32> } @llvm.aie2.vmin.ge8(<64 x i8> [[A:%.*]], <64 x i8> [[B:%.*]], i32 [[CONV_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 1
// CHECK-NEXT:    store <2 x i32> [[TMP1]], ptr [[CMP:%.*]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 0
// CHECK-NEXT:    ret <64 x i8> [[TMP2]]
//
v64uint8 test_min_ge_v64uint8(v64uint8 a, v64uint8 b, bool sgn,
                              unsigned long long &cmp) {
  return min_ge(a, b, sgn, cmp);
}

// CHECK-LABEL: @_Z17test_min_v64uint8Dv64_hS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <64 x i8>, <2 x i32> } @llvm.aie2.vmin.ge8(<64 x i8> [[A:%.*]], <64 x i8> [[B:%.*]], i32 0)
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 0
// CHECK-NEXT:    ret <64 x i8> [[TMP1]]
//
v64uint8 test_min_v64uint8(v64uint8 a, v64uint8 b) { return min(a, b); }

// CHECK-LABEL: @_Z17test_min_v64uint8Dv64_hS_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <64 x i8>, <2 x i32> } @llvm.aie2.vmin.ge8(<64 x i8> [[A:%.*]], <64 x i8> [[B:%.*]], i32 [[CONV_I_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 0
// CHECK-NEXT:    ret <64 x i8> [[TMP1]]
//
v64uint8 test_min_v64uint8(v64uint8 a, v64uint8 b, bool sgn) {
  return min(a, b, sgn);
}

// CHECK-LABEL: @_Z20test_max_lt_v64uint8Dv64_hS_Ry(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <64 x i8>, <2 x i32> } @llvm.aie2.vmax.lt8(<64 x i8> [[A:%.*]], <64 x i8> [[B:%.*]], i32 0)
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 1
// CHECK-NEXT:    store <2 x i32> [[TMP1]], ptr [[CMP:%.*]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 0
// CHECK-NEXT:    ret <64 x i8> [[TMP2]]
//
v64uint8 test_max_lt_v64uint8(v64uint8 a, v64uint8 b, unsigned long long &cmp) {
  return max_lt(a, b, cmp);
}

// CHECK-LABEL: @_Z20test_max_lt_v64uint8Dv64_hS_bRy(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <64 x i8>, <2 x i32> } @llvm.aie2.vmax.lt8(<64 x i8> [[A:%.*]], <64 x i8> [[B:%.*]], i32 [[CONV_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 1
// CHECK-NEXT:    store <2 x i32> [[TMP1]], ptr [[CMP:%.*]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 0
// CHECK-NEXT:    ret <64 x i8> [[TMP2]]
//
v64uint8 test_max_lt_v64uint8(v64uint8 a, v64uint8 b, bool sgn,
                              unsigned long long &cmp) {
  return max_lt(a, b, sgn, cmp);
}

// CHECK-LABEL: @_Z17test_max_v64uint8Dv64_hS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <64 x i8>, <2 x i32> } @llvm.aie2.vmax.lt8(<64 x i8> [[A:%.*]], <64 x i8> [[B:%.*]], i32 0)
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 0
// CHECK-NEXT:    ret <64 x i8> [[TMP1]]
//
v64uint8 test_max_v64uint8(v64uint8 a, v64uint8 b) { return max(a, b); }

// CHECK-LABEL: @_Z17test_max_v64uint8Dv64_hS_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <64 x i8>, <2 x i32> } @llvm.aie2.vmax.lt8(<64 x i8> [[A:%.*]], <64 x i8> [[B:%.*]], i32 [[CONV_I_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 0
// CHECK-NEXT:    ret <64 x i8> [[TMP1]]
//
v64uint8 test_max_v64uint8(v64uint8 a, v64uint8 b, bool sgn) {
  return max(a, b, sgn);
}

// CHECK-LABEL: @_Z18test_band_v64uint8Dv64_hS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[AND_I:%.*]] = and <64 x i8> [[B:%.*]], [[A:%.*]]
// CHECK-NEXT:    ret <64 x i8> [[AND_I]]
//
v64uint8 test_band_v64uint8(v64uint8 a, v64uint8 b) { return band(a, b); }

// CHECK-LABEL: @_Z17test_bor_v64uint8Dv64_hS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[OR_I:%.*]] = or <64 x i8> [[B:%.*]], [[A:%.*]]
// CHECK-NEXT:    ret <64 x i8> [[OR_I]]
//
v64uint8 test_bor_v64uint8(v64uint8 a, v64uint8 b) { return bor(a, b); }

// CHECK-LABEL: @_Z22test_bneg_ltz_v64uint8Dv64_hRy(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <64 x i8>, <2 x i32> } @llvm.aie2.vbneg.ltz8(<64 x i8> [[A:%.*]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 0
// CHECK-NEXT:    store i64 -1, ptr [[CMP:%.*]], align 4, !tbaa [[TBAA2:![0-9]+]]
// CHECK-NEXT:    ret <64 x i8> [[TMP1]]
//
v64uint8 test_bneg_ltz_v64uint8(v64uint8 a, unsigned long long &cmp) {
  return bneg_ltz(a, cmp);
}

// CHECK-LABEL: @_Z22test_bneg_ltz_v64uint8Dv64_hbRy(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <64 x i8>, <2 x i32> } @llvm.aie2.vbneg.ltz8(<64 x i8> [[A:%.*]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 1
// CHECK-NEXT:    store <2 x i32> [[TMP1]], ptr [[CMP:%.*]], align 8
// CHECK-NEXT:    br i1 [[SGN:%.*]], label [[_ZL8BNEG_LTZDV64_HBRY_EXIT:%.*]], label [[IF_THEN_I:%.*]]
// CHECK:       if.then.i:
// CHECK-NEXT:    store i64 -1, ptr [[CMP]], align 4, !tbaa [[TBAA2]]
// CHECK-NEXT:    br label [[_ZL8BNEG_LTZDV64_HBRY_EXIT]]
// CHECK:       _ZL8bneg_ltzDv64_hbRy.exit:
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 0
// CHECK-NEXT:    ret <64 x i8> [[TMP2]]
//
v64uint8 test_bneg_ltz_v64uint8(v64uint8 a, bool sgn, unsigned long long &cmp) {
  return bneg_ltz(a, sgn, cmp);
}

// CHECK-LABEL: @_Z18test_bneg_v64uint8Dv64_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <64 x i8>, <2 x i32> } @llvm.aie2.vbneg.ltz8(<64 x i8> [[A:%.*]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 0
// CHECK-NEXT:    ret <64 x i8> [[TMP1]]
//
v64uint8 test_bneg_v64uint8(v64uint8 a) { return bneg(a); }

// CHECK-LABEL: @_Z18test_bxor_v64uint8Dv64_hS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <64 x i8>, <2 x i32> } @llvm.aie2.vbneg.ltz8(<64 x i8> [[B:%.*]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 0
// CHECK-NEXT:    [[AND_I7_I:%.*]] = and <64 x i8> [[TMP1]], [[A:%.*]]
// CHECK-NEXT:    [[TMP2:%.*]] = tail call { <64 x i8>, <2 x i32> } @llvm.aie2.vbneg.ltz8(<64 x i8> [[A]])
// CHECK-NEXT:    [[TMP3:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP2]], 0
// CHECK-NEXT:    [[AND_I_I:%.*]] = and <64 x i8> [[TMP3]], [[B]]
// CHECK-NEXT:    [[OR_I_I:%.*]] = or <64 x i8> [[AND_I_I]], [[AND_I7_I]]
// CHECK-NEXT:    ret <64 x i8> [[OR_I_I]]
//
v64uint8 test_bxor_v64uint8(v64uint8 a, v64uint8 b) { return bxor(a, b); }

// CHECK-LABEL: @_Z21test_abs_gtz_v64uint8Dv64_hRy(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <64 x i8>, <2 x i32> } @llvm.aie2.vabs.gtz8(<64 x i8> [[A:%.*]], i32 0)
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 1
// CHECK-NEXT:    store <2 x i32> [[TMP1]], ptr [[CMP:%.*]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 0
// CHECK-NEXT:    ret <64 x i8> [[TMP2]]
//
v64uint8 test_abs_gtz_v64uint8(v64uint8 a, unsigned long long &cmp) {
  return abs_gtz(a, cmp);
}

// CHECK-LABEL: @_Z21test_abs_gtz_v64uint8Dv64_hbRy(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <64 x i8>, <2 x i32> } @llvm.aie2.vabs.gtz8(<64 x i8> [[A:%.*]], i32 [[CONV_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 1
// CHECK-NEXT:    store <2 x i32> [[TMP1]], ptr [[CMP:%.*]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 0
// CHECK-NEXT:    ret <64 x i8> [[TMP2]]
//
v64uint8 test_abs_gtz_v64uint8(v64uint8 a, bool sgn, unsigned long long &cmp) {
  return abs_gtz(a, sgn, cmp);
}

// CHECK-LABEL: @_Z17test_abs_v64uint8Dv64_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    ret <64 x i8> [[A:%.*]]
//
v64uint8 test_abs_v64uint8(v64uint8 a) { return abs(a); }

//
// CHECK-LABEL: @_Z17test_abs_v64uint8Dv64_hb(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <64 x i8>, <2 x i32> } @llvm.aie2.vabs.gtz8(<64 x i8> [[A:%.*]], i32 [[CONV_I_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 0
// CHECK-NEXT:    ret <64 x i8> [[TMP1]]
//
v64uint8 test_abs_v64uint8(v64uint8 a, bool sgn) { return abs(a, sgn); }

// CHECK-LABEL: @_Z16test_lt_v64uint8Dv64_hS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <2 x i32> @llvm.aie2.vlt8(<64 x i8> [[A:%.*]], <64 x i8> [[B:%.*]], i32 0)
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <2 x i32> [[TMP0]] to i64
// CHECK-NEXT:    ret i64 [[TMP1]]
//
unsigned long long test_lt_v64uint8(v64uint8 a, v64uint8 b) { return lt(a, b); }

// CHECK-LABEL: @_Z16test_ge_v64uint8Dv64_hS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <2 x i32> @llvm.aie2.vge8(<64 x i8> [[A:%.*]], <64 x i8> [[B:%.*]], i32 0)
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <2 x i32> [[TMP0]] to i64
// CHECK-NEXT:    ret i64 [[TMP1]]
//
unsigned long long test_ge_v64uint8(v64uint8 a, v64uint8 b) { return ge(a, b); }

// CHECK-LABEL: @_Z16test_le_v64uint8Dv64_hS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <2 x i32> @llvm.aie2.vge8(<64 x i8> [[B:%.*]], <64 x i8> [[A:%.*]], i32 0)
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <2 x i32> [[TMP0]] to i64
// CHECK-NEXT:    ret i64 [[TMP1]]
//
unsigned long long test_le_v64uint8(v64uint8 a, v64uint8 b) { return le(a, b); }

// CHECK-LABEL: @_Z16test_gt_v64uint8Dv64_hS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <2 x i32> @llvm.aie2.vlt8(<64 x i8> [[B:%.*]], <64 x i8> [[A:%.*]], i32 0)
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <2 x i32> [[TMP0]] to i64
// CHECK-NEXT:    ret i64 [[TMP1]]
//
unsigned long long test_gt_v64uint8(v64uint8 a, v64uint8 b) { return gt(a, b); }

// CHECK-LABEL: @_Z16test_lt_v64uint8Dv64_hS_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <2 x i32> @llvm.aie2.vlt8(<64 x i8> [[A:%.*]], <64 x i8> [[B:%.*]], i32 [[CONV_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <2 x i32> [[TMP0]] to i64
// CHECK-NEXT:    ret i64 [[TMP1]]
//
unsigned long long test_lt_v64uint8(v64uint8 a, v64uint8 b, bool sgn) {
  return lt(a, b, sgn);
}

// CHECK-LABEL: @_Z16test_ge_v64uint8Dv64_hS_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <2 x i32> @llvm.aie2.vge8(<64 x i8> [[A:%.*]], <64 x i8> [[B:%.*]], i32 [[CONV_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <2 x i32> [[TMP0]] to i64
// CHECK-NEXT:    ret i64 [[TMP1]]
//
unsigned long long test_ge_v64uint8(v64uint8 a, v64uint8 b, bool sgn) {
  return ge(a, b, sgn);
}

// CHECK-LABEL: @_Z16test_le_v64uint8Dv64_hS_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <2 x i32> @llvm.aie2.vge8(<64 x i8> [[B:%.*]], <64 x i8> [[A:%.*]], i32 [[CONV_I_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <2 x i32> [[TMP0]] to i64
// CHECK-NEXT:    ret i64 [[TMP1]]
//
unsigned long long test_le_v64uint8(v64uint8 a, v64uint8 b, bool sgn) {
  return le(a, b, sgn);
}

// CHECK-LABEL: @_Z16test_gt_v64uint8Dv64_hS_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <2 x i32> @llvm.aie2.vlt8(<64 x i8> [[B:%.*]], <64 x i8> [[A:%.*]], i32 [[CONV_I_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <2 x i32> [[TMP0]] to i64
// CHECK-NEXT:    ret i64 [[TMP1]]
//
unsigned long long test_gt_v64uint8(v64uint8 a, v64uint8 b, bool sgn) {
  return gt(a, b, sgn);
}

// CHECK-LABEL: @_Z17test_ltz_v64uint8Dv64_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    ret i64 -1
//
unsigned long long test_ltz_v64uint8(v64uint8 a) { return ltz(a); }

// CHECK-LABEL: @_Z17test_ltz_v64uint8Dv64_hb(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <64 x i8>, <2 x i32> } @llvm.aie2.vbneg.ltz8(<64 x i8> [[A:%.*]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 1
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <2 x i32> [[TMP1]] to i64
// CHECK-NEXT:    [[SPEC_SELECT:%.*]] = select i1 [[SGN:%.*]], i64 [[TMP2]], i64 -1
// CHECK-NEXT:    ret i64 [[SPEC_SELECT]]
//
unsigned long long test_ltz_v64uint8(v64uint8 a, bool sgn) {
  return ltz(a, sgn);
}

// CHECK-LABEL: @_Z17test_gtz_v64uint8Dv64_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <64 x i8>, <2 x i32> } @llvm.aie2.vabs.gtz8(<64 x i8> [[A:%.*]], i32 0)
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 1
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <2 x i32> [[TMP1]] to i64
// CHECK-NEXT:    ret i64 [[TMP2]]
//
unsigned long long test_gtz_v64uint8(v64uint8 a) { return gtz(a); }

// CHECK-LABEL: @_Z17test_gtz_v64uint8Dv64_hb(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <64 x i8>, <2 x i32> } @llvm.aie2.vabs.gtz8(<64 x i8> [[A:%.*]], i32 [[CONV_I_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 1
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <2 x i32> [[TMP1]] to i64
// CHECK-NEXT:    ret i64 [[TMP2]]
//
unsigned long long test_gtz_v64uint8(v64uint8 a, bool sgn) {
  return gtz(a, sgn);
}

// CHECK-LABEL: @_Z17test_eqz_v64uint8Dv64_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <2 x i32> @llvm.aie2.veqz8(<64 x i8> [[A:%.*]])
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <2 x i32> [[TMP0]] to i64
// CHECK-NEXT:    ret i64 [[TMP1]]
//
unsigned long long test_eqz_v64uint8(v64uint8 a) { return eqz(a); }

// CHECK-LABEL: @_Z16test_eq_v64uint8Dv64_hS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[SUB_I_I:%.*]] = sub <64 x i8> [[A:%.*]], [[B:%.*]]
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <2 x i32> @llvm.aie2.veqz8(<64 x i8> [[SUB_I_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <2 x i32> [[TMP0]] to i64
// CHECK-NEXT:    ret i64 [[TMP1]]
//
unsigned long long test_eq_v64uint8(v64uint8 a, v64uint8 b) { return eq(a, b); }

// CHECK-LABEL: @_Z16test_ne_v64uint8Dv64_hS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[SUB_I_I_I:%.*]] = sub <64 x i8> [[A:%.*]], [[B:%.*]]
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <2 x i32> @llvm.aie2.veqz8(<64 x i8> [[SUB_I_I_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <2 x i32> [[TMP0]] to i64
// CHECK-NEXT:    [[NOT_I:%.*]] = xor i64 [[TMP1]], -1
// CHECK-NEXT:    ret i64 [[NOT_I]]
//
unsigned long long test_ne_v64uint8(v64uint8 a, v64uint8 b) { return ne(a, b); }

// CHECK-LABEL: @_Z17test_sel_v64uint8Dv64_hS_y(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast i64 [[S:%.*]] to <2 x i32>
// CHECK-NEXT:    [[TMP1:%.*]] = tail call noundef <64 x i8> @llvm.aie2.vsel8(<64 x i8> [[A:%.*]], <64 x i8> [[B:%.*]], <2 x i32> [[TMP0]])
// CHECK-NEXT:    ret <64 x i8> [[TMP1]]
//
v64uint8 test_sel_v64uint8(v64uint8 a, v64uint8 b, unsigned long long s) {
  return sel(a, b, s);
}

// CHECK-LABEL: @_Z16test_add_v64int8Dv64_aS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ADD_I:%.*]] = add <64 x i8> [[B:%.*]], [[A:%.*]]
// CHECK-NEXT:    ret <64 x i8> [[ADD_I]]
//
v64int8 test_add_v64int8(v64int8 a, v64int8 b) { return add(a, b); }

// CHECK-LABEL: @_Z16test_sub_v64int8Dv64_aS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[SUB_I:%.*]] = sub <64 x i8> [[A:%.*]], [[B:%.*]]
// CHECK-NEXT:    ret <64 x i8> [[SUB_I]]
//
v64int8 test_sub_v64int8(v64int8 a, v64int8 b) { return sub(a, b); }

// CHECK-LABEL: @_Z19test_addsub_v64int8Dv64_aS_y(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast i64 [[AS:%.*]] to <2 x i32>
// CHECK-NEXT:    [[TMP1:%.*]] = tail call noundef <64 x i8> @llvm.aie2.vaddsub8(<64 x i8> [[A:%.*]], <64 x i8> [[B:%.*]], <2 x i32> [[TMP0]])
// CHECK-NEXT:    ret <64 x i8> [[TMP1]]
//
v64int8 test_addsub_v64int8(v64int8 a, v64int8 b, unsigned long long as) {
  return addsub(a, b, as);
}

// CHECK-LABEL: @_Z20test_neg_gtz_v64int8Dv64_aRy(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <64 x i8>, <2 x i32> } @llvm.aie2.vneg.gtz8(<64 x i8> [[A:%.*]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 1
// CHECK-NEXT:    store <2 x i32> [[TMP1]], ptr [[CMP:%.*]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 0
// CHECK-NEXT:    ret <64 x i8> [[TMP2]]
//
v64int8 test_neg_gtz_v64int8(v64int8 a, unsigned long long &cmp) {
  return neg_gtz(a, cmp);
}

// CHECK-LABEL: @_Z16test_neg_v64int8Dv64_a(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <64 x i8>, <2 x i32> } @llvm.aie2.vneg.gtz8(<64 x i8> [[A:%.*]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 0
// CHECK-NEXT:    ret <64 x i8> [[TMP1]]
//
v64int8 test_neg_v64int8(v64int8 a) { return neg(a); }

// CHECK-LABEL: @_Z19test_sub_lt_v64int8Dv64_aS_Ry(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <64 x i8>, <2 x i32> } @llvm.aie2.vsub.lt8(<64 x i8> [[A:%.*]], <64 x i8> [[B:%.*]], i32 1)
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 1
// CHECK-NEXT:    store <2 x i32> [[TMP1]], ptr [[CMP:%.*]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 0
// CHECK-NEXT:    ret <64 x i8> [[TMP2]]
//
v64int8 test_sub_lt_v64int8(v64int8 a, v64int8 b, unsigned long long &cmp) {
  return sub_lt(a, b, cmp);
}

// CHECK-LABEL: @_Z19test_sub_lt_v64int8Dv64_aS_bRy(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <64 x i8>, <2 x i32> } @llvm.aie2.vsub.lt8(<64 x i8> [[A:%.*]], <64 x i8> [[B:%.*]], i32 [[CONV_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 1
// CHECK-NEXT:    store <2 x i32> [[TMP1]], ptr [[CMP:%.*]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 0
// CHECK-NEXT:    ret <64 x i8> [[TMP2]]
//
v64int8 test_sub_lt_v64int8(v64int8 a, v64int8 b, bool sgn,
                            unsigned long long &cmp) {
  return sub_lt(a, b, sgn, cmp);
}

// CHECK-LABEL: @_Z19test_sub_ge_v64int8Dv64_aS_Ry(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <64 x i8>, <2 x i32> } @llvm.aie2.vsub.ge8(<64 x i8> [[A:%.*]], <64 x i8> [[B:%.*]], i32 1)
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 1
// CHECK-NEXT:    store <2 x i32> [[TMP1]], ptr [[CMP:%.*]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 0
// CHECK-NEXT:    ret <64 x i8> [[TMP2]]
//
v64int8 test_sub_ge_v64int8(v64int8 a, v64int8 b, unsigned long long &cmp) {
  return sub_ge(a, b, cmp);
}

// CHECK-LABEL: @_Z19test_sub_ge_v64int8Dv64_aS_bRy(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <64 x i8>, <2 x i32> } @llvm.aie2.vsub.ge8(<64 x i8> [[A:%.*]], <64 x i8> [[B:%.*]], i32 [[CONV_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 1
// CHECK-NEXT:    store <2 x i32> [[TMP1]], ptr [[CMP:%.*]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 0
// CHECK-NEXT:    ret <64 x i8> [[TMP2]]
//
v64int8 test_sub_ge_v64int8(v64int8 a, v64int8 b, bool sgn,
                            unsigned long long &cmp) {
  return sub_ge(a, b, sgn, cmp);
}

// CHECK-LABEL: @_Z23test_maxdiff_lt_v64int8Dv64_aS_Ry(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <64 x i8>, <2 x i32> } @llvm.aie2.vmaxdiff.lt8(<64 x i8> [[A:%.*]], <64 x i8> [[B:%.*]], i32 1)
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 1
// CHECK-NEXT:    store <2 x i32> [[TMP1]], ptr [[CMP:%.*]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 0
// CHECK-NEXT:    ret <64 x i8> [[TMP2]]
//
v64int8 test_maxdiff_lt_v64int8(v64int8 a, v64int8 b, unsigned long long &cmp) {
  return maxdiff_lt(a, b, cmp);
}

// CHECK-LABEL: @_Z23test_maxdiff_lt_v64int8Dv64_aS_bRy(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <64 x i8>, <2 x i32> } @llvm.aie2.vmaxdiff.lt8(<64 x i8> [[A:%.*]], <64 x i8> [[B:%.*]], i32 [[CONV_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 1
// CHECK-NEXT:    store <2 x i32> [[TMP1]], ptr [[CMP:%.*]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 0
// CHECK-NEXT:    ret <64 x i8> [[TMP2]]
//
v64int8 test_maxdiff_lt_v64int8(v64int8 a, v64int8 b, bool sgn,
                                unsigned long long &cmp) {
  return maxdiff_lt(a, b, sgn, cmp);
}

// CHECK-LABEL: @_Z20test_maxdiff_v64int8Dv64_aS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <64 x i8>, <2 x i32> } @llvm.aie2.vmaxdiff.lt8(<64 x i8> [[A:%.*]], <64 x i8> [[B:%.*]], i32 1)
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 0
// CHECK-NEXT:    ret <64 x i8> [[TMP1]]
//
v64int8 test_maxdiff_v64int8(v64int8 a, v64int8 b) { return maxdiff(a, b); }

// CHECK-LABEL: @_Z20test_maxdiff_v64int8Dv64_aS_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <64 x i8>, <2 x i32> } @llvm.aie2.vmaxdiff.lt8(<64 x i8> [[A:%.*]], <64 x i8> [[B:%.*]], i32 [[CONV_I_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 0
// CHECK-NEXT:    ret <64 x i8> [[TMP1]]
//
v64int8 test_maxdiff_v64int8(v64int8 a, v64int8 b, bool sgn) {
  return maxdiff(a, b, sgn);
}

// CHECK-LABEL: @_Z19test_min_ge_v64int8Dv64_aS_Ry(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <64 x i8>, <2 x i32> } @llvm.aie2.vmin.ge8(<64 x i8> [[A:%.*]], <64 x i8> [[B:%.*]], i32 1)
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 1
// CHECK-NEXT:    store <2 x i32> [[TMP1]], ptr [[CMP:%.*]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 0
// CHECK-NEXT:    ret <64 x i8> [[TMP2]]
//
v64int8 test_min_ge_v64int8(v64int8 a, v64int8 b, unsigned long long &cmp) {
  return min_ge(a, b, cmp);
}

// CHECK-LABEL: @_Z19test_min_ge_v64int8Dv64_aS_bRy(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <64 x i8>, <2 x i32> } @llvm.aie2.vmin.ge8(<64 x i8> [[A:%.*]], <64 x i8> [[B:%.*]], i32 [[CONV_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 1
// CHECK-NEXT:    store <2 x i32> [[TMP1]], ptr [[CMP:%.*]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 0
// CHECK-NEXT:    ret <64 x i8> [[TMP2]]
//
v64int8 test_min_ge_v64int8(v64int8 a, v64int8 b, bool sgn,
                            unsigned long long &cmp) {
  return min_ge(a, b, sgn, cmp);
}

// CHECK-LABEL: @_Z16test_min_v64int8Dv64_aS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <64 x i8>, <2 x i32> } @llvm.aie2.vmin.ge8(<64 x i8> [[A:%.*]], <64 x i8> [[B:%.*]], i32 1)
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 0
// CHECK-NEXT:    ret <64 x i8> [[TMP1]]
//
v64int8 test_min_v64int8(v64int8 a, v64int8 b) { return min(a, b); }

// CHECK-LABEL: @_Z16test_min_v64int8Dv64_aS_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <64 x i8>, <2 x i32> } @llvm.aie2.vmin.ge8(<64 x i8> [[A:%.*]], <64 x i8> [[B:%.*]], i32 [[CONV_I_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 0
// CHECK-NEXT:    ret <64 x i8> [[TMP1]]
//
v64int8 test_min_v64int8(v64int8 a, v64int8 b, bool sgn) {
  return min(a, b, sgn);
}

// CHECK-LABEL: @_Z19test_max_lt_v64int8Dv64_aS_Ry(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <64 x i8>, <2 x i32> } @llvm.aie2.vmax.lt8(<64 x i8> [[A:%.*]], <64 x i8> [[B:%.*]], i32 1)
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 1
// CHECK-NEXT:    store <2 x i32> [[TMP1]], ptr [[CMP:%.*]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 0
// CHECK-NEXT:    ret <64 x i8> [[TMP2]]
//
v64int8 test_max_lt_v64int8(v64int8 a, v64int8 b, unsigned long long &cmp) {
  return max_lt(a, b, cmp);
}

// CHECK-LABEL: @_Z19test_max_lt_v64int8Dv64_aS_bRy(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <64 x i8>, <2 x i32> } @llvm.aie2.vmax.lt8(<64 x i8> [[A:%.*]], <64 x i8> [[B:%.*]], i32 [[CONV_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 1
// CHECK-NEXT:    store <2 x i32> [[TMP1]], ptr [[CMP:%.*]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 0
// CHECK-NEXT:    ret <64 x i8> [[TMP2]]
//
v64int8 test_max_lt_v64int8(v64int8 a, v64int8 b, bool sgn,
                            unsigned long long &cmp) {
  return max_lt(a, b, sgn, cmp);
}

// CHECK-LABEL: @_Z16test_max_v64int8Dv64_aS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <64 x i8>, <2 x i32> } @llvm.aie2.vmax.lt8(<64 x i8> [[A:%.*]], <64 x i8> [[B:%.*]], i32 1)
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 0
// CHECK-NEXT:    ret <64 x i8> [[TMP1]]
//
v64int8 test_max_v64int8(v64int8 a, v64int8 b) { return max(a, b); }

// CHECK-LABEL: @_Z16test_max_v64int8Dv64_aS_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <64 x i8>, <2 x i32> } @llvm.aie2.vmax.lt8(<64 x i8> [[A:%.*]], <64 x i8> [[B:%.*]], i32 [[CONV_I_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 0
// CHECK-NEXT:    ret <64 x i8> [[TMP1]]
//
v64int8 test_max_v64int8(v64int8 a, v64int8 b, bool sgn) {
  return max(a, b, sgn);
}

// CHECK-LABEL: @_Z17test_band_v64int8Dv64_aS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[AND_I:%.*]] = and <64 x i8> [[B:%.*]], [[A:%.*]]
// CHECK-NEXT:    ret <64 x i8> [[AND_I]]
//
v64int8 test_band_v64int8(v64int8 a, v64int8 b) { return band(a, b); }

// CHECK-LABEL: @_Z16test_bor_v64int8Dv64_aS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[OR_I:%.*]] = or <64 x i8> [[B:%.*]], [[A:%.*]]
// CHECK-NEXT:    ret <64 x i8> [[OR_I]]
//
v64int8 test_bor_v64int8(v64int8 a, v64int8 b) { return bor(a, b); }

//
// CHECK-LABEL: @_Z21test_bneg_ltz_v64int8Dv64_aRy(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <64 x i8>, <2 x i32> } @llvm.aie2.vbneg.ltz8(<64 x i8> [[A:%.*]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 1
// CHECK-NEXT:    store <2 x i32> [[TMP1]], ptr [[CMP:%.*]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 0
// CHECK-NEXT:    ret <64 x i8> [[TMP2]]
//
v64int8 test_bneg_ltz_v64int8(v64int8 a, unsigned long long &cmp) {
  return bneg_ltz(a, cmp);
}

// CHECK-LABEL: @_Z21test_bneg_ltz_v64int8Dv64_abRy(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <64 x i8>, <2 x i32> } @llvm.aie2.vbneg.ltz8(<64 x i8> [[A:%.*]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 1
// CHECK-NEXT:    store <2 x i32> [[TMP1]], ptr [[CMP:%.*]], align 8
// CHECK-NEXT:    br i1 [[SGN:%.*]], label [[_ZL8BNEG_LTZDV64_ABRY_EXIT:%.*]], label [[IF_THEN_I:%.*]]
// CHECK:       if.then.i:
// CHECK-NEXT:    store i64 -1, ptr [[CMP]], align 4, !tbaa [[TBAA2]]
// CHECK-NEXT:    br label [[_ZL8BNEG_LTZDV64_ABRY_EXIT]]
// CHECK:       _ZL8bneg_ltzDv64_abRy.exit:
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 0
// CHECK-NEXT:    ret <64 x i8> [[TMP2]]
//
v64int8 test_bneg_ltz_v64int8(v64int8 a, bool sgn, unsigned long long &cmp) {
  return bneg_ltz(a, sgn, cmp);
}

// CHECK-LABEL: @_Z17test_bneg_v64int8Dv64_a(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <64 x i8>, <2 x i32> } @llvm.aie2.vbneg.ltz8(<64 x i8> [[A:%.*]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 0
// CHECK-NEXT:    ret <64 x i8> [[TMP1]]
//
v64int8 test_bneg_v64int8(v64int8 a) { return bneg(a); }

// CHECK-LABEL: @_Z17test_bxor_v64int8Dv64_aS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <64 x i8>, <2 x i32> } @llvm.aie2.vbneg.ltz8(<64 x i8> [[B:%.*]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 0
// CHECK-NEXT:    [[AND_I7_I:%.*]] = and <64 x i8> [[TMP1]], [[A:%.*]]
// CHECK-NEXT:    [[TMP2:%.*]] = tail call { <64 x i8>, <2 x i32> } @llvm.aie2.vbneg.ltz8(<64 x i8> [[A]])
// CHECK-NEXT:    [[TMP3:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP2]], 0
// CHECK-NEXT:    [[AND_I_I:%.*]] = and <64 x i8> [[TMP3]], [[B]]
// CHECK-NEXT:    [[OR_I_I:%.*]] = or <64 x i8> [[AND_I_I]], [[AND_I7_I]]
// CHECK-NEXT:    ret <64 x i8> [[OR_I_I]]
//
v64int8 test_bxor_v64int8(v64int8 a, v64int8 b) { return bxor(a, b); }

// CHECK-LABEL: @_Z20test_abs_gtz_v64int8Dv64_aRy(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <64 x i8>, <2 x i32> } @llvm.aie2.vabs.gtz8(<64 x i8> [[A:%.*]], i32 1)
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 1
// CHECK-NEXT:    store <2 x i32> [[TMP1]], ptr [[CMP:%.*]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 0
// CHECK-NEXT:    ret <64 x i8> [[TMP2]]
//
v64int8 test_abs_gtz_v64int8(v64int8 a, unsigned long long &cmp) {
  return abs_gtz(a, cmp);
}

// CHECK-LABEL: @_Z20test_abs_gtz_v64int8Dv64_abRy(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <64 x i8>, <2 x i32> } @llvm.aie2.vabs.gtz8(<64 x i8> [[A:%.*]], i32 [[CONV_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 1
// CHECK-NEXT:    store <2 x i32> [[TMP1]], ptr [[CMP:%.*]], align 8
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 0
// CHECK-NEXT:    ret <64 x i8> [[TMP2]]
//
v64int8 test_abs_gtz_v64int8(v64int8 a, bool sgn, unsigned long long &cmp) {
  return abs_gtz(a, sgn, cmp);
}

// CHECK-LABEL: @_Z16test_abs_v64int8Dv64_a(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <64 x i8>, <2 x i32> } @llvm.aie2.vabs.gtz8(<64 x i8> [[A:%.*]], i32 1)
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 0
// CHECK-NEXT:    ret <64 x i8> [[TMP1]]
//
v64int8 test_abs_v64int8(v64int8 a) { return abs(a); }

// CHECK-LABEL: @_Z16test_abs_v64int8Dv64_ab(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <64 x i8>, <2 x i32> } @llvm.aie2.vabs.gtz8(<64 x i8> [[A:%.*]], i32 [[CONV_I_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 0
// CHECK-NEXT:    ret <64 x i8> [[TMP1]]
//
v64int8 test_abs_v64int8(v64int8 a, bool sgn) { return abs(a, sgn); }

// CHECK-LABEL: @_Z15test_lt_v64int8Dv64_aS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <2 x i32> @llvm.aie2.vlt8(<64 x i8> [[A:%.*]], <64 x i8> [[B:%.*]], i32 1)
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <2 x i32> [[TMP0]] to i64
// CHECK-NEXT:    ret i64 [[TMP1]]
//
unsigned long long test_lt_v64int8(v64int8 a, v64int8 b) { return lt(a, b); }

// CHECK-LABEL: @_Z15test_ge_v64int8Dv64_aS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <2 x i32> @llvm.aie2.vge8(<64 x i8> [[A:%.*]], <64 x i8> [[B:%.*]], i32 1)
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <2 x i32> [[TMP0]] to i64
// CHECK-NEXT:    ret i64 [[TMP1]]
//
unsigned long long test_ge_v64int8(v64int8 a, v64int8 b) { return ge(a, b); }

// CHECK-LABEL: @_Z15test_le_v64int8Dv64_aS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <2 x i32> @llvm.aie2.vge8(<64 x i8> [[B:%.*]], <64 x i8> [[A:%.*]], i32 1)
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <2 x i32> [[TMP0]] to i64
// CHECK-NEXT:    ret i64 [[TMP1]]
//
unsigned long long test_le_v64int8(v64int8 a, v64int8 b) { return le(a, b); }

// CHECK-LABEL: @_Z15test_gt_v64int8Dv64_aS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <2 x i32> @llvm.aie2.vlt8(<64 x i8> [[B:%.*]], <64 x i8> [[A:%.*]], i32 1)
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <2 x i32> [[TMP0]] to i64
// CHECK-NEXT:    ret i64 [[TMP1]]
//
unsigned long long test_gt_v64int8(v64int8 a, v64int8 b) { return gt(a, b); }

// CHECK-LABEL: @_Z15test_lt_v64int8Dv64_aS_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <2 x i32> @llvm.aie2.vlt8(<64 x i8> [[A:%.*]], <64 x i8> [[B:%.*]], i32 [[CONV_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <2 x i32> [[TMP0]] to i64
// CHECK-NEXT:    ret i64 [[TMP1]]
//
unsigned long long test_lt_v64int8(v64int8 a, v64int8 b, bool sgn) {
  return lt(a, b, sgn);
}

// CHECK-LABEL: @_Z15test_ge_v64int8Dv64_aS_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <2 x i32> @llvm.aie2.vge8(<64 x i8> [[A:%.*]], <64 x i8> [[B:%.*]], i32 [[CONV_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <2 x i32> [[TMP0]] to i64
// CHECK-NEXT:    ret i64 [[TMP1]]
//
unsigned long long test_ge_v64int8(v64int8 a, v64int8 b, bool sgn) {
  return ge(a, b, sgn);
}

// CHECK-LABEL: @_Z15test_le_v64int8Dv64_aS_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <2 x i32> @llvm.aie2.vge8(<64 x i8> [[B:%.*]], <64 x i8> [[A:%.*]], i32 [[CONV_I_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <2 x i32> [[TMP0]] to i64
// CHECK-NEXT:    ret i64 [[TMP1]]
//
unsigned long long test_le_v64int8(v64int8 a, v64int8 b, bool sgn) {
  return le(a, b, sgn);
}

// CHECK-LABEL: @_Z15test_gt_v64int8Dv64_aS_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <2 x i32> @llvm.aie2.vlt8(<64 x i8> [[B:%.*]], <64 x i8> [[A:%.*]], i32 [[CONV_I_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <2 x i32> [[TMP0]] to i64
// CHECK-NEXT:    ret i64 [[TMP1]]
//
unsigned long long test_gt_v64int8(v64int8 a, v64int8 b, bool sgn) {
  return gt(a, b, sgn);
}

// CHECK-LABEL: @_Z16test_ltz_v64int8Dv64_a(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <64 x i8>, <2 x i32> } @llvm.aie2.vbneg.ltz8(<64 x i8> [[A:%.*]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 1
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <2 x i32> [[TMP1]] to i64
// CHECK-NEXT:    ret i64 [[TMP2]]
//
unsigned long long test_ltz_v64int8(v64int8 a) { return ltz(a); }

// CHECK-LABEL: @_Z16test_ltz_v64int8Dv64_ab(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <64 x i8>, <2 x i32> } @llvm.aie2.vbneg.ltz8(<64 x i8> [[A:%.*]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 1
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <2 x i32> [[TMP1]] to i64
// CHECK-NEXT:    [[SPEC_SELECT:%.*]] = select i1 [[SGN:%.*]], i64 [[TMP2]], i64 -1
// CHECK-NEXT:    ret i64 [[SPEC_SELECT]]
//
unsigned long long test_ltz_v64int8(v64int8 a, bool sgn) { return ltz(a, sgn); }

// CHECK-LABEL: @_Z16test_gtz_v64int8Dv64_a(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <64 x i8>, <2 x i32> } @llvm.aie2.vabs.gtz8(<64 x i8> [[A:%.*]], i32 1)
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 1
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <2 x i32> [[TMP1]] to i64
// CHECK-NEXT:    ret i64 [[TMP2]]
//
unsigned long long test_gtz_v64int8(v64int8 a) { return gtz(a); }

// CHECK-LABEL: @_Z16test_gtz_v64int8Dv64_ab(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <64 x i8>, <2 x i32> } @llvm.aie2.vabs.gtz8(<64 x i8> [[A:%.*]], i32 [[CONV_I_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <64 x i8>, <2 x i32> } [[TMP0]], 1
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <2 x i32> [[TMP1]] to i64
// CHECK-NEXT:    ret i64 [[TMP2]]
//
unsigned long long test_gtz_v64int8(v64int8 a, bool sgn) { return gtz(a, sgn); }

// CHECK-LABEL: @_Z16test_eqz_v64int8Dv64_a(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <2 x i32> @llvm.aie2.veqz8(<64 x i8> [[A:%.*]])
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <2 x i32> [[TMP0]] to i64
// CHECK-NEXT:    ret i64 [[TMP1]]
//
unsigned long long test_eqz_v64int8(v64int8 a) { return eqz(a); }

// CHECK-LABEL: @_Z15test_eq_v64int8Dv64_aS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[SUB_I_I:%.*]] = sub <64 x i8> [[A:%.*]], [[B:%.*]]
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <2 x i32> @llvm.aie2.veqz8(<64 x i8> [[SUB_I_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <2 x i32> [[TMP0]] to i64
// CHECK-NEXT:    ret i64 [[TMP1]]
//
unsigned long long test_eq_v64int8(v64int8 a, v64int8 b) { return eq(a, b); }

// CHECK-LABEL: @_Z15test_ne_v64int8Dv64_aS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[SUB_I_I_I:%.*]] = sub <64 x i8> [[A:%.*]], [[B:%.*]]
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <2 x i32> @llvm.aie2.veqz8(<64 x i8> [[SUB_I_I_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <2 x i32> [[TMP0]] to i64
// CHECK-NEXT:    [[NOT_I:%.*]] = xor i64 [[TMP1]], -1
// CHECK-NEXT:    ret i64 [[NOT_I]]
//
unsigned long long test_ne_v64int8(v64int8 a, v64int8 b) { return ne(a, b); }

// CHECK-LABEL: @_Z16test_sel_v64int8Dv64_aS_y(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast i64 [[S:%.*]] to <2 x i32>
// CHECK-NEXT:    [[TMP1:%.*]] = tail call noundef <64 x i8> @llvm.aie2.vsel8(<64 x i8> [[A:%.*]], <64 x i8> [[B:%.*]], <2 x i32> [[TMP0]])
// CHECK-NEXT:    ret <64 x i8> [[TMP1]]
//
v64int8 test_sel_v64int8(v64int8 a, v64int8 b, unsigned long long s) {
  return sel(a, b, s);
}

// CHECK-LABEL: @_Z18test_add_v32uint16Dv32_tS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ADD_I:%.*]] = add <32 x i16> [[B:%.*]], [[A:%.*]]
// CHECK-NEXT:    ret <32 x i16> [[ADD_I]]
//
v32uint16 test_add_v32uint16(v32uint16 a, v32uint16 b) { return add(a, b); }

// CHECK-LABEL: @_Z18test_sub_v32uint16Dv32_tS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[SUB_I:%.*]] = sub <32 x i16> [[A:%.*]], [[B:%.*]]
// CHECK-NEXT:    ret <32 x i16> [[SUB_I]]
//
v32uint16 test_sub_v32uint16(v32uint16 a, v32uint16 b) { return sub(a, b); }

// CHECK-LABEL: @_Z21test_addsub_v32uint16Dv32_tS_j(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef <32 x i16> @llvm.aie2.vaddsub16(<32 x i16> [[A:%.*]], <32 x i16> [[B:%.*]], i32 [[AS:%.*]])
// CHECK-NEXT:    ret <32 x i16> [[TMP0]]
//
v32uint16 test_addsub_v32uint16(v32uint16 a, v32uint16 b, unsigned int as) {
  return addsub(a, b, as);
}

// CHECK-LABEL: @_Z22test_neg_gtz_v32uint16Dv32_tRj(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <32 x i16>, i32 } @llvm.aie2.vneg.gtz16(<32 x i16> [[A:%.*]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 1
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[CMP:%.*]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <32 x i16> [[TMP2]]
//
v32uint16 test_neg_gtz_v32uint16(v32uint16 a, unsigned int &cmp) {
  return neg_gtz(a, cmp);
}

// CHECK-LABEL: @_Z18test_neg_v32uint16Dv32_t(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <32 x i16>, i32 } @llvm.aie2.vneg.gtz16(<32 x i16> [[A:%.*]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <32 x i16> [[TMP1]]
//
v32uint16 test_neg_v32uint16(v32uint16 a) { return neg(a); }

// CHECK-LABEL: @_Z21test_sub_lt_v32uint16Dv32_tS_Rj(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <32 x i16>, i32 } @llvm.aie2.vsub.lt16(<32 x i16> [[A:%.*]], <32 x i16> [[B:%.*]], i32 0)
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 1
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[CMP:%.*]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <32 x i16> [[TMP2]]
//
v32uint16 test_sub_lt_v32uint16(v32uint16 a, v32uint16 b, unsigned int &cmp) {
  return sub_lt(a, b, cmp);
}

// CHECK-LABEL: @_Z21test_sub_lt_v32uint16Dv32_tS_bRj(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <32 x i16>, i32 } @llvm.aie2.vsub.lt16(<32 x i16> [[A:%.*]], <32 x i16> [[B:%.*]], i32 [[CONV_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 1
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[CMP:%.*]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <32 x i16> [[TMP2]]
//
v32uint16 test_sub_lt_v32uint16(v32uint16 a, v32uint16 b, bool sgn,
                                unsigned int &cmp) {
  return sub_lt(a, b, sgn, cmp);
}

// CHECK-LABEL: @_Z21test_sub_ge_v32uint16Dv32_tS_Rj(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <32 x i16>, i32 } @llvm.aie2.vsub.ge16(<32 x i16> [[A:%.*]], <32 x i16> [[B:%.*]], i32 0)
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 1
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[CMP:%.*]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <32 x i16> [[TMP2]]
//
v32uint16 test_sub_ge_v32uint16(v32uint16 a, v32uint16 b, unsigned int &cmp) {
  return sub_ge(a, b, cmp);
}

// CHECK-LABEL: @_Z21test_sub_ge_v32uint16Dv32_tS_bRj(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <32 x i16>, i32 } @llvm.aie2.vsub.ge16(<32 x i16> [[A:%.*]], <32 x i16> [[B:%.*]], i32 [[CONV_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 1
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[CMP:%.*]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <32 x i16> [[TMP2]]
//
v32uint16 test_sub_ge_v32uint16(v32uint16 a, v32uint16 b, bool sgn,
                                unsigned int &cmp) {
  return sub_ge(a, b, sgn, cmp);
}

// CHECK-LABEL: @_Z25test_maxdiff_lt_v32uint16Dv32_tS_Rj(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <32 x i16>, i32 } @llvm.aie2.vmaxdiff.lt16(<32 x i16> [[A:%.*]], <32 x i16> [[B:%.*]], i32 0)
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 1
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[CMP:%.*]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <32 x i16> [[TMP2]]
//
v32uint16 test_maxdiff_lt_v32uint16(v32uint16 a, v32uint16 b,
                                    unsigned int &cmp) {
  return maxdiff_lt(a, b, cmp);
}

// CHECK-LABEL: @_Z25test_maxdiff_lt_v32uint16Dv32_tS_bRj(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <32 x i16>, i32 } @llvm.aie2.vmaxdiff.lt16(<32 x i16> [[A:%.*]], <32 x i16> [[B:%.*]], i32 [[CONV_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 1
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[CMP:%.*]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <32 x i16> [[TMP2]]
//
v32uint16 test_maxdiff_lt_v32uint16(v32uint16 a, v32uint16 b, bool sgn,
                                    unsigned int &cmp) {
  return maxdiff_lt(a, b, sgn, cmp);
}

// CHECK-LABEL: @_Z22test_maxdiff_v32uint16Dv32_tS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <32 x i16>, i32 } @llvm.aie2.vmaxdiff.lt16(<32 x i16> [[A:%.*]], <32 x i16> [[B:%.*]], i32 0)
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <32 x i16> [[TMP1]]
//
v32uint16 test_maxdiff_v32uint16(v32uint16 a, v32uint16 b) {
  return maxdiff(a, b);
}

// CHECK-LABEL: @_Z22test_maxdiff_v32uint16Dv32_tS_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <32 x i16>, i32 } @llvm.aie2.vmaxdiff.lt16(<32 x i16> [[A:%.*]], <32 x i16> [[B:%.*]], i32 [[CONV_I_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <32 x i16> [[TMP1]]
//
v32uint16 test_maxdiff_v32uint16(v32uint16 a, v32uint16 b, bool sgn) {
  return maxdiff(a, b, sgn);
}

// CHECK-LABEL: @_Z21test_min_ge_v32uint16Dv32_tS_Rj(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <32 x i16>, i32 } @llvm.aie2.vmin.ge16(<32 x i16> [[A:%.*]], <32 x i16> [[B:%.*]], i32 0)
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 1
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[CMP:%.*]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <32 x i16> [[TMP2]]
//
v32uint16 test_min_ge_v32uint16(v32uint16 a, v32uint16 b, unsigned int &cmp) {
  return min_ge(a, b, cmp);
}

// CHECK-LABEL: @_Z21test_min_ge_v32uint16Dv32_tS_bRj(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <32 x i16>, i32 } @llvm.aie2.vmin.ge16(<32 x i16> [[A:%.*]], <32 x i16> [[B:%.*]], i32 [[CONV_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 1
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[CMP:%.*]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <32 x i16> [[TMP2]]
//
v32uint16 test_min_ge_v32uint16(v32uint16 a, v32uint16 b, bool sgn,
                                unsigned int &cmp) {
  return min_ge(a, b, sgn, cmp);
}

// CHECK-LABEL: @_Z18test_min_v32uint16Dv32_tS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <32 x i16>, i32 } @llvm.aie2.vmin.ge16(<32 x i16> [[A:%.*]], <32 x i16> [[B:%.*]], i32 0)
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <32 x i16> [[TMP1]]
//
v32uint16 test_min_v32uint16(v32uint16 a, v32uint16 b) { return min(a, b); }

// CHECK-LABEL: @_Z18test_min_v32uint16Dv32_tS_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <32 x i16>, i32 } @llvm.aie2.vmin.ge16(<32 x i16> [[A:%.*]], <32 x i16> [[B:%.*]], i32 [[CONV_I_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <32 x i16> [[TMP1]]
//
v32uint16 test_min_v32uint16(v32uint16 a, v32uint16 b, bool sgn) {
  return min(a, b, sgn);
}

// CHECK-LABEL: @_Z21test_max_lt_v32uint16Dv32_tS_Rj(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <32 x i16>, i32 } @llvm.aie2.vmax.lt16(<32 x i16> [[A:%.*]], <32 x i16> [[B:%.*]], i32 0)
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 1
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[CMP:%.*]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <32 x i16> [[TMP2]]
//
v32uint16 test_max_lt_v32uint16(v32uint16 a, v32uint16 b, unsigned int &cmp) {
  return max_lt(a, b, cmp);
}

// CHECK-LABEL: @_Z21test_max_lt_v32uint16Dv32_tS_bRj(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <32 x i16>, i32 } @llvm.aie2.vmax.lt16(<32 x i16> [[A:%.*]], <32 x i16> [[B:%.*]], i32 [[CONV_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 1
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[CMP:%.*]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <32 x i16> [[TMP2]]
//
v32uint16 test_max_lt_v32uint16(v32uint16 a, v32uint16 b, bool sgn,
                                unsigned int &cmp) {
  return max_lt(a, b, sgn, cmp);
}

// CHECK-LABEL: @_Z18test_max_v32uint16Dv32_tS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <32 x i16>, i32 } @llvm.aie2.vmax.lt16(<32 x i16> [[A:%.*]], <32 x i16> [[B:%.*]], i32 0)
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <32 x i16> [[TMP1]]
//
v32uint16 test_max_v32uint16(v32uint16 a, v32uint16 b) { return max(a, b); }

// CHECK-LABEL: @_Z18test_max_v32uint16Dv32_tS_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <32 x i16>, i32 } @llvm.aie2.vmax.lt16(<32 x i16> [[A:%.*]], <32 x i16> [[B:%.*]], i32 [[CONV_I_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <32 x i16> [[TMP1]]
//
v32uint16 test_max_v32uint16(v32uint16 a, v32uint16 b, bool sgn) {
  return max(a, b, sgn);
}

// CHECK-LABEL: @_Z19test_band_v32uint16Dv32_tS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[AND_I:%.*]] = and <32 x i16> [[B:%.*]], [[A:%.*]]
// CHECK-NEXT:    ret <32 x i16> [[AND_I]]
//
v32uint16 test_band_v32uint16(v32uint16 a, v32uint16 b) { return band(a, b); }

// CHECK-LABEL: @_Z18test_bor_v32uint16Dv32_tS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[OR_I:%.*]] = or <32 x i16> [[B:%.*]], [[A:%.*]]
// CHECK-NEXT:    ret <32 x i16> [[OR_I]]
//
v32uint16 test_bor_v32uint16(v32uint16 a, v32uint16 b) { return bor(a, b); }

// CHECK-LABEL: @_Z23test_bneg_ltz_v32uint16Dv32_tRj(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <32 x i16>, i32 } @llvm.aie2.vbneg.ltz16(<32 x i16> [[A:%.*]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 0
// CHECK-NEXT:    store i32 -1, ptr [[CMP:%.*]], align 4, !tbaa [[TBAA6:![0-9]+]]
// CHECK-NEXT:    ret <32 x i16> [[TMP1]]
//
v32uint16 test_bneg_ltz_v32uint16(v32uint16 a, unsigned int &cmp) {
  return bneg_ltz(a, cmp);
}

// CHECK-LABEL: @_Z23test_bneg_ltz_v32uint16Dv32_tbRj(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <32 x i16>, i32 } @llvm.aie2.vbneg.ltz16(<32 x i16> [[A:%.*]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 1
// CHECK-NEXT:    [[SPEC_SELECT_I:%.*]] = select i1 [[SGN:%.*]], i32 [[TMP1]], i32 -1
// CHECK-NEXT:    store i32 [[SPEC_SELECT_I]], ptr [[CMP:%.*]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <32 x i16> [[TMP2]]
//
v32uint16 test_bneg_ltz_v32uint16(v32uint16 a, bool sgn, unsigned int &cmp) {
  return bneg_ltz(a, sgn, cmp);
}

// CHECK-LABEL: @_Z19test_bneg_v32uint16Dv32_t(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <32 x i16>, i32 } @llvm.aie2.vbneg.ltz16(<32 x i16> [[A:%.*]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <32 x i16> [[TMP1]]
//
v32uint16 test_bneg_v32uint16(v32uint16 a) { return bneg(a); }

// CHECK-LABEL: @_Z19test_bxor_v32uint16Dv32_tS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <32 x i16>, i32 } @llvm.aie2.vbneg.ltz16(<32 x i16> [[B:%.*]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 0
// CHECK-NEXT:    [[AND_I7_I:%.*]] = and <32 x i16> [[TMP1]], [[A:%.*]]
// CHECK-NEXT:    [[TMP2:%.*]] = tail call { <32 x i16>, i32 } @llvm.aie2.vbneg.ltz16(<32 x i16> [[A]])
// CHECK-NEXT:    [[TMP3:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP2]], 0
// CHECK-NEXT:    [[AND_I_I:%.*]] = and <32 x i16> [[TMP3]], [[B]]
// CHECK-NEXT:    [[OR_I_I:%.*]] = or <32 x i16> [[AND_I_I]], [[AND_I7_I]]
// CHECK-NEXT:    ret <32 x i16> [[OR_I_I]]
//
v32uint16 test_bxor_v32uint16(v32uint16 a, v32uint16 b) { return bxor(a, b); }

// CHECK-LABEL: @_Z22test_abs_gtz_v32uint16Dv32_tRj(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <32 x i16>, i32 } @llvm.aie2.vabs.gtz16(<32 x i16> [[A:%.*]], i32 0)
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 1
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[CMP:%.*]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <32 x i16> [[TMP2]]
//
v32uint16 test_abs_gtz_v32uint16(v32uint16 a, unsigned int &cmp) {
  return abs_gtz(a, cmp);
}

// CHECK-LABEL: @_Z22test_abs_gtz_v32uint16Dv32_tbRj(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <32 x i16>, i32 } @llvm.aie2.vabs.gtz16(<32 x i16> [[A:%.*]], i32 [[CONV_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 1
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[CMP:%.*]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <32 x i16> [[TMP2]]
//
v32uint16 test_abs_gtz_v32uint16(v32uint16 a, bool sgn, unsigned int &cmp) {
  return abs_gtz(a, sgn, cmp);
}

// CHECK-LABEL: @_Z18test_abs_v32uint16Dv32_t(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    ret <32 x i16> [[A:%.*]]
//
v32uint16 test_abs_v32uint16(v32uint16 a) { return abs(a); }

// CHECK-LABEL: @_Z18test_abs_v32uint16Dv32_tb(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <32 x i16>, i32 } @llvm.aie2.vabs.gtz16(<32 x i16> [[A:%.*]], i32 [[CONV_I_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <32 x i16> [[TMP1]]
//
v32uint16 test_abs_v32uint16(v32uint16 a, bool sgn) { return abs(a, sgn); }

// CHECK-LABEL: @_Z17test_lt_v32uint16Dv32_tS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef i32 @llvm.aie2.vlt16(<32 x i16> [[A:%.*]], <32 x i16> [[B:%.*]], i32 0)
// CHECK-NEXT:    ret i32 [[TMP0]]
//
unsigned int test_lt_v32uint16(v32uint16 a, v32uint16 b) { return lt(a, b); }

// CHECK-LABEL: @_Z17test_ge_v32uint16Dv32_tS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef i32 @llvm.aie2.vge16(<32 x i16> [[A:%.*]], <32 x i16> [[B:%.*]], i32 0)
// CHECK-NEXT:    ret i32 [[TMP0]]
//
unsigned int test_ge_v32uint16(v32uint16 a, v32uint16 b) { return ge(a, b); }

// CHECK-LABEL: @_Z17test_le_v32uint16Dv32_tS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef i32 @llvm.aie2.vge16(<32 x i16> [[B:%.*]], <32 x i16> [[A:%.*]], i32 0)
// CHECK-NEXT:    ret i32 [[TMP0]]
//
unsigned int test_le_v32uint16(v32uint16 a, v32uint16 b) { return le(a, b); }

// CHECK-LABEL: @_Z17test_gt_v32uint16Dv32_tS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef i32 @llvm.aie2.vlt16(<32 x i16> [[B:%.*]], <32 x i16> [[A:%.*]], i32 0)
// CHECK-NEXT:    ret i32 [[TMP0]]
//
unsigned int test_gt_v32uint16(v32uint16 a, v32uint16 b) { return gt(a, b); }

// CHECK-LABEL: @_Z17test_lt_v32uint16Dv32_tS_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef i32 @llvm.aie2.vlt16(<32 x i16> [[A:%.*]], <32 x i16> [[B:%.*]], i32 [[CONV_I]])
// CHECK-NEXT:    ret i32 [[TMP0]]
//
unsigned int test_lt_v32uint16(v32uint16 a, v32uint16 b, bool sgn) {
  return lt(a, b, sgn);
}

// CHECK-LABEL: @_Z17test_ge_v32uint16Dv32_tS_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef i32 @llvm.aie2.vge16(<32 x i16> [[A:%.*]], <32 x i16> [[B:%.*]], i32 [[CONV_I]])
// CHECK-NEXT:    ret i32 [[TMP0]]
//
unsigned int test_ge_v32uint16(v32uint16 a, v32uint16 b, bool sgn) {
  return ge(a, b, sgn);
}

// CHECK-LABEL: @_Z17test_le_v32uint16Dv32_tS_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef i32 @llvm.aie2.vge16(<32 x i16> [[B:%.*]], <32 x i16> [[A:%.*]], i32 [[CONV_I_I]])
// CHECK-NEXT:    ret i32 [[TMP0]]
//
unsigned int test_le_v32uint16(v32uint16 a, v32uint16 b, bool sgn) {
  return le(a, b, sgn);
}

// CHECK-LABEL: @_Z17test_gt_v32uint16Dv32_tS_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef i32 @llvm.aie2.vlt16(<32 x i16> [[B:%.*]], <32 x i16> [[A:%.*]], i32 [[CONV_I_I]])
// CHECK-NEXT:    ret i32 [[TMP0]]
//
unsigned int test_gt_v32uint16(v32uint16 a, v32uint16 b, bool sgn) {
  return gt(a, b, sgn);
}

// CHECK-LABEL: @_Z18test_ltz_v32uint16Dv32_t(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    ret i32 -1
//
unsigned int test_ltz_v32uint16(v32uint16 a) { return ltz(a); }

// CHECK-LABEL: @_Z18test_ltz_v32uint16Dv32_tb(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <32 x i16>, i32 } @llvm.aie2.vbneg.ltz16(<32 x i16> [[A:%.*]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 1
// CHECK-NEXT:    [[SPEC_SELECT_I_I:%.*]] = select i1 [[SGN:%.*]], i32 [[TMP1]], i32 -1
// CHECK-NEXT:    ret i32 [[SPEC_SELECT_I_I]]
//
unsigned int test_ltz_v32uint16(v32uint16 a, bool sgn) { return ltz(a, sgn); }

// CHECK-LABEL: @_Z18test_gtz_v32uint16Dv32_t(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <32 x i16>, i32 } @llvm.aie2.vabs.gtz16(<32 x i16> [[A:%.*]], i32 0)
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 1
// CHECK-NEXT:    ret i32 [[TMP1]]
//
unsigned int test_gtz_v32uint16(v32uint16 a) { return gtz(a); }

// CHECK-LABEL: @_Z18test_gtz_v32uint16Dv32_tb(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <32 x i16>, i32 } @llvm.aie2.vabs.gtz16(<32 x i16> [[A:%.*]], i32 [[CONV_I_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 1
// CHECK-NEXT:    ret i32 [[TMP1]]
//
unsigned int test_gtz_v32uint16(v32uint16 a, bool sgn) { return gtz(a, sgn); }

// CHECK-LABEL: @_Z18test_eqz_v32uint16Dv32_t(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef i32 @llvm.aie2.veqz16(<32 x i16> [[A:%.*]])
// CHECK-NEXT:    ret i32 [[TMP0]]
//
unsigned int test_eqz_v32uint16(v32uint16 a) { return eqz(a); }

// CHECK-LABEL: @_Z17test_eq_v32uint16Dv32_tS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[SUB_I_I:%.*]] = sub <32 x i16> [[A:%.*]], [[B:%.*]]
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef i32 @llvm.aie2.veqz16(<32 x i16> [[SUB_I_I]])
// CHECK-NEXT:    ret i32 [[TMP0]]
//
unsigned int test_eq_v32uint16(v32uint16 a, v32uint16 b) { return eq(a, b); }

// CHECK-LABEL: @_Z17test_ne_v32uint16Dv32_tS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[SUB_I_I_I:%.*]] = sub <32 x i16> [[A:%.*]], [[B:%.*]]
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef i32 @llvm.aie2.veqz16(<32 x i16> [[SUB_I_I_I]])
// CHECK-NEXT:    [[NOT_I:%.*]] = xor i32 [[TMP0]], -1
// CHECK-NEXT:    ret i32 [[NOT_I]]
//
unsigned int test_ne_v32uint16(v32uint16 a, v32uint16 b) { return ne(a, b); }

// CHECK-LABEL: @_Z18test_sel_v32uint16Dv32_tS_j(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef <32 x i16> @llvm.aie2.vsel16(<32 x i16> [[A:%.*]], <32 x i16> [[B:%.*]], i32 [[S:%.*]])
// CHECK-NEXT:    ret <32 x i16> [[TMP0]]
//
v32uint16 test_sel_v32uint16(v32uint16 a, v32uint16 b, unsigned int s) {
  return sel(a, b, s);
}

// CHECK-LABEL: @_Z17test_add_v32int16Dv32_sS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ADD_I:%.*]] = add <32 x i16> [[B:%.*]], [[A:%.*]]
// CHECK-NEXT:    ret <32 x i16> [[ADD_I]]
//
v32int16 test_add_v32int16(v32int16 a, v32int16 b) { return add(a, b); }

// CHECK-LABEL: @_Z17test_sub_v32int16Dv32_sS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[SUB_I:%.*]] = sub <32 x i16> [[A:%.*]], [[B:%.*]]
// CHECK-NEXT:    ret <32 x i16> [[SUB_I]]
//
v32int16 test_sub_v32int16(v32int16 a, v32int16 b) { return sub(a, b); }

// CHECK-LABEL: @_Z20test_addsub_v32int16Dv32_sS_j(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef <32 x i16> @llvm.aie2.vaddsub16(<32 x i16> [[A:%.*]], <32 x i16> [[B:%.*]], i32 [[AS:%.*]])
// CHECK-NEXT:    ret <32 x i16> [[TMP0]]
//
v32int16 test_addsub_v32int16(v32int16 a, v32int16 b, unsigned int as) {
  return addsub(a, b, as);
}

// CHECK-LABEL: @_Z21test_neg_gtz_v32int16Dv32_sRj(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <32 x i16>, i32 } @llvm.aie2.vneg.gtz16(<32 x i16> [[A:%.*]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 1
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[CMP:%.*]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <32 x i16> [[TMP2]]
//
v32int16 test_neg_gtz_v32int16(v32int16 a, unsigned int &cmp) {
  return neg_gtz(a, cmp);
}

// CHECK-LABEL: @_Z17test_neg_v32int16Dv32_s(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <32 x i16>, i32 } @llvm.aie2.vneg.gtz16(<32 x i16> [[A:%.*]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <32 x i16> [[TMP1]]
//
v32int16 test_neg_v32int16(v32int16 a) { return neg(a); }

// CHECK-LABEL: @_Z20test_sub_lt_v32int16Dv32_sS_Rj(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <32 x i16>, i32 } @llvm.aie2.vsub.lt16(<32 x i16> [[A:%.*]], <32 x i16> [[B:%.*]], i32 1)
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 1
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[CMP:%.*]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <32 x i16> [[TMP2]]
//
v32int16 test_sub_lt_v32int16(v32int16 a, v32int16 b, unsigned int &cmp) {
  return sub_lt(a, b, cmp);
}

// CHECK-LABEL: @_Z20test_sub_lt_v32int16Dv32_sS_bRj(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <32 x i16>, i32 } @llvm.aie2.vsub.lt16(<32 x i16> [[A:%.*]], <32 x i16> [[B:%.*]], i32 [[CONV_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 1
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[CMP:%.*]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <32 x i16> [[TMP2]]
//
v32int16 test_sub_lt_v32int16(v32int16 a, v32int16 b, bool sgn,
                              unsigned int &cmp) {
  return sub_lt(a, b, sgn, cmp);
}

// CHECK-LABEL: @_Z20test_sub_ge_v32int16Dv32_sS_Rj(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <32 x i16>, i32 } @llvm.aie2.vsub.ge16(<32 x i16> [[A:%.*]], <32 x i16> [[B:%.*]], i32 1)
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 1
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[CMP:%.*]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <32 x i16> [[TMP2]]
//
v32int16 test_sub_ge_v32int16(v32int16 a, v32int16 b, unsigned int &cmp) {
  return sub_ge(a, b, cmp);
}

// CHECK-LABEL: @_Z20test_sub_ge_v32int16Dv32_sS_bRj(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <32 x i16>, i32 } @llvm.aie2.vsub.ge16(<32 x i16> [[A:%.*]], <32 x i16> [[B:%.*]], i32 [[CONV_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 1
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[CMP:%.*]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <32 x i16> [[TMP2]]
//
v32int16 test_sub_ge_v32int16(v32int16 a, v32int16 b, bool sgn,
                              unsigned int &cmp) {
  return sub_ge(a, b, sgn, cmp);
}

// CHECK-LABEL: @_Z24test_maxdiff_lt_v32int16Dv32_sS_Rj(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <32 x i16>, i32 } @llvm.aie2.vmaxdiff.lt16(<32 x i16> [[A:%.*]], <32 x i16> [[B:%.*]], i32 1)
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 1
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[CMP:%.*]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <32 x i16> [[TMP2]]
//
v32int16 test_maxdiff_lt_v32int16(v32int16 a, v32int16 b, unsigned int &cmp) {
  return maxdiff_lt(a, b, cmp);
}

// CHECK-LABEL: @_Z24test_maxdiff_lt_v32int16Dv32_sS_bRj(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <32 x i16>, i32 } @llvm.aie2.vmaxdiff.lt16(<32 x i16> [[A:%.*]], <32 x i16> [[B:%.*]], i32 [[CONV_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 1
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[CMP:%.*]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <32 x i16> [[TMP2]]
//
v32int16 test_maxdiff_lt_v32int16(v32int16 a, v32int16 b, bool sgn,
                                  unsigned int &cmp) {
  return maxdiff_lt(a, b, sgn, cmp);
}

// CHECK-LABEL: @_Z21test_maxdiff_v32int16Dv32_sS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <32 x i16>, i32 } @llvm.aie2.vmaxdiff.lt16(<32 x i16> [[A:%.*]], <32 x i16> [[B:%.*]], i32 1)
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <32 x i16> [[TMP1]]
//
v32int16 test_maxdiff_v32int16(v32int16 a, v32int16 b) { return maxdiff(a, b); }

// CHECK-LABEL: @_Z21test_maxdiff_v32int16Dv32_sS_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <32 x i16>, i32 } @llvm.aie2.vmaxdiff.lt16(<32 x i16> [[A:%.*]], <32 x i16> [[B:%.*]], i32 [[CONV_I_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <32 x i16> [[TMP1]]
//
v32int16 test_maxdiff_v32int16(v32int16 a, v32int16 b, bool sgn) {
  return maxdiff(a, b, sgn);
}

// CHECK-LABEL: @_Z20test_min_ge_v32int16Dv32_sS_Rj(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <32 x i16>, i32 } @llvm.aie2.vmin.ge16(<32 x i16> [[A:%.*]], <32 x i16> [[B:%.*]], i32 1)
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 1
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[CMP:%.*]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <32 x i16> [[TMP2]]
//
v32int16 test_min_ge_v32int16(v32int16 a, v32int16 b, unsigned int &cmp) {
  return min_ge(a, b, cmp);
}

// CHECK-LABEL: @_Z20test_min_ge_v32int16Dv32_sS_bRj(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <32 x i16>, i32 } @llvm.aie2.vmin.ge16(<32 x i16> [[A:%.*]], <32 x i16> [[B:%.*]], i32 [[CONV_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 1
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[CMP:%.*]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <32 x i16> [[TMP2]]
//
v32int16 test_min_ge_v32int16(v32int16 a, v32int16 b, bool sgn,
                              unsigned int &cmp) {
  return min_ge(a, b, sgn, cmp);
}

// CHECK-LABEL: @_Z17test_min_v32int16Dv32_sS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <32 x i16>, i32 } @llvm.aie2.vmin.ge16(<32 x i16> [[A:%.*]], <32 x i16> [[B:%.*]], i32 1)
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <32 x i16> [[TMP1]]
//
v32int16 test_min_v32int16(v32int16 a, v32int16 b) { return min(a, b); }

// CHECK-LABEL: @_Z17test_min_v32int16Dv32_sS_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <32 x i16>, i32 } @llvm.aie2.vmin.ge16(<32 x i16> [[A:%.*]], <32 x i16> [[B:%.*]], i32 [[CONV_I_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <32 x i16> [[TMP1]]
//
v32int16 test_min_v32int16(v32int16 a, v32int16 b, bool sgn) {
  return min(a, b, sgn);
}

// CHECK-LABEL: @_Z20test_max_lt_v32int16Dv32_sS_Rj(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <32 x i16>, i32 } @llvm.aie2.vmax.lt16(<32 x i16> [[A:%.*]], <32 x i16> [[B:%.*]], i32 1)
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 1
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[CMP:%.*]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <32 x i16> [[TMP2]]
//
v32int16 test_max_lt_v32int16(v32int16 a, v32int16 b, unsigned int &cmp) {
  return max_lt(a, b, cmp);
}

// CHECK-LABEL: @_Z20test_max_lt_v32int16Dv32_sS_bRj(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <32 x i16>, i32 } @llvm.aie2.vmax.lt16(<32 x i16> [[A:%.*]], <32 x i16> [[B:%.*]], i32 [[CONV_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 1
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[CMP:%.*]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <32 x i16> [[TMP2]]
//
v32int16 test_max_lt_v32int16(v32int16 a, v32int16 b, bool sgn,
                              unsigned int &cmp) {
  return max_lt(a, b, sgn, cmp);
}

// CHECK-LABEL: @_Z17test_max_v32int16Dv32_sS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <32 x i16>, i32 } @llvm.aie2.vmax.lt16(<32 x i16> [[A:%.*]], <32 x i16> [[B:%.*]], i32 1)
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <32 x i16> [[TMP1]]
//
v32int16 test_max_v32int16(v32int16 a, v32int16 b) { return max(a, b); }

// CHECK-LABEL: @_Z17test_max_v32int16Dv32_sS_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <32 x i16>, i32 } @llvm.aie2.vmax.lt16(<32 x i16> [[A:%.*]], <32 x i16> [[B:%.*]], i32 [[CONV_I_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <32 x i16> [[TMP1]]
//
v32int16 test_max_v32int16(v32int16 a, v32int16 b, bool sgn) {
  return max(a, b, sgn);
}

// CHECK-LABEL: @_Z18test_band_v32int16Dv32_sS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[AND_I:%.*]] = and <32 x i16> [[B:%.*]], [[A:%.*]]
// CHECK-NEXT:    ret <32 x i16> [[AND_I]]
//
v32int16 test_band_v32int16(v32int16 a, v32int16 b) { return band(a, b); }

// CHECK-LABEL: @_Z17test_bor_v32int16Dv32_sS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[OR_I:%.*]] = or <32 x i16> [[B:%.*]], [[A:%.*]]
// CHECK-NEXT:    ret <32 x i16> [[OR_I]]
//
v32int16 test_bor_v32int16(v32int16 a, v32int16 b) { return bor(a, b); }

// CHECK-LABEL: @_Z22test_bneg_ltz_v32int16Dv32_sRj(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <32 x i16>, i32 } @llvm.aie2.vbneg.ltz16(<32 x i16> [[A:%.*]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 1
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[CMP:%.*]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <32 x i16> [[TMP2]]
//
v32int16 test_bneg_ltz_v32int16(v32int16 a, unsigned int &cmp) {
  return bneg_ltz(a, cmp);
}

// CHECK-LABEL: @_Z22test_bneg_ltz_v32int16Dv32_sbRj(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <32 x i16>, i32 } @llvm.aie2.vbneg.ltz16(<32 x i16> [[A:%.*]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 1
// CHECK-NEXT:    [[SPEC_SELECT_I:%.*]] = select i1 [[SGN:%.*]], i32 [[TMP1]], i32 -1
// CHECK-NEXT:    store i32 [[SPEC_SELECT_I]], ptr [[CMP:%.*]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <32 x i16> [[TMP2]]
//
v32int16 test_bneg_ltz_v32int16(v32int16 a, bool sgn, unsigned int &cmp) {
  return bneg_ltz(a, sgn, cmp);
}

// CHECK-LABEL: @_Z18test_bneg_v32int16Dv32_s(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <32 x i16>, i32 } @llvm.aie2.vbneg.ltz16(<32 x i16> [[A:%.*]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <32 x i16> [[TMP1]]
//
v32int16 test_bneg_v32int16(v32int16 a) { return bneg(a); }

// CHECK-LABEL: @_Z18test_bxor_v32int16Dv32_sS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <32 x i16>, i32 } @llvm.aie2.vbneg.ltz16(<32 x i16> [[B:%.*]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 0
// CHECK-NEXT:    [[AND_I7_I:%.*]] = and <32 x i16> [[TMP1]], [[A:%.*]]
// CHECK-NEXT:    [[TMP2:%.*]] = tail call { <32 x i16>, i32 } @llvm.aie2.vbneg.ltz16(<32 x i16> [[A]])
// CHECK-NEXT:    [[TMP3:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP2]], 0
// CHECK-NEXT:    [[AND_I_I:%.*]] = and <32 x i16> [[TMP3]], [[B]]
// CHECK-NEXT:    [[OR_I_I:%.*]] = or <32 x i16> [[AND_I_I]], [[AND_I7_I]]
// CHECK-NEXT:    ret <32 x i16> [[OR_I_I]]
//
v32int16 test_bxor_v32int16(v32int16 a, v32int16 b) { return bxor(a, b); }

// CHECK-LABEL: @_Z21test_abs_gtz_v32int16Dv32_sRj(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <32 x i16>, i32 } @llvm.aie2.vabs.gtz16(<32 x i16> [[A:%.*]], i32 1)
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 1
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[CMP:%.*]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <32 x i16> [[TMP2]]
//
v32int16 test_abs_gtz_v32int16(v32int16 a, unsigned int &cmp) {
  return abs_gtz(a, cmp);
}

// CHECK-LABEL: @_Z21test_abs_gtz_v32int16Dv32_sbRj(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <32 x i16>, i32 } @llvm.aie2.vabs.gtz16(<32 x i16> [[A:%.*]], i32 [[CONV_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 1
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[CMP:%.*]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <32 x i16> [[TMP2]]
//
v32int16 test_abs_gtz_v32int16(v32int16 a, bool sgn, unsigned int &cmp) {
  return abs_gtz(a, sgn, cmp);
}

// CHECK-LABEL: @_Z17test_abs_v32int16Dv32_s(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <32 x i16>, i32 } @llvm.aie2.vabs.gtz16(<32 x i16> [[A:%.*]], i32 1)
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <32 x i16> [[TMP1]]
//
v32int16 test_abs_v32int16(v32int16 a) { return abs(a); }

// CHECK-LABEL: @_Z17test_abs_v32int16Dv32_sb(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <32 x i16>, i32 } @llvm.aie2.vabs.gtz16(<32 x i16> [[A:%.*]], i32 [[CONV_I_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <32 x i16> [[TMP1]]
//
v32int16 test_abs_v32int16(v32int16 a, bool sgn) { return abs(a, sgn); }

// CHECK-LABEL: @_Z16test_lt_v32int16Dv32_sS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef i32 @llvm.aie2.vlt16(<32 x i16> [[A:%.*]], <32 x i16> [[B:%.*]], i32 1)
// CHECK-NEXT:    ret i32 [[TMP0]]
//
unsigned int test_lt_v32int16(v32int16 a, v32int16 b) { return lt(a, b); }

// CHECK-LABEL: @_Z16test_ge_v32int16Dv32_sS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef i32 @llvm.aie2.vge16(<32 x i16> [[A:%.*]], <32 x i16> [[B:%.*]], i32 1)
// CHECK-NEXT:    ret i32 [[TMP0]]
//
unsigned int test_ge_v32int16(v32int16 a, v32int16 b) { return ge(a, b); }

// CHECK-LABEL: @_Z16test_le_v32int16Dv32_sS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef i32 @llvm.aie2.vge16(<32 x i16> [[B:%.*]], <32 x i16> [[A:%.*]], i32 1)
// CHECK-NEXT:    ret i32 [[TMP0]]
//
unsigned int test_le_v32int16(v32int16 a, v32int16 b) { return le(a, b); }

// CHECK-LABEL: @_Z16test_gt_v32int16Dv32_sS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef i32 @llvm.aie2.vlt16(<32 x i16> [[B:%.*]], <32 x i16> [[A:%.*]], i32 1)
// CHECK-NEXT:    ret i32 [[TMP0]]
//
unsigned int test_gt_v32int16(v32int16 a, v32int16 b) { return gt(a, b); }

// CHECK-LABEL: @_Z16test_lt_v32int16Dv32_sS_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef i32 @llvm.aie2.vlt16(<32 x i16> [[A:%.*]], <32 x i16> [[B:%.*]], i32 [[CONV_I]])
// CHECK-NEXT:    ret i32 [[TMP0]]
//
unsigned int test_lt_v32int16(v32int16 a, v32int16 b, bool sgn) {
  return lt(a, b, sgn);
}

// CHECK-LABEL: @_Z16test_ge_v32int16Dv32_sS_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef i32 @llvm.aie2.vge16(<32 x i16> [[A:%.*]], <32 x i16> [[B:%.*]], i32 [[CONV_I]])
// CHECK-NEXT:    ret i32 [[TMP0]]
//
unsigned int test_ge_v32int16(v32int16 a, v32int16 b, bool sgn) {
  return ge(a, b, sgn);
}

// CHECK-LABEL: @_Z16test_le_v32int16Dv32_sS_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef i32 @llvm.aie2.vge16(<32 x i16> [[B:%.*]], <32 x i16> [[A:%.*]], i32 [[CONV_I_I]])
// CHECK-NEXT:    ret i32 [[TMP0]]
//
unsigned int test_le_v32int16(v32int16 a, v32int16 b, bool sgn) {
  return le(a, b, sgn);
}

// CHECK-LABEL: @_Z16test_gt_v32int16Dv32_sS_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef i32 @llvm.aie2.vlt16(<32 x i16> [[B:%.*]], <32 x i16> [[A:%.*]], i32 [[CONV_I_I]])
// CHECK-NEXT:    ret i32 [[TMP0]]
//
unsigned int test_gt_v32int16(v32int16 a, v32int16 b, bool sgn) {
  return gt(a, b, sgn);
}

// CHECK-LABEL: @_Z17test_ltz_v32int16Dv32_s(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <32 x i16>, i32 } @llvm.aie2.vbneg.ltz16(<32 x i16> [[A:%.*]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 1
// CHECK-NEXT:    ret i32 [[TMP1]]
//
unsigned int test_ltz_v32int16(v32int16 a) { return ltz(a); }

// CHECK-LABEL: @_Z17test_ltz_v32int16Dv32_sb(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <32 x i16>, i32 } @llvm.aie2.vbneg.ltz16(<32 x i16> [[A:%.*]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 1
// CHECK-NEXT:    [[SPEC_SELECT_I_I:%.*]] = select i1 [[SGN:%.*]], i32 [[TMP1]], i32 -1
// CHECK-NEXT:    ret i32 [[SPEC_SELECT_I_I]]
//
unsigned int test_ltz_v32int16(v32int16 a, bool sgn) { return ltz(a, sgn); }

// CHECK-LABEL: @_Z17test_gtz_v32int16Dv32_s(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <32 x i16>, i32 } @llvm.aie2.vabs.gtz16(<32 x i16> [[A:%.*]], i32 1)
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 1
// CHECK-NEXT:    ret i32 [[TMP1]]
//
unsigned int test_gtz_v32int16(v32int16 a) { return gtz(a); }

// CHECK-LABEL: @_Z17test_gtz_v32int16Dv32_sb(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <32 x i16>, i32 } @llvm.aie2.vabs.gtz16(<32 x i16> [[A:%.*]], i32 [[CONV_I_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <32 x i16>, i32 } [[TMP0]], 1
// CHECK-NEXT:    ret i32 [[TMP1]]
//
unsigned int test_gtz_v32int16(v32int16 a, bool sgn) { return gtz(a, sgn); }

// CHECK-LABEL: @_Z17test_eqz_v32int16Dv32_s(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef i32 @llvm.aie2.veqz16(<32 x i16> [[A:%.*]])
// CHECK-NEXT:    ret i32 [[TMP0]]
//
unsigned int test_eqz_v32int16(v32int16 a) { return eqz(a); }

// CHECK-LABEL: @_Z16test_eq_v32int16Dv32_sS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[SUB_I_I:%.*]] = sub <32 x i16> [[A:%.*]], [[B:%.*]]
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef i32 @llvm.aie2.veqz16(<32 x i16> [[SUB_I_I]])
// CHECK-NEXT:    ret i32 [[TMP0]]
//
unsigned int test_eq_v32int16(v32int16 a, v32int16 b) { return eq(a, b); }

// CHECK-LABEL: @_Z16test_ne_v32int16Dv32_sS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[SUB_I_I_I:%.*]] = sub <32 x i16> [[A:%.*]], [[B:%.*]]
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef i32 @llvm.aie2.veqz16(<32 x i16> [[SUB_I_I_I]])
// CHECK-NEXT:    [[NOT_I:%.*]] = xor i32 [[TMP0]], -1
// CHECK-NEXT:    ret i32 [[NOT_I]]
//
unsigned int test_ne_v32int16(v32int16 a, v32int16 b) { return ne(a, b); }

// CHECK-LABEL: @_Z17test_sel_v32int16Dv32_sS_j(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef <32 x i16> @llvm.aie2.vsel16(<32 x i16> [[A:%.*]], <32 x i16> [[B:%.*]], i32 [[S:%.*]])
// CHECK-NEXT:    ret <32 x i16> [[TMP0]]
//
v32int16 test_sel_v32int16(v32int16 a, v32int16 b, unsigned int s) {
  return sel(a, b, s);
}

// CHECK-LABEL: @_Z18test_add_v16uint32Dv16_jS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ADD_I:%.*]] = add <16 x i32> [[B:%.*]], [[A:%.*]]
// CHECK-NEXT:    ret <16 x i32> [[ADD_I]]
//
v16uint32 test_add_v16uint32(v16uint32 a, v16uint32 b) { return add(a, b); }

// CHECK-LABEL: @_Z18test_sub_v16uint32Dv16_jS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[SUB_I:%.*]] = sub <16 x i32> [[A:%.*]], [[B:%.*]]
// CHECK-NEXT:    ret <16 x i32> [[SUB_I]]
//
v16uint32 test_sub_v16uint32(v16uint32 a, v16uint32 b) { return sub(a, b); }

// CHECK-LABEL: @_Z21test_addsub_v16uint32Dv16_jS_j(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vaddsub32(<16 x i32> [[A:%.*]], <16 x i32> [[B:%.*]], i32 [[AS:%.*]])
// CHECK-NEXT:    ret <16 x i32> [[TMP0]]
//
v16uint32 test_addsub_v16uint32(v16uint32 a, v16uint32 b, unsigned int as) {
  return addsub(a, b, as);
}

// CHECK-LABEL: @_Z22test_neg_gtz_v16uint32Dv16_jRj(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <16 x i32>, i32 } @llvm.aie2.vneg.gtz32(<16 x i32> [[A:%.*]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 1
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[CMP:%.*]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <16 x i32> [[TMP2]]
//
v16uint32 test_neg_gtz_v16uint32(v16uint32 a, unsigned int &cmp) {
  return neg_gtz(a, cmp);
}

// CHECK-LABEL: @_Z18test_neg_v16uint32Dv16_j(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <16 x i32>, i32 } @llvm.aie2.vneg.gtz32(<16 x i32> [[A:%.*]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <16 x i32> [[TMP1]]
//
v16uint32 test_neg_v16uint32(v16uint32 a) { return neg(a); }

// CHECK-LABEL: @_Z21test_sub_lt_v16uint32Dv16_jS_Rj(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <16 x i32>, i32 } @llvm.aie2.vsub.lt32(<16 x i32> [[A:%.*]], <16 x i32> [[B:%.*]], i32 0)
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 1
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[CMP:%.*]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <16 x i32> [[TMP2]]
//
v16uint32 test_sub_lt_v16uint32(v16uint32 a, v16uint32 b, unsigned int &cmp) {
  return sub_lt(a, b, cmp);
}

// CHECK-LABEL: @_Z21test_sub_lt_v16uint32Dv16_jS_bRj(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <16 x i32>, i32 } @llvm.aie2.vsub.lt32(<16 x i32> [[A:%.*]], <16 x i32> [[B:%.*]], i32 [[CONV_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 1
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[CMP:%.*]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <16 x i32> [[TMP2]]
//
v16uint32 test_sub_lt_v16uint32(v16uint32 a, v16uint32 b, bool sgn,
                                unsigned int &cmp) {
  return sub_lt(a, b, sgn, cmp);
}

// CHECK-LABEL: @_Z21test_sub_ge_v16uint32Dv16_jS_Rj(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <16 x i32>, i32 } @llvm.aie2.vsub.ge32(<16 x i32> [[A:%.*]], <16 x i32> [[B:%.*]], i32 0)
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 1
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[CMP:%.*]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <16 x i32> [[TMP2]]
//
v16uint32 test_sub_ge_v16uint32(v16uint32 a, v16uint32 b, unsigned int &cmp) {
  return sub_ge(a, b, cmp);
}

// CHECK-LABEL: @_Z21test_sub_ge_v16uint32Dv16_jS_bRj(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <16 x i32>, i32 } @llvm.aie2.vsub.ge32(<16 x i32> [[A:%.*]], <16 x i32> [[B:%.*]], i32 [[CONV_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 1
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[CMP:%.*]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <16 x i32> [[TMP2]]
//
v16uint32 test_sub_ge_v16uint32(v16uint32 a, v16uint32 b, bool sgn,
                                unsigned int &cmp) {
  return sub_ge(a, b, sgn, cmp);
}

// CHECK-LABEL: @_Z25test_maxdiff_lt_v16uint32Dv16_jS_Rj(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <16 x i32>, i32 } @llvm.aie2.vmaxdiff.lt32(<16 x i32> [[A:%.*]], <16 x i32> [[B:%.*]], i32 0)
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 1
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[CMP:%.*]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <16 x i32> [[TMP2]]
//
v16uint32 test_maxdiff_lt_v16uint32(v16uint32 a, v16uint32 b,
                                    unsigned int &cmp) {
  return maxdiff_lt(a, b, cmp);
}

// CHECK-LABEL: @_Z25test_maxdiff_lt_v16uint32Dv16_jS_bRj(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <16 x i32>, i32 } @llvm.aie2.vmaxdiff.lt32(<16 x i32> [[A:%.*]], <16 x i32> [[B:%.*]], i32 [[CONV_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 1
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[CMP:%.*]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <16 x i32> [[TMP2]]
//
v16uint32 test_maxdiff_lt_v16uint32(v16uint32 a, v16uint32 b, bool sgn,
                                    unsigned int &cmp) {
  return maxdiff_lt(a, b, sgn, cmp);
}

// CHECK-LABEL: @_Z22test_maxdiff_v16uint32Dv16_jS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <16 x i32>, i32 } @llvm.aie2.vmaxdiff.lt32(<16 x i32> [[A:%.*]], <16 x i32> [[B:%.*]], i32 0)
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <16 x i32> [[TMP1]]
//
v16uint32 test_maxdiff_v16uint32(v16uint32 a, v16uint32 b) {
  return maxdiff(a, b);
}

// CHECK-LABEL: @_Z22test_maxdiff_v16uint32Dv16_jS_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <16 x i32>, i32 } @llvm.aie2.vmaxdiff.lt32(<16 x i32> [[A:%.*]], <16 x i32> [[B:%.*]], i32 [[CONV_I_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <16 x i32> [[TMP1]]
//
v16uint32 test_maxdiff_v16uint32(v16uint32 a, v16uint32 b, bool sgn) {
  return maxdiff(a, b, sgn);
}

// CHECK-LABEL: @_Z21test_min_ge_v16uint32Dv16_jS_Rj(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <16 x i32>, i32 } @llvm.aie2.vmin.ge32(<16 x i32> [[A:%.*]], <16 x i32> [[B:%.*]], i32 0)
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 1
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[CMP:%.*]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <16 x i32> [[TMP2]]
//
v16uint32 test_min_ge_v16uint32(v16uint32 a, v16uint32 b, unsigned int &cmp) {
  return min_ge(a, b, cmp);
}

// CHECK-LABEL: @_Z21test_min_ge_v16uint32Dv16_jS_bRj(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <16 x i32>, i32 } @llvm.aie2.vmin.ge32(<16 x i32> [[A:%.*]], <16 x i32> [[B:%.*]], i32 [[CONV_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 1
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[CMP:%.*]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <16 x i32> [[TMP2]]
//
v16uint32 test_min_ge_v16uint32(v16uint32 a, v16uint32 b, bool sgn,
                                unsigned int &cmp) {
  return min_ge(a, b, sgn, cmp);
}

// CHECK-LABEL: @_Z18test_min_v16uint32Dv16_jS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <16 x i32>, i32 } @llvm.aie2.vmin.ge32(<16 x i32> [[A:%.*]], <16 x i32> [[B:%.*]], i32 0)
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <16 x i32> [[TMP1]]
//
v16uint32 test_min_v16uint32(v16uint32 a, v16uint32 b) { return min(a, b); }

// CHECK-LABEL: @_Z18test_min_v16uint32Dv16_jS_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <16 x i32>, i32 } @llvm.aie2.vmin.ge32(<16 x i32> [[A:%.*]], <16 x i32> [[B:%.*]], i32 [[CONV_I_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <16 x i32> [[TMP1]]
//
v16uint32 test_min_v16uint32(v16uint32 a, v16uint32 b, bool sgn) {
  return min(a, b, sgn);
}

// CHECK-LABEL: @_Z21test_max_lt_v16uint32Dv16_jS_Rj(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <16 x i32>, i32 } @llvm.aie2.vmax.lt32(<16 x i32> [[A:%.*]], <16 x i32> [[B:%.*]], i32 0)
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 1
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[CMP:%.*]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <16 x i32> [[TMP2]]
//
v16uint32 test_max_lt_v16uint32(v16uint32 a, v16uint32 b, unsigned int &cmp) {
  return max_lt(a, b, cmp);
}

// CHECK-LABEL: @_Z21test_max_lt_v16uint32Dv16_jS_bRj(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <16 x i32>, i32 } @llvm.aie2.vmax.lt32(<16 x i32> [[A:%.*]], <16 x i32> [[B:%.*]], i32 [[CONV_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 1
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[CMP:%.*]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <16 x i32> [[TMP2]]
//
v16uint32 test_max_lt_v16uint32(v16uint32 a, v16uint32 b, bool sgn,
                                unsigned int &cmp) {
  return max_lt(a, b, sgn, cmp);
}

// CHECK-LABEL: @_Z18test_max_v16uint32Dv16_jS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <16 x i32>, i32 } @llvm.aie2.vmax.lt32(<16 x i32> [[A:%.*]], <16 x i32> [[B:%.*]], i32 0)
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <16 x i32> [[TMP1]]
//
v16uint32 test_max_v16uint32(v16uint32 a, v16uint32 b) { return max(a, b); }

// CHECK-LABEL: @_Z18test_max_v16uint32Dv16_jS_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <16 x i32>, i32 } @llvm.aie2.vmax.lt32(<16 x i32> [[A:%.*]], <16 x i32> [[B:%.*]], i32 [[CONV_I_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <16 x i32> [[TMP1]]
//
v16uint32 test_max_v16uint32(v16uint32 a, v16uint32 b, bool sgn) {
  return max(a, b, sgn);
}

// CHECK-LABEL: @_Z19test_band_v16uint32Dv16_jS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[AND_I:%.*]] = and <16 x i32> [[B:%.*]], [[A:%.*]]
// CHECK-NEXT:    ret <16 x i32> [[AND_I]]
//
v16uint32 test_band_v16uint32(v16uint32 a, v16uint32 b) { return band(a, b); }

// CHECK-LABEL: @_Z18test_bor_v16uint32Dv16_jS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[OR_I:%.*]] = or <16 x i32> [[B:%.*]], [[A:%.*]]
// CHECK-NEXT:    ret <16 x i32> [[OR_I]]
//
v16uint32 test_bor_v16uint32(v16uint32 a, v16uint32 b) { return bor(a, b); }

// CHECK-LABEL: @_Z23test_bneg_ltz_v16uint32Dv16_jRj(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <16 x i32>, i32 } @llvm.aie2.vbneg.ltz32(<16 x i32> [[A:%.*]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 0
// CHECK-NEXT:    store i32 -1, ptr [[CMP:%.*]], align 4, !tbaa [[TBAA6]]
// CHECK-NEXT:    ret <16 x i32> [[TMP1]]
//
v16uint32 test_bneg_ltz_v16uint32(v16uint32 a, unsigned int &cmp) {
  return bneg_ltz(a, cmp);
}

// CHECK-LABEL: @_Z23test_bneg_ltz_v16uint32Dv16_jbRj(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <16 x i32>, i32 } @llvm.aie2.vbneg.ltz32(<16 x i32> [[A:%.*]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 1
// CHECK-NEXT:    [[SPEC_SELECT_I:%.*]] = select i1 [[SGN:%.*]], i32 [[TMP1]], i32 -1
// CHECK-NEXT:    store i32 [[SPEC_SELECT_I]], ptr [[CMP:%.*]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <16 x i32> [[TMP2]]
//
v16uint32 test_bneg_ltz_v16uint32(v16uint32 a, bool sgn, unsigned int &cmp) {
  return bneg_ltz(a, sgn, cmp);
}

// CHECK-LABEL: @_Z19test_bneg_v16uint32Dv16_j(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <16 x i32>, i32 } @llvm.aie2.vbneg.ltz32(<16 x i32> [[A:%.*]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <16 x i32> [[TMP1]]
//
v16uint32 test_bneg_v16uint32(v16uint32 a) { return bneg(a); }

// CHECK-LABEL: @_Z19test_bxor_v16uint32Dv16_jS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <16 x i32>, i32 } @llvm.aie2.vbneg.ltz32(<16 x i32> [[B:%.*]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 0
// CHECK-NEXT:    [[AND_I7_I:%.*]] = and <16 x i32> [[TMP1]], [[A:%.*]]
// CHECK-NEXT:    [[TMP2:%.*]] = tail call { <16 x i32>, i32 } @llvm.aie2.vbneg.ltz32(<16 x i32> [[A]])
// CHECK-NEXT:    [[TMP3:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP2]], 0
// CHECK-NEXT:    [[AND_I_I:%.*]] = and <16 x i32> [[TMP3]], [[B]]
// CHECK-NEXT:    [[OR_I_I:%.*]] = or <16 x i32> [[AND_I_I]], [[AND_I7_I]]
// CHECK-NEXT:    ret <16 x i32> [[OR_I_I]]
//
v16uint32 test_bxor_v16uint32(v16uint32 a, v16uint32 b) { return bxor(a, b); }

// CHECK-LABEL: @_Z22test_abs_gtz_v16uint32Dv16_jRj(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <16 x i32>, i32 } @llvm.aie2.vabs.gtz32(<16 x i32> [[A:%.*]], i32 0)
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 1
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[CMP:%.*]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <16 x i32> [[TMP2]]
//
v16uint32 test_abs_gtz_v16uint32(v16uint32 a, unsigned int &cmp) {
  return abs_gtz(a, cmp);
}

// CHECK-LABEL: @_Z22test_abs_gtz_v16uint32Dv16_jbRj(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <16 x i32>, i32 } @llvm.aie2.vabs.gtz32(<16 x i32> [[A:%.*]], i32 [[CONV_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 1
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[CMP:%.*]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <16 x i32> [[TMP2]]
//
v16uint32 test_abs_gtz_v16uint32(v16uint32 a, bool sgn, unsigned int &cmp) {
  return abs_gtz(a, sgn, cmp);
}

// CHECK-LABEL: @_Z18test_abs_v16uint32Dv16_j(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    ret <16 x i32> [[A:%.*]]
//
v16uint32 test_abs_v16uint32(v16uint32 a) { return abs(a); }

// CHECK-LABEL: @_Z18test_abs_v16uint32Dv16_jb(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <16 x i32>, i32 } @llvm.aie2.vabs.gtz32(<16 x i32> [[A:%.*]], i32 [[CONV_I_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <16 x i32> [[TMP1]]
//
v16uint32 test_abs_v16uint32(v16uint32 a, bool sgn) { return abs(a, sgn); }

// CHECK-LABEL: @_Z17test_lt_v16uint32Dv16_jS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef i32 @llvm.aie2.vlt32(<16 x i32> [[A:%.*]], <16 x i32> [[B:%.*]], i32 0)
// CHECK-NEXT:    ret i32 [[TMP0]]
//
unsigned int test_lt_v16uint32(v16uint32 a, v16uint32 b) { return lt(a, b); }

// CHECK-LABEL: @_Z17test_ge_v16uint32Dv16_jS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef i32 @llvm.aie2.vge32(<16 x i32> [[A:%.*]], <16 x i32> [[B:%.*]], i32 0)
// CHECK-NEXT:    ret i32 [[TMP0]]
//
unsigned int test_ge_v16uint32(v16uint32 a, v16uint32 b) { return ge(a, b); }

// CHECK-LABEL: @_Z17test_le_v16uint32Dv16_jS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef i32 @llvm.aie2.vge32(<16 x i32> [[B:%.*]], <16 x i32> [[A:%.*]], i32 0)
// CHECK-NEXT:    ret i32 [[TMP0]]
//
unsigned int test_le_v16uint32(v16uint32 a, v16uint32 b) { return le(a, b); }

// CHECK-LABEL: @_Z17test_gt_v16uint32Dv16_jS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef i32 @llvm.aie2.vlt32(<16 x i32> [[B:%.*]], <16 x i32> [[A:%.*]], i32 0)
// CHECK-NEXT:    ret i32 [[TMP0]]
//
unsigned int test_gt_v16uint32(v16uint32 a, v16uint32 b) { return gt(a, b); }

// CHECK-LABEL: @_Z17test_lt_v16uint32Dv16_jS_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef i32 @llvm.aie2.vlt32(<16 x i32> [[A:%.*]], <16 x i32> [[B:%.*]], i32 [[CONV_I]])
// CHECK-NEXT:    ret i32 [[TMP0]]
//
unsigned int test_lt_v16uint32(v16uint32 a, v16uint32 b, bool sgn) {
  return lt(a, b, sgn);
}

// CHECK-LABEL: @_Z17test_ge_v16uint32Dv16_jS_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef i32 @llvm.aie2.vge32(<16 x i32> [[A:%.*]], <16 x i32> [[B:%.*]], i32 [[CONV_I]])
// CHECK-NEXT:    ret i32 [[TMP0]]
//
unsigned int test_ge_v16uint32(v16uint32 a, v16uint32 b, bool sgn) {
  return ge(a, b, sgn);
}

// CHECK-LABEL: @_Z17test_le_v16uint32Dv16_jS_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef i32 @llvm.aie2.vge32(<16 x i32> [[B:%.*]], <16 x i32> [[A:%.*]], i32 [[CONV_I_I]])
// CHECK-NEXT:    ret i32 [[TMP0]]
//
unsigned int test_le_v16uint32(v16uint32 a, v16uint32 b, bool sgn) {
  return le(a, b, sgn);
}

// CHECK-LABEL: @_Z17test_gt_v16uint32Dv16_jS_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef i32 @llvm.aie2.vlt32(<16 x i32> [[B:%.*]], <16 x i32> [[A:%.*]], i32 [[CONV_I_I]])
// CHECK-NEXT:    ret i32 [[TMP0]]
//
unsigned int test_gt_v16uint32(v16uint32 a, v16uint32 b, bool sgn) {
  return gt(a, b, sgn);
}

// CHECK-LABEL: @_Z18test_ltz_v16uint32Dv16_j(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    ret i32 -1
//
unsigned int test_ltz_v16uint32(v16uint32 a) { return ltz(a); }

// CHECK-LABEL: @_Z18test_ltz_v16uint32Dv16_jb(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <16 x i32>, i32 } @llvm.aie2.vbneg.ltz32(<16 x i32> [[A:%.*]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 1
// CHECK-NEXT:    [[SPEC_SELECT_I_I:%.*]] = select i1 [[SGN:%.*]], i32 [[TMP1]], i32 -1
// CHECK-NEXT:    ret i32 [[SPEC_SELECT_I_I]]
//
unsigned int test_ltz_v16uint32(v16uint32 a, bool sgn) { return ltz(a, sgn); }

// CHECK-LABEL: @_Z18test_gtz_v16uint32Dv16_j(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <16 x i32>, i32 } @llvm.aie2.vabs.gtz32(<16 x i32> [[A:%.*]], i32 0)
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 1
// CHECK-NEXT:    ret i32 [[TMP1]]
//
unsigned int test_gtz_v16uint32(v16uint32 a) { return gtz(a); }

// CHECK-LABEL: @_Z18test_gtz_v16uint32Dv16_jb(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <16 x i32>, i32 } @llvm.aie2.vabs.gtz32(<16 x i32> [[A:%.*]], i32 [[CONV_I_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 1
// CHECK-NEXT:    ret i32 [[TMP1]]
//
unsigned int test_gtz_v16uint32(v16uint32 a, bool sgn) { return gtz(a, sgn); }

// CHECK-LABEL: @_Z18test_eqz_v16uint32Dv16_j(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef i32 @llvm.aie2.veqz32(<16 x i32> [[A:%.*]])
// CHECK-NEXT:    ret i32 [[TMP0]]
//
unsigned int test_eqz_v16uint32(v16uint32 a) { return eqz(a); }

// CHECK-LABEL: @_Z17test_eq_v16uint32Dv16_jS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[SUB_I_I:%.*]] = sub <16 x i32> [[A:%.*]], [[B:%.*]]
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef i32 @llvm.aie2.veqz32(<16 x i32> [[SUB_I_I]])
// CHECK-NEXT:    ret i32 [[TMP0]]
//
unsigned int test_eq_v16uint32(v16uint32 a, v16uint32 b) { return eq(a, b); }

// CHECK-LABEL: @_Z17test_ne_v16uint32Dv16_jS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[SUB_I_I_I:%.*]] = sub <16 x i32> [[A:%.*]], [[B:%.*]]
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef i32 @llvm.aie2.veqz32(<16 x i32> [[SUB_I_I_I]])
// CHECK-NEXT:    [[NOT_I:%.*]] = xor i32 [[TMP0]], -1
// CHECK-NEXT:    ret i32 [[NOT_I]]
//
unsigned int test_ne_v16uint32(v16uint32 a, v16uint32 b) { return ne(a, b); }

// CHECK-LABEL: @_Z18test_sel_v16uint32Dv16_jS_j(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vsel32(<16 x i32> [[A:%.*]], <16 x i32> [[B:%.*]], i32 [[S:%.*]])
// CHECK-NEXT:    ret <16 x i32> [[TMP0]]
//
v16uint32 test_sel_v16uint32(v16uint32 a, v16uint32 b, unsigned int s) {
  return sel(a, b, s);
}

// CHECK-LABEL: @_Z17test_add_v16int32Dv16_iS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[ADD_I:%.*]] = add <16 x i32> [[B:%.*]], [[A:%.*]]
// CHECK-NEXT:    ret <16 x i32> [[ADD_I]]
//
v16int32 test_add_v16int32(v16int32 a, v16int32 b) { return add(a, b); }

// CHECK-LABEL: @_Z17test_sub_v16int32Dv16_iS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[SUB_I:%.*]] = sub <16 x i32> [[A:%.*]], [[B:%.*]]
// CHECK-NEXT:    ret <16 x i32> [[SUB_I]]
//
v16int32 test_sub_v16int32(v16int32 a, v16int32 b) { return sub(a, b); }

// CHECK-LABEL: @_Z20test_addsub_v16int32Dv16_iS_j(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vaddsub32(<16 x i32> [[A:%.*]], <16 x i32> [[B:%.*]], i32 [[AS:%.*]])
// CHECK-NEXT:    ret <16 x i32> [[TMP0]]
//
v16int32 test_addsub_v16int32(v16int32 a, v16int32 b, unsigned int as) {
  return addsub(a, b, as);
}

// CHECK-LABEL: @_Z21test_neg_gtz_v16int32Dv16_iRj(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <16 x i32>, i32 } @llvm.aie2.vneg.gtz32(<16 x i32> [[A:%.*]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 1
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[CMP:%.*]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <16 x i32> [[TMP2]]
//
v16int32 test_neg_gtz_v16int32(v16int32 a, unsigned int &cmp) {
  return neg_gtz(a, cmp);
}

// CHECK-LABEL: @_Z17test_neg_v16int32Dv16_i(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <16 x i32>, i32 } @llvm.aie2.vneg.gtz32(<16 x i32> [[A:%.*]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <16 x i32> [[TMP1]]
//
v16int32 test_neg_v16int32(v16int32 a) { return neg(a); }

// CHECK-LABEL: @_Z20test_sub_lt_v16int32Dv16_iS_Rj(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <16 x i32>, i32 } @llvm.aie2.vsub.lt32(<16 x i32> [[A:%.*]], <16 x i32> [[B:%.*]], i32 1)
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 1
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[CMP:%.*]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <16 x i32> [[TMP2]]
//
v16int32 test_sub_lt_v16int32(v16int32 a, v16int32 b, unsigned int &cmp) {
  return sub_lt(a, b, cmp);
}

// CHECK-LABEL: @_Z20test_sub_lt_v16int32Dv16_iS_bRj(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <16 x i32>, i32 } @llvm.aie2.vsub.lt32(<16 x i32> [[A:%.*]], <16 x i32> [[B:%.*]], i32 [[CONV_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 1
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[CMP:%.*]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <16 x i32> [[TMP2]]
//
v16int32 test_sub_lt_v16int32(v16int32 a, v16int32 b, bool sgn,
                              unsigned int &cmp) {
  return sub_lt(a, b, sgn, cmp);
}

// CHECK-LABEL: @_Z20test_sub_ge_v16int32Dv16_iS_Rj(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <16 x i32>, i32 } @llvm.aie2.vsub.ge32(<16 x i32> [[A:%.*]], <16 x i32> [[B:%.*]], i32 1)
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 1
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[CMP:%.*]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <16 x i32> [[TMP2]]
//
v16int32 test_sub_ge_v16int32(v16int32 a, v16int32 b, unsigned int &cmp) {
  return sub_ge(a, b, cmp);
}

// CHECK-LABEL: @_Z20test_sub_ge_v16int32Dv16_iS_bRj(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <16 x i32>, i32 } @llvm.aie2.vsub.ge32(<16 x i32> [[A:%.*]], <16 x i32> [[B:%.*]], i32 [[CONV_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 1
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[CMP:%.*]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <16 x i32> [[TMP2]]
//
v16int32 test_sub_ge_v16int32(v16int32 a, v16int32 b, bool sgn,
                              unsigned int &cmp) {
  return sub_ge(a, b, sgn, cmp);
}

// CHECK-LABEL: @_Z24test_maxdiff_lt_v16int32Dv16_iS_Rj(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <16 x i32>, i32 } @llvm.aie2.vmaxdiff.lt32(<16 x i32> [[A:%.*]], <16 x i32> [[B:%.*]], i32 1)
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 1
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[CMP:%.*]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <16 x i32> [[TMP2]]
//
v16int32 test_maxdiff_lt_v16int32(v16int32 a, v16int32 b, unsigned int &cmp) {
  return maxdiff_lt(a, b, cmp);
}

// CHECK-LABEL: @_Z24test_maxdiff_lt_v16int32Dv16_iS_bRj(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <16 x i32>, i32 } @llvm.aie2.vmaxdiff.lt32(<16 x i32> [[A:%.*]], <16 x i32> [[B:%.*]], i32 [[CONV_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 1
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[CMP:%.*]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <16 x i32> [[TMP2]]
//
v16int32 test_maxdiff_lt_v16int32(v16int32 a, v16int32 b, bool sgn,
                                  unsigned int &cmp) {
  return maxdiff_lt(a, b, sgn, cmp);
}

// CHECK-LABEL: @_Z21test_maxdiff_v16int32Dv16_iS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <16 x i32>, i32 } @llvm.aie2.vmaxdiff.lt32(<16 x i32> [[A:%.*]], <16 x i32> [[B:%.*]], i32 1)
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <16 x i32> [[TMP1]]
//
v16int32 test_maxdiff_v16int32(v16int32 a, v16int32 b) { return maxdiff(a, b); }

// CHECK-LABEL: @_Z21test_maxdiff_v16int32Dv16_iS_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <16 x i32>, i32 } @llvm.aie2.vmaxdiff.lt32(<16 x i32> [[A:%.*]], <16 x i32> [[B:%.*]], i32 [[CONV_I_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <16 x i32> [[TMP1]]
//
v16int32 test_maxdiff_v16int32(v16int32 a, v16int32 b, bool sgn) {
  return maxdiff(a, b, sgn);
}

// CHECK-LABEL: @_Z20test_min_ge_v16int32Dv16_iS_Rj(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <16 x i32>, i32 } @llvm.aie2.vmin.ge32(<16 x i32> [[A:%.*]], <16 x i32> [[B:%.*]], i32 1)
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 1
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[CMP:%.*]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <16 x i32> [[TMP2]]
//
v16int32 test_min_ge_v16int32(v16int32 a, v16int32 b, unsigned int &cmp) {
  return min_ge(a, b, cmp);
}

// CHECK-LABEL: @_Z20test_min_ge_v16int32Dv16_iS_bRj(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <16 x i32>, i32 } @llvm.aie2.vmin.ge32(<16 x i32> [[A:%.*]], <16 x i32> [[B:%.*]], i32 [[CONV_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 1
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[CMP:%.*]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <16 x i32> [[TMP2]]
//
v16int32 test_min_ge_v16int32(v16int32 a, v16int32 b, bool sgn,
                              unsigned int &cmp) {
  return min_ge(a, b, sgn, cmp);
}

// CHECK-LABEL: @_Z17test_min_v16int32Dv16_iS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <16 x i32>, i32 } @llvm.aie2.vmin.ge32(<16 x i32> [[A:%.*]], <16 x i32> [[B:%.*]], i32 1)
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <16 x i32> [[TMP1]]
//
v16int32 test_min_v16int32(v16int32 a, v16int32 b) { return min(a, b); }

// CHECK-LABEL: @_Z17test_min_v16int32Dv16_iS_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <16 x i32>, i32 } @llvm.aie2.vmin.ge32(<16 x i32> [[A:%.*]], <16 x i32> [[B:%.*]], i32 [[CONV_I_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <16 x i32> [[TMP1]]
//
v16int32 test_min_v16int32(v16int32 a, v16int32 b, bool sgn) {
  return min(a, b, sgn);
}

// CHECK-LABEL: @_Z20test_max_lt_v16int32Dv16_iS_Rj(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <16 x i32>, i32 } @llvm.aie2.vmax.lt32(<16 x i32> [[A:%.*]], <16 x i32> [[B:%.*]], i32 1)
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 1
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[CMP:%.*]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <16 x i32> [[TMP2]]
//
v16int32 test_max_lt_v16int32(v16int32 a, v16int32 b, unsigned int &cmp) {
  return max_lt(a, b, cmp);
}

// CHECK-LABEL: @_Z20test_max_lt_v16int32Dv16_iS_bRj(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <16 x i32>, i32 } @llvm.aie2.vmax.lt32(<16 x i32> [[A:%.*]], <16 x i32> [[B:%.*]], i32 [[CONV_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 1
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[CMP:%.*]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <16 x i32> [[TMP2]]
//
v16int32 test_max_lt_v16int32(v16int32 a, v16int32 b, bool sgn,
                              unsigned int &cmp) {
  return max_lt(a, b, sgn, cmp);
}

// CHECK-LABEL: @_Z17test_max_v16int32Dv16_iS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <16 x i32>, i32 } @llvm.aie2.vmax.lt32(<16 x i32> [[A:%.*]], <16 x i32> [[B:%.*]], i32 1)
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <16 x i32> [[TMP1]]
//
v16int32 test_max_v16int32(v16int32 a, v16int32 b) { return max(a, b); }

// CHECK-LABEL: @_Z17test_max_v16int32Dv16_iS_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <16 x i32>, i32 } @llvm.aie2.vmax.lt32(<16 x i32> [[A:%.*]], <16 x i32> [[B:%.*]], i32 [[CONV_I_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <16 x i32> [[TMP1]]
//
v16int32 test_max_v16int32(v16int32 a, v16int32 b, bool sgn) {
  return max(a, b, sgn);
}

// CHECK-LABEL: @_Z18test_band_v16int32Dv16_iS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[AND_I:%.*]] = and <16 x i32> [[B:%.*]], [[A:%.*]]
// CHECK-NEXT:    ret <16 x i32> [[AND_I]]
//
v16int32 test_band_v16int32(v16int32 a, v16int32 b) { return band(a, b); }

// CHECK-LABEL: @_Z17test_bor_v16int32Dv16_iS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[OR_I:%.*]] = or <16 x i32> [[B:%.*]], [[A:%.*]]
// CHECK-NEXT:    ret <16 x i32> [[OR_I]]
//
v16int32 test_bor_v16int32(v16int32 a, v16int32 b) { return bor(a, b); }

// CHECK-LABEL: @_Z22test_bneg_ltz_v16int32Dv16_iRj(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <16 x i32>, i32 } @llvm.aie2.vbneg.ltz32(<16 x i32> [[A:%.*]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 1
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[CMP:%.*]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <16 x i32> [[TMP2]]
//
v16int32 test_bneg_ltz_v16int32(v16int32 a, unsigned int &cmp) {
  return bneg_ltz(a, cmp);
}

// CHECK-LABEL: @_Z22test_bneg_ltz_v16int32Dv16_ibRj(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <16 x i32>, i32 } @llvm.aie2.vbneg.ltz32(<16 x i32> [[A:%.*]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 1
// CHECK-NEXT:    [[SPEC_SELECT_I:%.*]] = select i1 [[SGN:%.*]], i32 [[TMP1]], i32 -1
// CHECK-NEXT:    store i32 [[SPEC_SELECT_I]], ptr [[CMP:%.*]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <16 x i32> [[TMP2]]
//
v16int32 test_bneg_ltz_v16int32(v16int32 a, bool sgn, unsigned int &cmp) {
  return bneg_ltz(a, sgn, cmp);
}

// CHECK-LABEL: @_Z18test_bneg_v16int32Dv16_i(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <16 x i32>, i32 } @llvm.aie2.vbneg.ltz32(<16 x i32> [[A:%.*]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <16 x i32> [[TMP1]]
//
v16int32 test_bneg_v16int32(v16int32 a) { return bneg(a); }

// CHECK-LABEL: @_Z18test_bxor_v16int32Dv16_iS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <16 x i32>, i32 } @llvm.aie2.vbneg.ltz32(<16 x i32> [[B:%.*]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 0
// CHECK-NEXT:    [[AND_I7_I:%.*]] = and <16 x i32> [[TMP1]], [[A:%.*]]
// CHECK-NEXT:    [[TMP2:%.*]] = tail call { <16 x i32>, i32 } @llvm.aie2.vbneg.ltz32(<16 x i32> [[A]])
// CHECK-NEXT:    [[TMP3:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP2]], 0
// CHECK-NEXT:    [[AND_I_I:%.*]] = and <16 x i32> [[TMP3]], [[B]]
// CHECK-NEXT:    [[OR_I_I:%.*]] = or <16 x i32> [[AND_I_I]], [[AND_I7_I]]
// CHECK-NEXT:    ret <16 x i32> [[OR_I_I]]
//
v16int32 test_bxor_v16int32(v16int32 a, v16int32 b) { return bxor(a, b); }

// CHECK-LABEL: @_Z21test_abs_gtz_v16int32Dv16_iRj(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <16 x i32>, i32 } @llvm.aie2.vabs.gtz32(<16 x i32> [[A:%.*]], i32 1)
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 1
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[CMP:%.*]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <16 x i32> [[TMP2]]
//
v16int32 test_abs_gtz_v16int32(v16int32 a, unsigned int &cmp) {
  return abs_gtz(a, cmp);
}

// CHECK-LABEL: @_Z21test_abs_gtz_v16int32Dv16_ibRj(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <16 x i32>, i32 } @llvm.aie2.vabs.gtz32(<16 x i32> [[A:%.*]], i32 [[CONV_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 1
// CHECK-NEXT:    store i32 [[TMP1]], ptr [[CMP:%.*]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <16 x i32> [[TMP2]]
//
v16int32 test_abs_gtz_v16int32(v16int32 a, bool sgn, unsigned int &cmp) {
  return abs_gtz(a, sgn, cmp);
}

// CHECK-LABEL: @_Z17test_abs_v16int32Dv16_i(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <16 x i32>, i32 } @llvm.aie2.vabs.gtz32(<16 x i32> [[A:%.*]], i32 1)
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <16 x i32> [[TMP1]]
//
v16int32 test_abs_v16int32(v16int32 a) { return abs(a); }

// CHECK-LABEL: @_Z17test_abs_v16int32Dv16_ib(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <16 x i32>, i32 } @llvm.aie2.vabs.gtz32(<16 x i32> [[A:%.*]], i32 [[CONV_I_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 0
// CHECK-NEXT:    ret <16 x i32> [[TMP1]]
//
v16int32 test_abs_v16int32(v16int32 a, bool sgn) { return abs(a, sgn); }

// CHECK-LABEL: @_Z16test_lt_v16int32Dv16_iS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef i32 @llvm.aie2.vlt32(<16 x i32> [[A:%.*]], <16 x i32> [[B:%.*]], i32 1)
// CHECK-NEXT:    ret i32 [[TMP0]]
//
unsigned int test_lt_v16int32(v16int32 a, v16int32 b) { return lt(a, b); }

// CHECK-LABEL: @_Z16test_ge_v16int32Dv16_iS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef i32 @llvm.aie2.vge32(<16 x i32> [[A:%.*]], <16 x i32> [[B:%.*]], i32 1)
// CHECK-NEXT:    ret i32 [[TMP0]]
//
unsigned int test_ge_v16int32(v16int32 a, v16int32 b) { return ge(a, b); }

// CHECK-LABEL: @_Z16test_le_v16int32Dv16_iS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef i32 @llvm.aie2.vge32(<16 x i32> [[B:%.*]], <16 x i32> [[A:%.*]], i32 1)
// CHECK-NEXT:    ret i32 [[TMP0]]
//
unsigned int test_le_v16int32(v16int32 a, v16int32 b) { return le(a, b); }

// CHECK-LABEL: @_Z16test_gt_v16int32Dv16_iS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef i32 @llvm.aie2.vlt32(<16 x i32> [[B:%.*]], <16 x i32> [[A:%.*]], i32 1)
// CHECK-NEXT:    ret i32 [[TMP0]]
//
unsigned int test_gt_v16int32(v16int32 a, v16int32 b) { return gt(a, b); }

// CHECK-LABEL: @_Z16test_lt_v16int32Dv16_iS_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef i32 @llvm.aie2.vlt32(<16 x i32> [[A:%.*]], <16 x i32> [[B:%.*]], i32 [[CONV_I]])
// CHECK-NEXT:    ret i32 [[TMP0]]
//
unsigned int test_lt_v16int32(v16int32 a, v16int32 b, bool sgn) {
  return lt(a, b, sgn);
}

// CHECK-LABEL: @_Z16test_ge_v16int32Dv16_iS_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef i32 @llvm.aie2.vge32(<16 x i32> [[A:%.*]], <16 x i32> [[B:%.*]], i32 [[CONV_I]])
// CHECK-NEXT:    ret i32 [[TMP0]]
//
unsigned int test_ge_v16int32(v16int32 a, v16int32 b, bool sgn) {
  return ge(a, b, sgn);
}

// CHECK-LABEL: @_Z16test_le_v16int32Dv16_iS_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef i32 @llvm.aie2.vge32(<16 x i32> [[B:%.*]], <16 x i32> [[A:%.*]], i32 [[CONV_I_I]])
// CHECK-NEXT:    ret i32 [[TMP0]]
//
unsigned int test_le_v16int32(v16int32 a, v16int32 b, bool sgn) {
  return le(a, b, sgn);
}

// CHECK-LABEL: @_Z16test_gt_v16int32Dv16_iS_b(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef i32 @llvm.aie2.vlt32(<16 x i32> [[B:%.*]], <16 x i32> [[A:%.*]], i32 [[CONV_I_I]])
// CHECK-NEXT:    ret i32 [[TMP0]]
//
unsigned int test_gt_v16int32(v16int32 a, v16int32 b, bool sgn) {
  return gt(a, b, sgn);
}

// CHECK-LABEL: @_Z17test_ltz_v16int32Dv16_i(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <16 x i32>, i32 } @llvm.aie2.vbneg.ltz32(<16 x i32> [[A:%.*]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 1
// CHECK-NEXT:    ret i32 [[TMP1]]
//
unsigned int test_ltz_v16int32(v16int32 a) { return ltz(a); }

// CHECK-LABEL: @_Z17test_ltz_v16int32Dv16_ib(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <16 x i32>, i32 } @llvm.aie2.vbneg.ltz32(<16 x i32> [[A:%.*]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 1
// CHECK-NEXT:    [[SPEC_SELECT_I_I:%.*]] = select i1 [[SGN:%.*]], i32 [[TMP1]], i32 -1
// CHECK-NEXT:    ret i32 [[SPEC_SELECT_I_I]]
//
unsigned int test_ltz_v16int32(v16int32 a, bool sgn) { return ltz(a, sgn); }

// CHECK-LABEL: @_Z17test_gtz_v16int32Dv16_i(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <16 x i32>, i32 } @llvm.aie2.vabs.gtz32(<16 x i32> [[A:%.*]], i32 1)
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 1
// CHECK-NEXT:    ret i32 [[TMP1]]
//
unsigned int test_gtz_v16int32(v16int32 a) { return gtz(a); }

// CHECK-LABEL: @_Z17test_gtz_v16int32Dv16_ib(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I_I:%.*]] = zext i1 [[SGN:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call { <16 x i32>, i32 } @llvm.aie2.vabs.gtz32(<16 x i32> [[A:%.*]], i32 [[CONV_I_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP0]], 1
// CHECK-NEXT:    ret i32 [[TMP1]]
//
unsigned int test_gtz_v16int32(v16int32 a, bool sgn) { return gtz(a, sgn); }

// CHECK-LABEL: @_Z17test_eqz_v16int32Dv16_i(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef i32 @llvm.aie2.veqz32(<16 x i32> [[A:%.*]])
// CHECK-NEXT:    ret i32 [[TMP0]]
//
unsigned int test_eqz_v16int32(v16int32 a) { return eqz(a); }

// CHECK-LABEL: @_Z16test_eq_v16int32Dv16_iS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[SUB_I_I:%.*]] = sub <16 x i32> [[A:%.*]], [[B:%.*]]
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef i32 @llvm.aie2.veqz32(<16 x i32> [[SUB_I_I]])
// CHECK-NEXT:    ret i32 [[TMP0]]
//
unsigned int test_eq_v16int32(v16int32 a, v16int32 b) { return eq(a, b); }

// CHECK-LABEL: @_Z16test_ne_v16int32Dv16_iS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[SUB_I_I_I:%.*]] = sub <16 x i32> [[A:%.*]], [[B:%.*]]
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef i32 @llvm.aie2.veqz32(<16 x i32> [[SUB_I_I_I]])
// CHECK-NEXT:    [[NOT_I:%.*]] = xor i32 [[TMP0]], -1
// CHECK-NEXT:    ret i32 [[NOT_I]]
//
unsigned int test_ne_v16int32(v16int32 a, v16int32 b) { return ne(a, b); }

// CHECK-LABEL: @_Z17test_sel_v16int32Dv16_iS_j(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vsel32(<16 x i32> [[A:%.*]], <16 x i32> [[B:%.*]], i32 [[S:%.*]])
// CHECK-NEXT:    ret <16 x i32> [[TMP0]]
//
v16int32 test_sel_v16int32(v16int32 a, v16int32 b, unsigned int s) {
  return sel(a, b, s);
}

// CHECK-LABEL: @_Z20test_sel_v32bfloat16Dv32_u6__bf16S_j(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <32 x bfloat> [[A:%.*]] to <32 x i16>
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <32 x bfloat> [[B:%.*]] to <32 x i16>
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <32 x i16> @llvm.aie2.vsel16(<32 x i16> [[TMP0]], <32 x i16> [[TMP1]], i32 [[S:%.*]])
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <32 x i16> [[TMP2]] to <32 x bfloat>
// CHECK-NEXT:    ret <32 x bfloat> [[TMP3]]
//
v32bfloat16 test_sel_v32bfloat16(v32bfloat16 a, v32bfloat16 b, unsigned int s) {
  return sel(a, b, s);
}

// CHECK-LABEL: @_Z17test_sel_v16floatDv16_fS_j(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <16 x float> [[A:%.*]] to <16 x i32>
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <16 x float> [[B:%.*]] to <16 x i32>
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <16 x i32> @llvm.aie2.vsel32(<16 x i32> [[TMP0]], <16 x i32> [[TMP1]], i32 [[S:%.*]])
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <16 x i32> [[TMP2]] to <16 x float>
// CHECK-NEXT:    ret <16 x float> [[TMP3]]
//
v16float test_sel_v16float(v16float a, v16float b, unsigned int s) {
  return sel(a, b, s);
}

// CHECK-LABEL: @_Z19test_lt_v32bfloat16Dv32_u6__bf16S_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef i32 @llvm.aie2.vltbf16(<32 x bfloat> [[A:%.*]], <32 x bfloat> [[B:%.*]])
// CHECK-NEXT:    ret i32 [[TMP0]]
//
unsigned int test_lt_v32bfloat16(v32bfloat16 a, v32bfloat16 b) {
  return lt(a, b);
}

// CHECK-LABEL: @_Z19test_ge_v32bfloat16Dv32_u6__bf16S_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef i32 @llvm.aie2.vgebf16(<32 x bfloat> [[A:%.*]], <32 x bfloat> [[B:%.*]])
// CHECK-NEXT:    ret i32 [[TMP0]]
//
unsigned int test_ge_v32bfloat16(v32bfloat16 a, v32bfloat16 b) {
  return ge(a, b);
}

// CHECK-LABEL: @_Z19test_le_v32bfloat16Dv32_u6__bf16S_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef i32 @llvm.aie2.vgebf16(<32 x bfloat> [[B:%.*]], <32 x bfloat> [[A:%.*]])
// CHECK-NEXT:    ret i32 [[TMP0]]
//
unsigned int test_le_v32bfloat16(v32bfloat16 a, v32bfloat16 b) {
  return le(a, b);
}

// CHECK-LABEL: @_Z19test_gt_v32bfloat16Dv32_u6__bf16S_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef i32 @llvm.aie2.vltbf16(<32 x bfloat> [[B:%.*]], <32 x bfloat> [[A:%.*]])
// CHECK-NEXT:    ret i32 [[TMP0]]
//
unsigned int test_gt_v32bfloat16(v32bfloat16 a, v32bfloat16 b) {
  return gt(a, b);
}

// CHECK-LABEL: @_Z20test_ltz_v32bfloat16Dv32_u6__bf16(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef <32 x i16> @llvm.aie2.vbroadcast16.I512(i32 32768)
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <32 x i16> [[TMP0]] to <32 x bfloat>
// CHECK-NEXT:    [[TMP2:%.*]] = tail call noundef i32 @llvm.aie2.vltbf16(<32 x bfloat> [[A:%.*]], <32 x bfloat> [[TMP1]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
unsigned int test_ltz_v32bfloat16(v32bfloat16 a) {
  return ltz(a);
}

// CHECK-LABEL: @_Z20test_gtz_v32bfloat16Dv32_u6__bf16(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef <32 x i16> @llvm.aie2.vbroadcast16.I512(i32 0)
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <32 x i16> [[TMP0]] to <32 x bfloat>
// CHECK-NEXT:    [[TMP2:%.*]] = tail call noundef i32 @llvm.aie2.vltbf16(<32 x bfloat> [[TMP1]], <32 x bfloat> [[A:%.*]])
// CHECK-NEXT:    ret i32 [[TMP2]]
//
unsigned int test_gtz_v32bfloat16(v32bfloat16 a) {
  return gtz(a);
}

// CHECK-LABEL: @_Z20test_eqz_v32bfloat16Dv32_u6__bf16(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <32 x bfloat> [[A:%.*]] to <32 x i16>
// CHECK-NEXT:    [[TMP1:%.*]] = tail call noundef i32 @llvm.aie2.veqz16(<32 x i16> [[TMP0]])
// CHECK-NEXT:    ret i32 [[TMP1]]
//
unsigned int test_eqz_v32bfloat16(v32bfloat16 a) {
  return eqz(a);
}

// CHECK-LABEL: @_Z19test_eq_v32bfloat16Dv32_u6__bf16S_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef i32 @llvm.aie2.vgebf16(<32 x bfloat> [[B:%.*]], <32 x bfloat> [[A:%.*]])
// CHECK-NEXT:    [[TMP1:%.*]] = tail call noundef i32 @llvm.aie2.vgebf16(<32 x bfloat> [[A]], <32 x bfloat> [[B]])
// CHECK-NEXT:    [[AND_I:%.*]] = and i32 [[TMP1]], [[TMP0]]
// CHECK-NEXT:    ret i32 [[AND_I]]
//
unsigned int test_eq_v32bfloat16(v32bfloat16 a, v32bfloat16 b) {
  return eq(a, b);
}

// CHECK-LABEL: @_Z19test_ne_v32bfloat16Dv32_u6__bf16S_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef i32 @llvm.aie2.vltbf16(<32 x bfloat> [[A:%.*]], <32 x bfloat> [[B:%.*]])
// CHECK-NEXT:    [[TMP1:%.*]] = tail call noundef i32 @llvm.aie2.vltbf16(<32 x bfloat> [[B]], <32 x bfloat> [[A]])
// CHECK-NEXT:    [[OR_I:%.*]] = or i32 [[TMP1]], [[TMP0]]
// CHECK-NEXT:    ret i32 [[OR_I]]
//
unsigned int test_ne_v32bfloat16(v32bfloat16 a, v32bfloat16 b) {
  return ne(a, b);
}

// CHECK-LABEL: @_Z7test_ltDv16_fS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef <32 x bfloat> @llvm.aie2.vbroadcast16.bf512(bfloat 0xR0000)
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <16 x float> [[V1:%.*]] to <8 x i64>
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <16 x float> [[V2:%.*]] to <8 x i64>
// CHECK-NEXT:    [[TMP3:%.*]] = tail call noundef <8 x i64> @llvm.aie2.sub.accfloat(<8 x i64> [[TMP1]], <8 x i64> [[TMP2]], i32 28)
// CHECK-NEXT:    [[TMP4:%.*]] = tail call noundef <16 x bfloat> @llvm.aie2.v16accfloat.to.v16bf16(<8 x i64> [[TMP3]])
// CHECK-NEXT:    [[TMP5:%.*]] = tail call noundef <32 x bfloat> @llvm.aie2.upd.bf512.bf256(<32 x bfloat> [[TMP0]], <16 x bfloat> [[TMP4]], i32 0)
// CHECK-NEXT:    [[TMP6:%.*]] = tail call noundef i32 @llvm.aie2.vltbf16(<32 x bfloat> [[TMP5]], <32 x bfloat> [[TMP0]])
// CHECK-NEXT:    ret i32 [[TMP6]]
//
unsigned int test_lt(v16float v1, v16float v2) {
  return lt(v1, v2);
}

// CHECK-LABEL: @_Z11test_max_ltDv16_fS_Rj(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef <32 x bfloat> @llvm.aie2.vbroadcast16.bf512(bfloat 0xR0000)
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <16 x float> [[V1:%.*]] to <8 x i64>
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <16 x float> [[V2:%.*]] to <8 x i64>
// CHECK-NEXT:    [[TMP3:%.*]] = tail call noundef <8 x i64> @llvm.aie2.sub.accfloat(<8 x i64> [[TMP1]], <8 x i64> [[TMP2]], i32 28)
// CHECK-NEXT:    [[TMP4:%.*]] = tail call noundef <16 x bfloat> @llvm.aie2.v16accfloat.to.v16bf16(<8 x i64> [[TMP3]])
// CHECK-NEXT:    [[TMP5:%.*]] = tail call noundef <32 x bfloat> @llvm.aie2.upd.bf512.bf256(<32 x bfloat> [[TMP0]], <16 x bfloat> [[TMP4]], i32 0)
// CHECK-NEXT:    [[TMP6:%.*]] = tail call noundef i32 @llvm.aie2.vltbf16(<32 x bfloat> [[TMP5]], <32 x bfloat> [[TMP0]])
// CHECK-NEXT:    store i32 [[TMP6]], ptr [[CMP:%.*]], align 4, !tbaa [[TBAA6]]
// CHECK-NEXT:    [[TMP7:%.*]] = bitcast <16 x float> [[V1]] to <16 x i32>
// CHECK-NEXT:    [[TMP8:%.*]] = bitcast <16 x float> [[V2]] to <16 x i32>
// CHECK-NEXT:    [[TMP9:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vsel32(<16 x i32> [[TMP7]], <16 x i32> [[TMP8]], i32 [[TMP6]])
// CHECK-NEXT:    [[TMP10:%.*]] = bitcast <16 x i32> [[TMP9]] to <16 x float>
// CHECK-NEXT:    ret <16 x float> [[TMP10]]
//
v16float test_max_lt(v16float v1, v16float v2, unsigned int &cmp) {
  return max_lt(v1, v2, cmp);
}

// CHECK-LABEL: @_Z8test_maxDv16_fS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef <32 x bfloat> @llvm.aie2.vbroadcast16.bf512(bfloat 0xR0000)
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <16 x float> [[V1:%.*]] to <8 x i64>
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <16 x float> [[V2:%.*]] to <8 x i64>
// CHECK-NEXT:    [[TMP3:%.*]] = tail call noundef <8 x i64> @llvm.aie2.sub.accfloat(<8 x i64> [[TMP1]], <8 x i64> [[TMP2]], i32 28)
// CHECK-NEXT:    [[TMP4:%.*]] = tail call noundef <16 x bfloat> @llvm.aie2.v16accfloat.to.v16bf16(<8 x i64> [[TMP3]])
// CHECK-NEXT:    [[TMP5:%.*]] = tail call noundef <32 x bfloat> @llvm.aie2.upd.bf512.bf256(<32 x bfloat> [[TMP0]], <16 x bfloat> [[TMP4]], i32 0)
// CHECK-NEXT:    [[TMP6:%.*]] = tail call noundef i32 @llvm.aie2.vltbf16(<32 x bfloat> [[TMP5]], <32 x bfloat> [[TMP0]])
// CHECK-NEXT:    [[TMP7:%.*]] = bitcast <16 x float> [[V1]] to <16 x i32>
// CHECK-NEXT:    [[TMP8:%.*]] = bitcast <16 x float> [[V2]] to <16 x i32>
// CHECK-NEXT:    [[TMP9:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vsel32(<16 x i32> [[TMP7]], <16 x i32> [[TMP8]], i32 [[TMP6]])
// CHECK-NEXT:    [[TMP10:%.*]] = bitcast <16 x i32> [[TMP9]] to <16 x float>
// CHECK-NEXT:    ret <16 x float> [[TMP10]]
//
v16float test_max(v16float v1, v16float v2) {
  return max(v1, v2);
}

// CHECK-LABEL: @_Z7test_geDv16_fS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef <32 x bfloat> @llvm.aie2.vbroadcast16.bf512(bfloat 0xR0000)
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <16 x float> [[V1:%.*]] to <8 x i64>
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <16 x float> [[V2:%.*]] to <8 x i64>
// CHECK-NEXT:    [[TMP3:%.*]] = tail call noundef <8 x i64> @llvm.aie2.sub.accfloat(<8 x i64> [[TMP1]], <8 x i64> [[TMP2]], i32 28)
// CHECK-NEXT:    [[TMP4:%.*]] = tail call noundef <16 x bfloat> @llvm.aie2.v16accfloat.to.v16bf16(<8 x i64> [[TMP3]])
// CHECK-NEXT:    [[TMP5:%.*]] = tail call noundef <32 x bfloat> @llvm.aie2.upd.bf512.bf256(<32 x bfloat> [[TMP0]], <16 x bfloat> [[TMP4]], i32 0)
// CHECK-NEXT:    [[TMP6:%.*]] = tail call noundef i32 @llvm.aie2.vgebf16(<32 x bfloat> [[TMP5]], <32 x bfloat> [[TMP0]])
// CHECK-NEXT:    [[AND_I:%.*]] = and i32 [[TMP6]], 65535
// CHECK-NEXT:    ret i32 [[AND_I]]
//
unsigned int test_ge(v16float v1, v16float v2) {
  return ge(v1, v2);
}

// CHECK-LABEL: @_Z11test_min_geDv16_fS_Rj(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef <32 x bfloat> @llvm.aie2.vbroadcast16.bf512(bfloat 0xR0000)
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <16 x float> [[V1:%.*]] to <8 x i64>
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <16 x float> [[V2:%.*]] to <8 x i64>
// CHECK-NEXT:    [[TMP3:%.*]] = tail call noundef <8 x i64> @llvm.aie2.sub.accfloat(<8 x i64> [[TMP1]], <8 x i64> [[TMP2]], i32 28)
// CHECK-NEXT:    [[TMP4:%.*]] = tail call noundef <16 x bfloat> @llvm.aie2.v16accfloat.to.v16bf16(<8 x i64> [[TMP3]])
// CHECK-NEXT:    [[TMP5:%.*]] = tail call noundef <32 x bfloat> @llvm.aie2.upd.bf512.bf256(<32 x bfloat> [[TMP0]], <16 x bfloat> [[TMP4]], i32 0)
// CHECK-NEXT:    [[TMP6:%.*]] = tail call noundef i32 @llvm.aie2.vgebf16(<32 x bfloat> [[TMP5]], <32 x bfloat> [[TMP0]])
// CHECK-NEXT:    [[AND_I_I:%.*]] = and i32 [[TMP6]], 65535
// CHECK-NEXT:    store i32 [[AND_I_I]], ptr [[CMP:%.*]], align 4, !tbaa [[TBAA6]]
// CHECK-NEXT:    [[TMP7:%.*]] = bitcast <16 x float> [[V1]] to <16 x i32>
// CHECK-NEXT:    [[TMP8:%.*]] = bitcast <16 x float> [[V2]] to <16 x i32>
// CHECK-NEXT:    [[TMP9:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vsel32(<16 x i32> [[TMP7]], <16 x i32> [[TMP8]], i32 [[AND_I_I]])
// CHECK-NEXT:    [[TMP10:%.*]] = bitcast <16 x i32> [[TMP9]] to <16 x float>
// CHECK-NEXT:    ret <16 x float> [[TMP10]]
//
v16float test_min_ge(v16float v1, v16float v2, unsigned int &cmp) {
  return min_ge(v1, v2, cmp);
}

// CHECK-LABEL: @_Z8test_minDv16_fS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef <32 x bfloat> @llvm.aie2.vbroadcast16.bf512(bfloat 0xR0000)
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <16 x float> [[V1:%.*]] to <8 x i64>
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <16 x float> [[V2:%.*]] to <8 x i64>
// CHECK-NEXT:    [[TMP3:%.*]] = tail call noundef <8 x i64> @llvm.aie2.sub.accfloat(<8 x i64> [[TMP1]], <8 x i64> [[TMP2]], i32 28)
// CHECK-NEXT:    [[TMP4:%.*]] = tail call noundef <16 x bfloat> @llvm.aie2.v16accfloat.to.v16bf16(<8 x i64> [[TMP3]])
// CHECK-NEXT:    [[TMP5:%.*]] = tail call noundef <32 x bfloat> @llvm.aie2.upd.bf512.bf256(<32 x bfloat> [[TMP0]], <16 x bfloat> [[TMP4]], i32 0)
// CHECK-NEXT:    [[TMP6:%.*]] = tail call noundef i32 @llvm.aie2.vgebf16(<32 x bfloat> [[TMP5]], <32 x bfloat> [[TMP0]])
// CHECK-NEXT:    [[AND_I_I:%.*]] = and i32 [[TMP6]], 65535
// CHECK-NEXT:    [[TMP7:%.*]] = bitcast <16 x float> [[V1]] to <16 x i32>
// CHECK-NEXT:    [[TMP8:%.*]] = bitcast <16 x float> [[V2]] to <16 x i32>
// CHECK-NEXT:    [[TMP9:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vsel32(<16 x i32> [[TMP7]], <16 x i32> [[TMP8]], i32 [[AND_I_I]])
// CHECK-NEXT:    [[TMP10:%.*]] = bitcast <16 x i32> [[TMP9]] to <16 x float>
// CHECK-NEXT:    ret <16 x float> [[TMP10]]
//
v16float test_min(v16float v1, v16float v2) {
  return min(v1,v2);
}

// CHECK-LABEL: @_Z7test_gtDv16_fS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef <32 x bfloat> @llvm.aie2.vbroadcast16.bf512(bfloat 0xR0000)
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <16 x float> [[V1:%.*]] to <8 x i64>
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <16 x float> [[V2:%.*]] to <8 x i64>
// CHECK-NEXT:    [[TMP3:%.*]] = tail call noundef <8 x i64> @llvm.aie2.sub.accfloat(<8 x i64> [[TMP1]], <8 x i64> [[TMP2]], i32 28)
// CHECK-NEXT:    [[TMP4:%.*]] = tail call noundef <16 x bfloat> @llvm.aie2.v16accfloat.to.v16bf16(<8 x i64> [[TMP3]])
// CHECK-NEXT:    [[TMP5:%.*]] = tail call noundef <32 x bfloat> @llvm.aie2.upd.bf512.bf256(<32 x bfloat> [[TMP0]], <16 x bfloat> [[TMP4]], i32 0)
// CHECK-NEXT:    [[TMP6:%.*]] = tail call noundef i32 @llvm.aie2.vltbf16(<32 x bfloat> [[TMP0]], <32 x bfloat> [[TMP5]])
// CHECK-NEXT:    ret i32 [[TMP6]]
//
unsigned int test_gt(v16float v1, v16float v2) {
  return gt(v1, v2);
}

// CHECK-LABEL: @_Z8test_absDv16_f(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <16 x float> [[V1:%.*]] to <16 x i32>
// CHECK-NEXT:    [[TMP1:%.*]] = tail call { <16 x i32>, i32 } @llvm.aie2.vabs.gtz32(<16 x i32> [[TMP0]], i32 1)
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP1]], 0
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <16 x i32> [[TMP2]] to <16 x float>
// CHECK-NEXT:    ret <16 x float> [[TMP3]]
//
v16float test_abs(v16float v1) { return abs(v1); }

// CHECK-LABEL: @_Z12test_min_absDv16_fS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <16 x float> [[V2:%.*]] to <16 x i32>
// CHECK-NEXT:    [[TMP1:%.*]] = tail call { <16 x i32>, i32 } @llvm.aie2.vabs.gtz32(<16 x i32> [[TMP0]], i32 1)
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP1]], 0
// CHECK-NEXT:    [[TMP3:%.*]] = tail call noundef <32 x bfloat> @llvm.aie2.vbroadcast16.bf512(bfloat 0xR0000)
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <16 x float> [[V1:%.*]] to <8 x i64>
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <16 x i32> [[TMP2]] to <8 x i64>
// CHECK-NEXT:    [[TMP6:%.*]] = tail call noundef <8 x i64> @llvm.aie2.sub.accfloat(<8 x i64> [[TMP4]], <8 x i64> [[TMP5]], i32 28)
// CHECK-NEXT:    [[TMP7:%.*]] = tail call noundef <16 x bfloat> @llvm.aie2.v16accfloat.to.v16bf16(<8 x i64> [[TMP6]])
// CHECK-NEXT:    [[TMP8:%.*]] = tail call noundef <32 x bfloat> @llvm.aie2.upd.bf512.bf256(<32 x bfloat> [[TMP3]], <16 x bfloat> [[TMP7]], i32 0)
// CHECK-NEXT:    [[TMP9:%.*]] = tail call noundef i32 @llvm.aie2.vgebf16(<32 x bfloat> [[TMP8]], <32 x bfloat> [[TMP3]])
// CHECK-NEXT:    [[AND_I_I:%.*]] = and i32 [[TMP9]], 65535
// CHECK-NEXT:    [[TMP10:%.*]] = bitcast <16 x float> [[V1]] to <16 x i32>
// CHECK-NEXT:    [[TMP11:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vsel32(<16 x i32> [[TMP10]], <16 x i32> [[TMP0]], i32 [[AND_I_I]])
// CHECK-NEXT:    [[TMP12:%.*]] = bitcast <16 x i32> [[TMP11]] to <16 x float>
// CHECK-NEXT:    ret <16 x float> [[TMP12]]
//
v16float test_min_abs(v16float v1, v16float v2) {
  return min_abs(v1, v2);
}

// CHECK-LABEL: @_Z12test_max_absDv16_fS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <16 x float> [[V2:%.*]] to <16 x i32>
// CHECK-NEXT:    [[TMP1:%.*]] = tail call { <16 x i32>, i32 } @llvm.aie2.vabs.gtz32(<16 x i32> [[TMP0]], i32 1)
// CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP1]], 0
// CHECK-NEXT:    [[TMP3:%.*]] = tail call noundef <32 x bfloat> @llvm.aie2.vbroadcast16.bf512(bfloat 0xR0000)
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <16 x float> [[V1:%.*]] to <8 x i64>
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <16 x i32> [[TMP2]] to <8 x i64>
// CHECK-NEXT:    [[TMP6:%.*]] = tail call noundef <8 x i64> @llvm.aie2.sub.accfloat(<8 x i64> [[TMP4]], <8 x i64> [[TMP5]], i32 28)
// CHECK-NEXT:    [[TMP7:%.*]] = tail call noundef <16 x bfloat> @llvm.aie2.v16accfloat.to.v16bf16(<8 x i64> [[TMP6]])
// CHECK-NEXT:    [[TMP8:%.*]] = tail call noundef <32 x bfloat> @llvm.aie2.upd.bf512.bf256(<32 x bfloat> [[TMP3]], <16 x bfloat> [[TMP7]], i32 0)
// CHECK-NEXT:    [[TMP9:%.*]] = tail call noundef i32 @llvm.aie2.vltbf16(<32 x bfloat> [[TMP8]], <32 x bfloat> [[TMP3]])
// CHECK-NEXT:    [[TMP10:%.*]] = bitcast <16 x float> [[V1]] to <16 x i32>
// CHECK-NEXT:    [[TMP11:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vsel32(<16 x i32> [[TMP10]], <16 x i32> [[TMP0]], i32 [[TMP9]])
// CHECK-NEXT:    [[TMP12:%.*]] = bitcast <16 x i32> [[TMP11]] to <16 x float>
// CHECK-NEXT:    ret <16 x float> [[TMP12]]
//
v16float test_max_abs(v16float v1, v16float v2) {
  return max_abs(v1, v2);
}

// CHECK-LABEL: @_Z11test_ge_absDv16_fS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef <32 x bfloat> @llvm.aie2.vbroadcast16.bf512(bfloat 0xR0000)
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <16 x float> [[V2:%.*]] to <16 x i32>
// CHECK-NEXT:    [[TMP2:%.*]] = tail call { <16 x i32>, i32 } @llvm.aie2.vabs.gtz32(<16 x i32> [[TMP1]], i32 1)
// CHECK-NEXT:    [[TMP3:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP2]], 0
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <16 x float> [[V1:%.*]] to <8 x i64>
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <16 x i32> [[TMP3]] to <8 x i64>
// CHECK-NEXT:    [[TMP6:%.*]] = tail call noundef <8 x i64> @llvm.aie2.sub.accfloat(<8 x i64> [[TMP4]], <8 x i64> [[TMP5]], i32 28)
// CHECK-NEXT:    [[TMP7:%.*]] = tail call noundef <16 x bfloat> @llvm.aie2.v16accfloat.to.v16bf16(<8 x i64> [[TMP6]])
// CHECK-NEXT:    [[TMP8:%.*]] = tail call noundef <32 x bfloat> @llvm.aie2.upd.bf512.bf256(<32 x bfloat> [[TMP0]], <16 x bfloat> [[TMP7]], i32 0)
// CHECK-NEXT:    [[TMP9:%.*]] = tail call noundef i32 @llvm.aie2.vgebf16(<32 x bfloat> [[TMP8]], <32 x bfloat> [[TMP0]])
// CHECK-NEXT:    [[AND_I:%.*]] = and i32 [[TMP9]], 65535
// CHECK-NEXT:    ret i32 [[AND_I]]
//
unsigned int test_ge_abs(v16float v1, v16float v2) {
  return ge_abs(v1, v2);
}

// CHECK-LABEL: @_Z11test_lt_absDv16_fS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef <32 x bfloat> @llvm.aie2.vbroadcast16.bf512(bfloat 0xR0000)
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <16 x float> [[V2:%.*]] to <16 x i32>
// CHECK-NEXT:    [[TMP2:%.*]] = tail call { <16 x i32>, i32 } @llvm.aie2.vabs.gtz32(<16 x i32> [[TMP1]], i32 1)
// CHECK-NEXT:    [[TMP3:%.*]] = extractvalue { <16 x i32>, i32 } [[TMP2]], 0
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <16 x float> [[V1:%.*]] to <8 x i64>
// CHECK-NEXT:    [[TMP5:%.*]] = bitcast <16 x i32> [[TMP3]] to <8 x i64>
// CHECK-NEXT:    [[TMP6:%.*]] = tail call noundef <8 x i64> @llvm.aie2.sub.accfloat(<8 x i64> [[TMP4]], <8 x i64> [[TMP5]], i32 28)
// CHECK-NEXT:    [[TMP7:%.*]] = tail call noundef <16 x bfloat> @llvm.aie2.v16accfloat.to.v16bf16(<8 x i64> [[TMP6]])
// CHECK-NEXT:    [[TMP8:%.*]] = tail call noundef <32 x bfloat> @llvm.aie2.upd.bf512.bf256(<32 x bfloat> [[TMP0]], <16 x bfloat> [[TMP7]], i32 0)
// CHECK-NEXT:    [[TMP9:%.*]] = tail call noundef i32 @llvm.aie2.vltbf16(<32 x bfloat> [[TMP8]], <32 x bfloat> [[TMP0]])
// CHECK-NEXT:    ret i32 [[TMP9]]
//
unsigned int test_lt_abs(v16float v1, v16float v2) {
  return lt_abs(v1, v2);
}
