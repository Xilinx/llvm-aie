// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py
//===- aie2-scl2vec-intrinsic.cpp -------------------------------*- C++ -*-===//
//
// This file is licensed under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
//
// (c) Copyright 2023-2024 Advanced Micro Devices, Inc. or its affiliates
//
//===----------------------------------------------------------------------===//
// RUN: %clang -O2 %s --target=aie2 -S -emit-llvm -o - | FileCheck %s


// CHECK-LABEL: @_Z11test_shiftxDv16_iS_ii(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vshift.I512.I512(<16 x i32> [[A:%.*]], <16 x i32> [[B:%.*]], i32 [[STEP:%.*]], i32 [[SHIFT:%.*]])
// CHECK-NEXT:    ret <16 x i32> [[TMP0]]
//
v16int32 test_shiftx(v16int32 a, v16int32 b, int step, int shift) {
  return shiftx(a,b,step,shift);
}

// CHECK-LABEL: @_Z11test_shiftxDv32_tS_ii(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <32 x i16> [[A:%.*]] to <16 x i32>
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <32 x i16> [[B:%.*]] to <16 x i32>
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <16 x i32> @llvm.aie2.vshift.I512.I512(<16 x i32> [[TMP0]], <16 x i32> [[TMP1]], i32 [[STEP:%.*]], i32 [[SHIFT:%.*]])
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <16 x i32> [[TMP2]] to <32 x i16>
// CHECK-NEXT:    ret <32 x i16> [[TMP3]]
//
v32uint16 test_shiftx(v32uint16 a, v32uint16 b, int step, int shift) {
  return shiftx(a,b,step,shift);
}

// CHECK-LABEL: @_Z16test_shift_bytesDv64_aS_i(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <64 x i8> [[A:%.*]] to <16 x i32>
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <64 x i8> [[B:%.*]] to <16 x i32>
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <16 x i32> @llvm.aie2.vshift.I512.I512(<16 x i32> [[TMP0]], <16 x i32> [[TMP1]], i32 0, i32 [[SHIFT:%.*]])
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <16 x i32> [[TMP2]] to <64 x i8>
// CHECK-NEXT:    ret <64 x i8> [[TMP3]]
//
v64int8 test_shift_bytes(v64int8 a, v64int8 b, int shift) {
  return shift_bytes(a,b,shift);
}

// CHECK-LABEL: @_Z10test_shiftDv64_hS_i(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <64 x i8> [[A:%.*]] to <16 x i32>
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <64 x i8> [[B:%.*]] to <16 x i32>
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <16 x i32> @llvm.aie2.vshift.I512.I512(<16 x i32> [[TMP0]], <16 x i32> [[TMP1]], i32 0, i32 [[SHIFT_BY:%.*]])
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <16 x i32> [[TMP2]] to <64 x i8>
// CHECK-NEXT:    ret <64 x i8> [[TMP3]]
//
v64uint8 test_shift(v64uint8 a, v64uint8 b, int shift_by) {
   return shift(a,b,shift_by);
}

// CHECK-LABEL: @_Z13test_upd_elemDv64_aic(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <64 x i8> [[V:%.*]] to <16 x i32>
// CHECK-NEXT:    [[CONV_I:%.*]] = sext i8 [[B:%.*]] to i32
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <16 x i32> @llvm.aie2.vinsert8.I512(<16 x i32> [[TMP0]], i32 [[IDX:%.*]], i32 [[CONV_I]])
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <16 x i32> [[TMP1]] to <64 x i8>
// CHECK-NEXT:    ret <64 x i8> [[TMP2]]
//
v64int8 test_upd_elem(v64int8 v, int idx, char b) {
  return upd_elem(v, idx, b);
}
// CHECK-LABEL: @_Z13test_upd_elemDv32_sis(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <32 x i16> [[V:%.*]] to <16 x i32>
// CHECK-NEXT:    [[CONV_I:%.*]] = sext i16 [[B:%.*]] to i32
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <16 x i32> @llvm.aie2.vinsert16.I512(<16 x i32> [[TMP0]], i32 [[IDX:%.*]], i32 [[CONV_I]])
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <16 x i32> [[TMP1]] to <32 x i16>
// CHECK-NEXT:    ret <32 x i16> [[TMP2]]
//
v32int16 test_upd_elem(v32int16 v, int idx, short b) {
  return upd_elem(v, idx, b);
}
// CHECK-LABEL: @_Z13test_upd_elemDv16_iii(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vinsert32.I512(<16 x i32> [[V:%.*]], i32 [[IDX:%.*]], i32 [[B:%.*]])
// CHECK-NEXT:    ret <16 x i32> [[TMP0]]
//
v16int32 test_upd_elem(v16int32 v, int idx, int b) {
  return upd_elem(v, idx, b);
}
// CHECK-LABEL: @_Z13test_upd_elemDv64_hih(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <64 x i8> [[V:%.*]] to <16 x i32>
// CHECK-NEXT:    [[CONV_I:%.*]] = zext i8 [[B:%.*]] to i32
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <16 x i32> @llvm.aie2.vinsert8.I512(<16 x i32> [[TMP0]], i32 [[IDX:%.*]], i32 [[CONV_I]])
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <16 x i32> [[TMP1]] to <64 x i8>
// CHECK-NEXT:    ret <64 x i8> [[TMP2]]
//
v64uint8 test_upd_elem(v64uint8 v, int idx, unsigned char b) {
  return upd_elem(v, idx, b);
}
// CHECK-LABEL: @_Z13test_upd_elemDv32_tit(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <32 x i16> [[V:%.*]] to <16 x i32>
// CHECK-NEXT:    [[CONV_I:%.*]] = zext i16 [[B:%.*]] to i32
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <16 x i32> @llvm.aie2.vinsert16.I512(<16 x i32> [[TMP0]], i32 [[IDX:%.*]], i32 [[CONV_I]])
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <16 x i32> [[TMP1]] to <32 x i16>
// CHECK-NEXT:    ret <32 x i16> [[TMP2]]
//
v32uint16 test_upd_elem(v32uint16 v, int idx, unsigned short b) {
  return upd_elem(v, idx, b);
}
// CHECK-LABEL: @_Z13test_upd_elemDv16_jij(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vinsert32.I512(<16 x i32> [[V:%.*]], i32 [[IDX:%.*]], i32 [[B:%.*]])
// CHECK-NEXT:    ret <16 x i32> [[TMP0]]
//
v16uint32 test_upd_elem(v16uint32 v, int idx, unsigned int b) {
  return upd_elem(v, idx, b);
}

// CHECK-LABEL: @_Z11test_insertDv64_DB8_iS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <64 x i8> [[V:%.*]] to <16 x i32>
// CHECK-NEXT:    [[CONV_I:%.*]] = sext i8 [[B:%.*]] to i32
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <16 x i32> @llvm.aie2.vinsert8.I512(<16 x i32> [[TMP0]], i32 [[IDX:%.*]], i32 [[CONV_I]])
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <16 x i32> [[TMP1]] to <64 x i8>
// CHECK-NEXT:    ret <64 x i8> [[TMP2]]
//
v128int4 test_insert(v128int4 v, int idx, v2int4 b) {
  return insert(v, idx, b);
}
// CHECK-LABEL: @_Z11test_insertDv64_DB8_iDv2_S_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <64 x i8> [[V:%.*]] to <16 x i32>
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <2 x i8> [[B:%.*]] to i16
// CHECK-NEXT:    [[CONV_I:%.*]] = sext i16 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <16 x i32> @llvm.aie2.vinsert16.I512(<16 x i32> [[TMP0]], i32 [[IDX:%.*]], i32 [[CONV_I]])
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <16 x i32> [[TMP2]] to <64 x i8>
// CHECK-NEXT:    ret <64 x i8> [[TMP3]]
//
v128int4 test_insert(v128int4 v, int idx, v4int4 b) {
  return insert(v, idx, b);
}
// CHECK-LABEL: @_Z11test_insertDv64_DB8_iDv4_S_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <64 x i8> [[V:%.*]] to <16 x i32>
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <4 x i8> [[B:%.*]] to i32
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <16 x i32> @llvm.aie2.vinsert32.I512(<16 x i32> [[TMP0]], i32 [[IDX:%.*]], i32 [[TMP1]])
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <16 x i32> [[TMP2]] to <64 x i8>
// CHECK-NEXT:    ret <64 x i8> [[TMP3]]
//
v128int4 test_insert(v128int4 v, int idx, v8int4 b) {
  return insert(v, idx, b);
}
// CHECK-LABEL: @_Z11test_insertDv64_DB8_iDv8_S_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <64 x i8> [[V:%.*]] to <16 x i32>
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <8 x i8> [[B:%.*]] to <2 x i32>
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <16 x i32> @llvm.aie2.vinsert64.I512(<16 x i32> [[TMP0]], i32 [[IDX:%.*]], <2 x i32> [[TMP1]])
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <16 x i32> [[TMP2]] to <64 x i8>
// CHECK-NEXT:    ret <64 x i8> [[TMP3]]
//
v128int4 test_insert(v128int4 v, int idx, v16int4 b) {
  return insert(v, idx, b);
}

// CHECK-LABEL: @_Z11test_insertDv64_aic(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <64 x i8> [[V:%.*]] to <16 x i32>
// CHECK-NEXT:    [[CONV_I:%.*]] = sext i8 [[B:%.*]] to i32
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <16 x i32> @llvm.aie2.vinsert8.I512(<16 x i32> [[TMP0]], i32 [[IDX:%.*]], i32 [[CONV_I]])
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <16 x i32> [[TMP1]] to <64 x i8>
// CHECK-NEXT:    ret <64 x i8> [[TMP2]]
//
v64int8 test_insert(v64int8 v, int idx, char b) {
  return insert(v, idx, b);
}
// CHECK-LABEL: @_Z11test_insertDv64_aiDv2_a(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <64 x i8> [[V:%.*]] to <16 x i32>
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <2 x i8> [[B:%.*]] to i16
// CHECK-NEXT:    [[CONV_I:%.*]] = sext i16 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <16 x i32> @llvm.aie2.vinsert16.I512(<16 x i32> [[TMP0]], i32 [[IDX:%.*]], i32 [[CONV_I]])
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <16 x i32> [[TMP2]] to <64 x i8>
// CHECK-NEXT:    ret <64 x i8> [[TMP3]]
//
v64int8 test_insert(v64int8 v, int idx, v2int8 b) {
  return insert(v, idx, b);
}
// CHECK-LABEL: @_Z11test_insertDv64_aiDv4_a(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <64 x i8> [[V:%.*]] to <16 x i32>
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <4 x i8> [[B:%.*]] to i32
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <16 x i32> @llvm.aie2.vinsert32.I512(<16 x i32> [[TMP0]], i32 [[IDX:%.*]], i32 [[TMP1]])
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <16 x i32> [[TMP2]] to <64 x i8>
// CHECK-NEXT:    ret <64 x i8> [[TMP3]]
//
v64int8 test_insert(v64int8 v, int idx, v4int8 b) {
  return insert(v, idx, b);
}
// CHECK-LABEL: @_Z11test_insertDv64_aiDv8_a(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <64 x i8> [[V:%.*]] to <16 x i32>
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <8 x i8> [[B:%.*]] to <2 x i32>
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <16 x i32> @llvm.aie2.vinsert64.I512(<16 x i32> [[TMP0]], i32 [[IDX:%.*]], <2 x i32> [[TMP1]])
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <16 x i32> [[TMP2]] to <64 x i8>
// CHECK-NEXT:    ret <64 x i8> [[TMP3]]
//
v64int8 test_insert(v64int8 v, int idx, v8int8 b) {
  return insert(v, idx, b);
}

// CHECK-LABEL: @_Z11test_insertDv32_sis(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <32 x i16> [[V:%.*]] to <16 x i32>
// CHECK-NEXT:    [[CONV_I:%.*]] = sext i16 [[B:%.*]] to i32
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <16 x i32> @llvm.aie2.vinsert16.I512(<16 x i32> [[TMP0]], i32 [[IDX:%.*]], i32 [[CONV_I]])
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <16 x i32> [[TMP1]] to <32 x i16>
// CHECK-NEXT:    ret <32 x i16> [[TMP2]]
//
v32int16 test_insert(v32int16 v, int idx, short b) {
  return insert(v, idx, b);
}
//
// CHECK-LABEL: @_Z11test_insertDv32_siDv2_s(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <32 x i16> [[V:%.*]] to <16 x i32>
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <2 x i16> [[B:%.*]] to i32
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <16 x i32> @llvm.aie2.vinsert32.I512(<16 x i32> [[TMP0]], i32 [[IDX:%.*]], i32 [[TMP1]])
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <16 x i32> [[TMP2]] to <32 x i16>
// CHECK-NEXT:    ret <32 x i16> [[TMP3]]
//
v32int16 test_insert(v32int16 v, int idx, v2int16 b) {
  return insert(v, idx, b);
}
// CHECK-LABEL: @_Z11test_insertDv32_siDv4_s(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <32 x i16> [[V:%.*]] to <16 x i32>
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <4 x i16> [[B:%.*]] to <2 x i32>
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <16 x i32> @llvm.aie2.vinsert64.I512(<16 x i32> [[TMP0]], i32 [[IDX:%.*]], <2 x i32> [[TMP1]])
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <16 x i32> [[TMP2]] to <32 x i16>
// CHECK-NEXT:    ret <32 x i16> [[TMP3]]
//
v32int16 test_insert(v32int16 v, int idx, v4int16 b) {
  return insert(v, idx, b);
}

// CHECK-LABEL: @_Z11test_insertDv64_DU8_iS_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <64 x i8> [[V:%.*]] to <16 x i32>
// CHECK-NEXT:    [[CONV_I:%.*]] = zext i8 [[B:%.*]] to i32
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <16 x i32> @llvm.aie2.vinsert8.I512(<16 x i32> [[TMP0]], i32 [[IDX:%.*]], i32 [[CONV_I]])
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <16 x i32> [[TMP1]] to <64 x i8>
// CHECK-NEXT:    ret <64 x i8> [[TMP2]]
//
v128uint4 test_insert(v128uint4 v, int idx, v2uint4 b) {
  return insert(v, idx, b);
}
//
// CHECK-LABEL: @_Z11test_insertDv64_DU8_iDv2_S_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <64 x i8> [[V:%.*]] to <16 x i32>
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <2 x i8> [[B:%.*]] to i16
// CHECK-NEXT:    [[CONV_I:%.*]] = zext i16 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <16 x i32> @llvm.aie2.vinsert16.I512(<16 x i32> [[TMP0]], i32 [[IDX:%.*]], i32 [[CONV_I]])
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <16 x i32> [[TMP2]] to <64 x i8>
// CHECK-NEXT:    ret <64 x i8> [[TMP3]]
//
v128uint4 test_insert(v128uint4 v, int idx, v4uint4 b) {
  return insert(v, idx, b);
}
//
// CHECK-LABEL: @_Z11test_insertDv64_DU8_iDv4_S_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <64 x i8> [[V:%.*]] to <16 x i32>
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <4 x i8> [[B:%.*]] to i32
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <16 x i32> @llvm.aie2.vinsert32.I512(<16 x i32> [[TMP0]], i32 [[IDX:%.*]], i32 [[TMP1]])
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <16 x i32> [[TMP2]] to <64 x i8>
// CHECK-NEXT:    ret <64 x i8> [[TMP3]]
//
v128uint4 test_insert(v128uint4 v, int idx, v8uint4 b) {
  return insert(v, idx, b);
}
// CHECK-LABEL: @_Z11test_insertDv64_DU8_iDv8_S_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <64 x i8> [[V:%.*]] to <16 x i32>
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <8 x i8> [[B:%.*]] to <2 x i32>
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <16 x i32> @llvm.aie2.vinsert64.I512(<16 x i32> [[TMP0]], i32 [[IDX:%.*]], <2 x i32> [[TMP1]])
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <16 x i32> [[TMP2]] to <64 x i8>
// CHECK-NEXT:    ret <64 x i8> [[TMP3]]
//
v128uint4 test_insert(v128uint4 v, int idx, v16uint4 b) {
  return insert(v, idx, b);
}

// CHECK-LABEL: @_Z11test_insertDv64_hih(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <64 x i8> [[V:%.*]] to <16 x i32>
// CHECK-NEXT:    [[CONV_I:%.*]] = zext i8 [[B:%.*]] to i32
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <16 x i32> @llvm.aie2.vinsert8.I512(<16 x i32> [[TMP0]], i32 [[IDX:%.*]], i32 [[CONV_I]])
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <16 x i32> [[TMP1]] to <64 x i8>
// CHECK-NEXT:    ret <64 x i8> [[TMP2]]
//
v64uint8 test_insert(v64uint8 v, int idx, unsigned char b) {
  return insert(v, idx, b);
}
// CHECK-LABEL: @_Z11test_insertDv64_hiDv2_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <64 x i8> [[V:%.*]] to <16 x i32>
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <2 x i8> [[B:%.*]] to i16
// CHECK-NEXT:    [[CONV_I:%.*]] = zext i16 [[TMP1]] to i32
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <16 x i32> @llvm.aie2.vinsert16.I512(<16 x i32> [[TMP0]], i32 [[IDX:%.*]], i32 [[CONV_I]])
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <16 x i32> [[TMP2]] to <64 x i8>
// CHECK-NEXT:    ret <64 x i8> [[TMP3]]
//
v64uint8 test_insert(v64uint8 v, int idx, v2uint8 b) {
  return insert(v, idx, b);
}
// CHECK-LABEL: @_Z11test_insertDv64_hiDv4_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <64 x i8> [[V:%.*]] to <16 x i32>
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <4 x i8> [[B:%.*]] to i32
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <16 x i32> @llvm.aie2.vinsert32.I512(<16 x i32> [[TMP0]], i32 [[IDX:%.*]], i32 [[TMP1]])
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <16 x i32> [[TMP2]] to <64 x i8>
// CHECK-NEXT:    ret <64 x i8> [[TMP3]]
//
v64uint8 test_insert(v64uint8 v, int idx, v4uint8 b) {
  return insert(v, idx, b);
}
// CHECK-LABEL: @_Z11test_insertDv64_hiDv8_h(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <64 x i8> [[V:%.*]] to <16 x i32>
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <8 x i8> [[B:%.*]] to <2 x i32>
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <16 x i32> @llvm.aie2.vinsert64.I512(<16 x i32> [[TMP0]], i32 [[IDX:%.*]], <2 x i32> [[TMP1]])
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <16 x i32> [[TMP2]] to <64 x i8>
// CHECK-NEXT:    ret <64 x i8> [[TMP3]]
//
v64uint8 test_insert(v64uint8 v, int idx, v8uint8 b) {
  return insert(v, idx, b);
}

// CHECK-LABEL: @_Z11test_insertDv32_tit(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <32 x i16> [[V:%.*]] to <16 x i32>
// CHECK-NEXT:    [[CONV_I:%.*]] = zext i16 [[B:%.*]] to i32
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <16 x i32> @llvm.aie2.vinsert16.I512(<16 x i32> [[TMP0]], i32 [[IDX:%.*]], i32 [[CONV_I]])
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <16 x i32> [[TMP1]] to <32 x i16>
// CHECK-NEXT:    ret <32 x i16> [[TMP2]]
//
v32uint16 test_insert(v32uint16 v, int idx, unsigned short b) {
  return insert(v, idx, b);
}
// CHECK-LABEL: @_Z11test_insertDv32_tiDv2_t(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <32 x i16> [[V:%.*]] to <16 x i32>
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <2 x i16> [[B:%.*]] to i32
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <16 x i32> @llvm.aie2.vinsert32.I512(<16 x i32> [[TMP0]], i32 [[IDX:%.*]], i32 [[TMP1]])
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <16 x i32> [[TMP2]] to <32 x i16>
// CHECK-NEXT:    ret <32 x i16> [[TMP3]]
//
v32uint16 test_insert(v32uint16 v, int idx, v2uint16 b) {
  return insert(v, idx, b);
}
// CHECK-LABEL: @_Z11test_insertDv32_tiDv4_t(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <32 x i16> [[V:%.*]] to <16 x i32>
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <4 x i16> [[B:%.*]] to <2 x i32>
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <16 x i32> @llvm.aie2.vinsert64.I512(<16 x i32> [[TMP0]], i32 [[IDX:%.*]], <2 x i32> [[TMP1]])
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <16 x i32> [[TMP2]] to <32 x i16>
// CHECK-NEXT:    ret <32 x i16> [[TMP3]]
//
v32uint16 test_insert(v32uint16 v, int idx, v4uint16 b) {
  return insert(v, idx, b);
}


// CHECK-LABEL: @_Z11test_insertDv16_iiy(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast i64 [[B:%.*]] to <2 x i32>
// CHECK-NEXT:    [[TMP1:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vinsert64.I512(<16 x i32> [[V:%.*]], i32 [[IDX:%.*]], <2 x i32> [[TMP0]])
// CHECK-NEXT:    ret <16 x i32> [[TMP1]]
//
v16int32 test_insert(v16int32 v, int idx, unsigned long long b) {
  return insert(v, idx, b);
}


// CHECK-LABEL: @_Z11test_insertDv64_DU8_iy(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <64 x i8> [[V:%.*]] to <16 x i32>
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast i64 [[B:%.*]] to <2 x i32>
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <16 x i32> @llvm.aie2.vinsert64.I512(<16 x i32> [[TMP0]], i32 [[IDX:%.*]], <2 x i32> [[TMP1]])
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <16 x i32> [[TMP2]] to <64 x i8>
// CHECK-NEXT:    ret <64 x i8> [[TMP3]]
//
v128uint4 test_insert(v128uint4 v, int idx, unsigned long long b) {
  return insert(v, idx, b);
}


// CHECK-LABEL: @_Z11test_insertDv64_hiy(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <64 x i8> [[V:%.*]] to <16 x i32>
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast i64 [[B:%.*]] to <2 x i32>
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <16 x i32> @llvm.aie2.vinsert64.I512(<16 x i32> [[TMP0]], i32 [[IDX:%.*]], <2 x i32> [[TMP1]])
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <16 x i32> [[TMP2]] to <64 x i8>
// CHECK-NEXT:    ret <64 x i8> [[TMP3]]
//
v64uint8 test_insert(v64uint8 v, int idx, unsigned long long b) {
  return insert(v, idx, b);
}


// CHECK-LABEL: @_Z11test_insertDv32_tiy(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <32 x i16> [[V:%.*]] to <16 x i32>
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast i64 [[B:%.*]] to <2 x i32>
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <16 x i32> @llvm.aie2.vinsert64.I512(<16 x i32> [[TMP0]], i32 [[IDX:%.*]], <2 x i32> [[TMP1]])
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <16 x i32> [[TMP2]] to <32 x i16>
// CHECK-NEXT:    ret <32 x i16> [[TMP3]]
//
v32uint16 test_insert(v32uint16 v, int idx, unsigned long long b) {
  return insert(v, idx, b);
}

// CHECK-LABEL: @_Z18test_broadcast_s16s(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I:%.*]] = sext i16 [[B:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef <32 x i16> @llvm.aie2.vbroadcast16.I512(i32 [[CONV_I]])
// CHECK-NEXT:    ret <32 x i16> [[TMP0]]
//
v32int16 test_broadcast_s16 (short b) {
   return  broadcast_s16(b);
}

// CHECK-LABEL: @_Z18test_broadcast_s32i(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vbroadcast32.I512(i32 [[B:%.*]])
// CHECK-NEXT:    ret <16 x i32> [[TMP0]]
//
v16int32 test_broadcast_s32 (int b) {
   return  broadcast_s32(b);
}

// CHECK-LABEL: @_Z18test_broadcast_s64x(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast i64 [[B:%.*]] to <2 x i32>
// CHECK-NEXT:    [[TMP1:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vbroadcast64.I512(<2 x i32> [[TMP0]])
// CHECK-NEXT:    ret <16 x i32> [[TMP1]]
//
v16int32 test_broadcast_s64 (long long b) {
   return  broadcast_s64(b);
}

// CHECK-LABEL: @_Z20test_broadcast_v2s32Dv2_i(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vbroadcast64.I512(<2 x i32> [[B:%.*]])
// CHECK-NEXT:    ret <16 x i32> [[TMP0]]
//
v16int32 test_broadcast_v2s32 (v2int32 b) {
   return  broadcast_v2s32(b);
}

// CHECK-LABEL: @_Z26test_broadcast_to_v128int4DB8_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I:%.*]] = sext i8 [[B:%.*]] to i32
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef <64 x i8> @llvm.aie2.vbroadcast8.I512(i32 [[CONV_I]])
// CHECK-NEXT:    ret <64 x i8> [[TMP0]]
//
v128int4 test_broadcast_to_v128int4 (v2int4 b) {
   return  broadcast_to_v128int4(b);
}

// CHECK-LABEL: @_Z25test_broadcast_to_v64int8Dv2_a(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <2 x i8> [[B:%.*]] to i16
// CHECK-NEXT:    [[CONV_I:%.*]] = sext i16 [[TMP0]] to i32
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <32 x i16> @llvm.aie2.vbroadcast16.I512(i32 [[CONV_I]])
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <32 x i16> [[TMP1]] to <64 x i8>
// CHECK-NEXT:    ret <64 x i8> [[TMP2]]
//
v64int8 test_broadcast_to_v64int8 (v2int8 b) {
   return  broadcast_to_v64int8(b);
}

// CHECK-LABEL: @_Z26test_broadcast_to_v128int4Dv8_DB8_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <8 x i8> [[B:%.*]] to <2 x i32>
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <16 x i32> @llvm.aie2.vbroadcast64.I512(<2 x i32> [[TMP0]])
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <16 x i32> [[TMP1]] to <64 x i8>
// CHECK-NEXT:    ret <64 x i8> [[TMP2]]
//
v128int4 test_broadcast_to_v128int4 (v16int4 b) {
   return  broadcast_to_v128int4(b);
}

// CHECK-LABEL: @_Z25test_broadcast_to_v64int8Dv8_a(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <8 x i8> [[B:%.*]] to <2 x i32>
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <16 x i32> @llvm.aie2.vbroadcast64.I512(<2 x i32> [[TMP0]])
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <16 x i32> [[TMP1]] to <64 x i8>
// CHECK-NEXT:    ret <64 x i8> [[TMP2]]
//
v64int8 test_broadcast_to_v64int8 (v8int8 b) {
   return  broadcast_to_v64int8(b);
}

// CHECK-LABEL: @_Z26test_broadcast_to_v16int32i(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vbroadcast32.I512(i32 [[B:%.*]])
// CHECK-NEXT:    ret <16 x i32> [[TMP0]]
//
v16int32 test_broadcast_to_v16int32 (int b) {
   return  broadcast_to_v16int32(b);
}

// CHECK-LABEL: @_Z26test_broadcast_to_v16int32Dv2_i(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vbroadcast64.I512(<2 x i32> [[B:%.*]])
// CHECK-NEXT:    ret <16 x i32> [[TMP0]]
//
v16int32 test_broadcast_to_v16int32 (v2int32 b) {
   return  broadcast_to_v16int32(b);
}

// CHECK-LABEL: @_Z27test_broadcast_to_v16uint32y(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast i64 [[B:%.*]] to <2 x i32>
// CHECK-NEXT:    [[TMP1:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vbroadcast64.I512(<2 x i32> [[TMP0]])
// CHECK-NEXT:    ret <16 x i32> [[TMP1]]
//
v16uint32 test_broadcast_to_v16uint32(unsigned long long b) {
  return broadcast_to_v16uint32(b);
}

// CHECK-LABEL: @_Z20test_broadcast_floatf(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast float [[B:%.*]] to i32
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <16 x i32> @llvm.aie2.vbroadcast32.I512(i32 [[TMP0]])
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <16 x i32> [[TMP1]] to <16 x float>
// CHECK-NEXT:    ret <16 x float> [[TMP2]]
//
v16float test_broadcast_float(float b) {
  return broadcast_float(b);
}

// CHECK-LABEL: @_Z16test_shiftl_elemDv64_ai(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[SEXT:%.*]] = shl i32 [[S:%.*]], 24
// CHECK-NEXT:    [[CONV_I1:%.*]] = ashr exact i32 [[SEXT]], 24
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef <64 x i8> @llvm.aie2.vbroadcast8.I512(i32 [[CONV_I1]])
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <64 x i8> [[V:%.*]] to <16 x i32>
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <64 x i8> [[TMP0]] to <16 x i32>
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <16 x i32> @llvm.aie2.vshift.I512.I512(<16 x i32> [[TMP1]], <16 x i32> [[TMP2]], i32 0, i32 1)
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <16 x i32> [[TMP3]] to <64 x i8>
// CHECK-NEXT:    ret <64 x i8> [[TMP4]]
//
v64int8 test_shiftl_elem(v64int8 v, int s) {
  return shiftl_elem(v, s);
}

// CHECK-LABEL: @_Z16test_shiftl_elemDv16_ii(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vbroadcast32.I512(i32 [[S:%.*]])
// CHECK-NEXT:    [[TMP1:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vshift.I512.I512(<16 x i32> [[V:%.*]], <16 x i32> [[TMP0]], i32 0, i32 4)
// CHECK-NEXT:    ret <16 x i32> [[TMP1]]
//
v16int32 test_shiftl_elem(v16int32 v, int s) {
  return shiftl_elem(v, s);
}

// CHECK-LABEL: @_Z16test_shiftl_elemDv32_tj(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I1:%.*]] = and i32 [[S:%.*]], 65535
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef <32 x i16> @llvm.aie2.vbroadcast16.I512(i32 [[CONV_I1]])
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <32 x i16> [[V:%.*]] to <16 x i32>
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <32 x i16> [[TMP0]] to <16 x i32>
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <16 x i32> @llvm.aie2.vshift.I512.I512(<16 x i32> [[TMP1]], <16 x i32> [[TMP2]], i32 0, i32 2)
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <16 x i32> [[TMP3]] to <32 x i16>
// CHECK-NEXT:    ret <32 x i16> [[TMP4]]
//
v32uint16 test_shiftl_elem(v32uint16 v, unsigned int s) {
  return shiftl_elem(v, s);
}

// CHECK-LABEL: @_Z16test_shiftr_elemDv32_si(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[SEXT:%.*]] = shl i32 [[S:%.*]], 16
// CHECK-NEXT:    [[CONV_I_I:%.*]] = ashr exact i32 [[SEXT]], 16
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef <32 x i16> @llvm.aie2.vbroadcast16.I512(i32 [[CONV_I_I]])
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <32 x i16> [[TMP0]] to <16 x i32>
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <32 x i16> [[V:%.*]] to <16 x i32>
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <16 x i32> @llvm.aie2.vshift.I512.I512(<16 x i32> [[TMP1]], <16 x i32> [[TMP2]], i32 0, i32 62)
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <16 x i32> [[TMP3]] to <32 x i16>
// CHECK-NEXT:    ret <32 x i16> [[TMP4]]
//
v32int16 test_shiftr_elem(v32int16 v, int s) {
  return shiftr_elem(v, s);
}

//
// CHECK-LABEL: @_Z16test_shiftr_elemDv64_hj(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[CONV_I1:%.*]] = and i32 [[S:%.*]], 255
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef <64 x i8> @llvm.aie2.vbroadcast8.I512(i32 [[CONV_I1]])
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <64 x i8> [[TMP0]] to <16 x i32>
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <64 x i8> [[V:%.*]] to <16 x i32>
// CHECK-NEXT:    [[TMP3:%.*]] = tail call <16 x i32> @llvm.aie2.vshift.I512.I512(<16 x i32> [[TMP1]], <16 x i32> [[TMP2]], i32 0, i32 63)
// CHECK-NEXT:    [[TMP4:%.*]] = bitcast <16 x i32> [[TMP3]] to <64 x i8>
// CHECK-NEXT:    ret <64 x i8> [[TMP4]]
//
v64uint8 test_shiftr_elem(v64uint8 v, unsigned int s) {
  return shiftr_elem(v, s);
}

// CHECK-LABEL: @_Z16test_shiftr_elemDv16_jj(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vbroadcast32.I512(i32 [[S:%.*]])
// CHECK-NEXT:    [[TMP1:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vshift.I512.I512(<16 x i32> [[TMP0]], <16 x i32> [[V:%.*]], i32 0, i32 60)
// CHECK-NEXT:    ret <16 x i32> [[TMP1]]
//
v16uint32 test_shiftr_elem(v16uint32 v, unsigned int s) {
  return shiftr_elem(v, s);
}

// CHECK-LABEL: @_Z21test_broadcast_one_u8v(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef <64 x i8> @llvm.aie2.vbroadcast8.I512(i32 1)
// CHECK-NEXT:    ret <64 x i8> [[TMP0]]
//
v64uint8 test_broadcast_one_u8 () {
   return broadcast_one_u8();
}

// CHECK-LABEL: @_Z22test_broadcast_one_u16v(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef <32 x i16> @llvm.aie2.vbroadcast16.I512(i32 1)
// CHECK-NEXT:    ret <32 x i16> [[TMP0]]
//
v32uint16 test_broadcast_one_u16 () {
   return broadcast_one_u16();
}

// CHECK-LABEL: @_Z22test_broadcast_one_u32v(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vbroadcast32.I512(i32 1)
// CHECK-NEXT:    ret <16 x i32> [[TMP0]]
//
v16uint32 test_broadcast_one_u32 () {
   return broadcast_one_u32();
}

// CHECK-LABEL: @_Z30test_broadcast_zero_to_v64int8v(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef <64 x i8> @llvm.aie2.vbroadcast8.I512(i32 0)
// CHECK-NEXT:    ret <64 x i8> [[TMP0]]
//
v64uint8 test_broadcast_zero_to_v64int8 () {
   return broadcast_zero_to_v64int8();
}

// CHECK-LABEL: @_Z31test_broadcast_zero_to_v32int16v(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef <32 x i16> @llvm.aie2.vbroadcast16.I512(i32 0)
// CHECK-NEXT:    ret <32 x i16> [[TMP0]]
//
v32uint16 test_broadcast_zero_to_v32int16 () {
   return broadcast_zero_to_v32int16();
}

// CHECK-LABEL: @_Z31test_broadcast_zero_to_v16int32v(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vbroadcast32.I512(i32 0)
// CHECK-NEXT:    ret <16 x i32> [[TMP0]]
//
v16uint32 test_broadcast_zero_to_v16int32 () {
   return broadcast_zero_to_v16int32();
}

// CHECK-LABEL: @_Z31test_broadcast_zero_to_v16floatv(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <16 x i32> @llvm.aie2.vbroadcast32.I512(i32 0)
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <16 x i32> [[TMP0]] to <16 x float>
// CHECK-NEXT:    ret <16 x float> [[TMP1]]
//
v16float test_broadcast_zero_to_v16float () {
   return broadcast_zero_to_v16float();
}

// CHECK-LABEL: @_Z31test_broadcast_zero_to_v16acc64v(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef <16 x i64> @llvm.aie2.vbroadcast.zero.acc1024()
// CHECK-NEXT:    ret <16 x i64> [[TMP0]]
//
v16acc64 test_broadcast_zero_to_v16acc64 () {
   return broadcast_zero_to_v16acc64();
}

// CHECK-LABEL: @_Z31test_broadcast_zero_to_v32acc32v(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef <16 x i64> @llvm.aie2.vbroadcast.zero.acc1024()
// CHECK-NEXT:    ret <16 x i64> [[TMP0]]
//
v32acc32 test_broadcast_zero_to_v32acc32 () {
   return broadcast_zero_to_v32acc32();
}

// CHECK-LABEL: @_Z19test_broadcast_elemDv32_si(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef <32 x i16> @llvm.aie2.vextract.broadcast16.I512(<32 x i16> [[V:%.*]], i32 [[IDX:%.*]])
// CHECK-NEXT:    ret <32 x i16> [[TMP0]]
//
v32int16 test_broadcast_elem (v32int16 v, int idx) {
   return broadcast_elem(v, idx);
}

// CHECK-LABEL: @_Z19test_broadcast_elemDv64_hi(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef <64 x i8> @llvm.aie2.vextract.broadcast8.I512(<64 x i8> [[V:%.*]], i32 [[IDX:%.*]])
// CHECK-NEXT:    ret <64 x i8> [[TMP0]]
//
v64uint8 test_broadcast_elem (v64uint8 v, int idx) {
   return broadcast_elem(v, idx);
}

// CHECK-LABEL: @_Z19test_broadcast_elemDv32_ti(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef <32 x i16> @llvm.aie2.vextract.broadcast16.I512(<32 x i16> [[V:%.*]], i32 [[IDX:%.*]])
// CHECK-NEXT:    ret <32 x i16> [[TMP0]]
//
v32uint16 test_broadcast_elem (v32uint16 v, int idx) {
   return broadcast_elem(v, idx);
}

// CHECK-LABEL: @_Z19test_broadcast_elemDv16_ji(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vextract.broadcast32.I512(<16 x i32> [[V:%.*]], i32 [[IDX:%.*]])
// CHECK-NEXT:    ret <16 x i32> [[TMP0]]
//
v16uint32 test_broadcast_elem (v16uint32 v, int idx) {
   return broadcast_elem(v, idx);
}

// CHECK-LABEL: @_Z12test_shuffleDv16_iS_j(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vshuffle(<16 x i32> [[A:%.*]], <16 x i32> [[B:%.*]], i32 [[MODE:%.*]])
// CHECK-NEXT:    ret <16 x i32> [[TMP0]]
//
v16int32 test_shuffle(v16int32 a,    v16int32 b,    unsigned int mode){
   return shuffle(a,b,mode);
}

// CHECK-LABEL: @_Z15test_shuffle_s8ij(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <16 x i32> @llvm.aie2.vbcst.shuffle8(i32 [[B:%.*]], i32 [[M:%.*]])
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <16 x i32> [[TMP0]] to <64 x i8>
// CHECK-NEXT:    ret <64 x i8> [[TMP1]]
//
v64int8 test_shuffle_s8   (int b, unsigned int m) {
   return shuffle_s8(b,m);
}

// CHECK-LABEL: @_Z16test_shuffle_s16ij(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <16 x i32> @llvm.aie2.vbcst.shuffle16(i32 [[B:%.*]], i32 [[M:%.*]])
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <16 x i32> [[TMP0]] to <32 x i16>
// CHECK-NEXT:    ret <32 x i16> [[TMP1]]
//
v32int16 test_shuffle_s16 (int b, unsigned int m) {
   return shuffle_s16(b,m);
}

// CHECK-LABEL: @_Z16test_shuffle_s32ij(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vbcst.shuffle32(i32 [[B:%.*]], i32 [[M:%.*]])
// CHECK-NEXT:    ret <16 x i32> [[TMP0]]
//
v16int32 test_shuffle_s32 (int b, unsigned int m) {
   return shuffle_s32(b,m);
}

// CHECK-LABEL: @_Z16test_shuffle_s64Dv2_ij(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vbcst.shuffle64(<2 x i32> [[B:%.*]], i32 [[M:%.*]])
// CHECK-NEXT:    ret <16 x i32> [[TMP0]]
//
v16int32 test_shuffle_s64 (v2int32 b, unsigned int m) {
   return shuffle_v2s32(b,m);
}

// CHECK-LABEL: @_Z16test_shuffle_s64xj(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast i64 [[B:%.*]] to <2 x i32>
// CHECK-NEXT:    [[TMP1:%.*]] = tail call noundef <16 x i32> @llvm.aie2.vbcst.shuffle64(<2 x i32> [[TMP0]], i32 [[M:%.*]])
// CHECK-NEXT:    ret <16 x i32> [[TMP1]]
//
v16int32 test_shuffle_s64(long long b, unsigned int m) {
  return shuffle_s64(b, m);
}

// CHECK-LABEL: @_Z15test_ext_v2int4Dv64_DB8_ii(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i32 @llvm.aie2.vextract.elem8.I512(<64 x i8> [[V:%.*]], i32 [[IDX:%.*]], i32 [[SIGN:%.*]])
// CHECK-NEXT:    [[CONV_I:%.*]] = trunc i32 [[TMP0]] to i8
// CHECK-NEXT:    ret i8 [[CONV_I]]
//
v2int4 test_ext_v2int4(v128int4 v, int idx, int sign) {
  return ext_v2int4(v, idx, sign);
}

// CHECK-LABEL: @_Z13test_ext_elemDv64_aii(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i32 @llvm.aie2.vextract.elem8.I512(<64 x i8> [[V:%.*]], i32 [[IDX:%.*]], i32 [[SIGN:%.*]])
// CHECK-NEXT:    [[CONV_I:%.*]] = trunc i32 [[TMP0]] to i8
// CHECK-NEXT:    ret i8 [[CONV_I]]
//
char test_ext_elem(v64int8 v, int idx, int sign) {
  return ext_elem(v, idx, sign);
}

// CHECK-LABEL: @_Z18test_ext_elem_idx3Dv64_ai(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i32 @llvm.aie2.vextract.elem8.I512(<64 x i8> [[V:%.*]], i32 3, i32 [[SIGN:%.*]])
// CHECK-NEXT:    [[CONV_I:%.*]] = trunc i32 [[TMP0]] to i8
// CHECK-NEXT:    ret i8 [[CONV_I]]
//
char test_ext_elem_idx3(v64int8 v, int sign) {
  return ext_elem(v, 3, sign);
}

// CHECK-LABEL: @_Z16test_ext_v2uint8Dv64_hii(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <64 x i8> [[V:%.*]] to <32 x i16>
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i32 @llvm.aie2.vextract.elem16.I512(<32 x i16> [[TMP0]], i32 [[IDX:%.*]], i32 [[SIGN:%.*]])
// CHECK-NEXT:    [[CONV_I:%.*]] = trunc i32 [[TMP1]] to i16
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast i16 [[CONV_I]] to <2 x i8>
// CHECK-NEXT:    ret <2 x i8> [[TMP2]]
//
v2uint8 test_ext_v2uint8(v64uint8 v, int idx, int sign) {
  return ext_v2uint8(v, idx, sign);
}

// CHECK-LABEL: @_Z13test_ext_elemDv32_tii(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i32 @llvm.aie2.vextract.elem16.I512(<32 x i16> [[V:%.*]], i32 [[IDX:%.*]], i32 [[SIGN:%.*]])
// CHECK-NEXT:    [[CONV_I:%.*]] = trunc i32 [[TMP0]] to i16
// CHECK-NEXT:    ret i16 [[CONV_I]]
//
unsigned short test_ext_elem(v32uint16 v, int idx, int sign) {
  return ext_elem(v, idx, sign);
}

// CHECK-LABEL: @_Z18test_ext_elem_idx4Dv32_ti(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i32 @llvm.aie2.vextract.elem16.I512(<32 x i16> [[V:%.*]], i32 4, i32 [[SIGN:%.*]])
// CHECK-NEXT:    [[CONV_I:%.*]] = trunc i32 [[TMP0]] to i16
// CHECK-NEXT:    ret i16 [[CONV_I]]
//
unsigned short test_ext_elem_idx4(v32uint16 v, int sign) {
  return ext_elem(v, 4, sign);
}

// CHECK-LABEL: @_Z16test_ext_v2int16Dv32_sii(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <32 x i16> [[V:%.*]] to <16 x i32>
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i32 @llvm.aie2.vextract.elem32.I512(<16 x i32> [[TMP0]], i32 [[IDX:%.*]], i32 [[SIGN:%.*]])
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast i32 [[TMP1]] to <2 x i16>
// CHECK-NEXT:    ret <2 x i16> [[TMP2]]
//
v2int16 test_ext_v2int16(v32int16 v, int idx, int sign) {
  return ext_v2int16(v, idx, sign);
}

// CHECK-LABEL: @_Z13test_ext_elemDv16_iii(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef i32 @llvm.aie2.vextract.elem32.I512(<16 x i32> [[V:%.*]], i32 [[IDX:%.*]], i32 [[SIGN:%.*]])
// CHECK-NEXT:    ret i32 [[TMP0]]
//
int test_ext_elem(v16int32 v, int idx, int sign) {
  return ext_elem(v, idx, sign);
}

// CHECK-LABEL: @_Z18test_ext_elem_idx5Dv16_ii(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef i32 @llvm.aie2.vextract.elem32.I512(<16 x i32> [[V:%.*]], i32 5, i32 [[SIGN:%.*]])
// CHECK-NEXT:    ret i32 [[TMP0]]
//
int test_ext_elem_idx5(v16int32 v, int sign) {
  return ext_elem(v, 5, sign);
}

// CHECK-LABEL: @_Z16test_ext_v4int16Dv32_sii(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <32 x i16> [[V:%.*]] to <16 x i32>
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <2 x i32> @llvm.aie2.vextract.elem64.I512(<16 x i32> [[TMP0]], i32 [[IDX:%.*]], i32 [[SIGN:%.*]])
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <2 x i32> [[TMP1]] to <4 x i16>
// CHECK-NEXT:    ret <4 x i16> [[TMP2]]
//
v4int16 test_ext_v4int16(v32int16 v, int idx, int sign) {
  return ext_v4int16(v, idx, sign);
}

// CHECK-LABEL: @_Z16test_ext_v2int32Dv16_iii(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef <2 x i32> @llvm.aie2.vextract.elem64.I512(<16 x i32> [[V:%.*]], i32 [[IDX:%.*]], i32 [[SIGN:%.*]])
// CHECK-NEXT:    ret <2 x i32> [[TMP0]]
//
v2int32 test_ext_v2int32(v16int32 v, int idx, int sign) {
  return ext_v2int32(v, idx, sign);
}

// CHECK-LABEL: @_Z21test_ext_v2int32_idx4Dv16_ii(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef <2 x i32> @llvm.aie2.vextract.elem64.I512(<16 x i32> [[V:%.*]], i32 4, i32 [[SIGN:%.*]])
// CHECK-NEXT:    ret <2 x i32> [[TMP0]]
//
v2int32 test_ext_v2int32_idx4(v16int32 v, int sign) {
  return ext_v2int32(v, 4, sign);
}

// CHECK-LABEL: @_Z19test_extract_v8int4Dv64_DB8_ii(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <64 x i8> [[V:%.*]] to <16 x i32>
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i32 @llvm.aie2.vextract.elem32.I512(<16 x i32> [[TMP0]], i32 [[IDX:%.*]], i32 [[SIGN:%.*]])
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast i32 [[TMP1]] to <4 x i8>
// CHECK-NEXT:    ret <4 x i8> [[TMP2]]
//
v8int4 test_extract_v8int4(v128int4 v, int idx, int sign) {
  return ext_v8int4(v, idx, sign);
}

// CHECK-LABEL: @_Z19test_extract_v8int8Dv64_aii(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <64 x i8> [[V:%.*]] to <16 x i32>
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <2 x i32> @llvm.aie2.vextract.elem64.I512(<16 x i32> [[TMP0]], i32 [[IDX:%.*]], i32 [[SIGN:%.*]])
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <2 x i32> [[TMP1]] to <8 x i8>
// CHECK-NEXT:    ret <8 x i8> [[TMP2]]
//
v8int8 test_extract_v8int8(v64int8 v, int idx, int sign) {
  return ext_v8int8(v, idx, sign);
}

// CHECK-LABEL: @_Z17test_extract_elemDv16_jii(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef i32 @llvm.aie2.vextract.elem32.I512(<16 x i32> [[V:%.*]], i32 [[IDX:%.*]], i32 [[SIGN:%.*]])
// CHECK-NEXT:    ret i32 [[TMP0]]
//
unsigned int test_extract_elem(v16uint32 v, int idx, int sign) {
  return ext_elem(v, idx, sign);
}

// CHECK-LABEL: @_Z23test_extract_elem_floatDv16_fii(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <16 x float> [[V:%.*]] to <16 x i32>
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i32 @llvm.aie2.vextract.elem32.I512(<16 x i32> [[TMP0]], i32 [[IDX:%.*]], i32 [[SIGN:%.*]])
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast i32 [[TMP1]] to float
// CHECK-NEXT:    ret float [[TMP2]]
//
float test_extract_elem_float(v16float v, int idx, int sign) {
  return extract_elem(v, idx, sign);
}

// CHECK-LABEL: @_Z22test_extract_elem_idx5Dv16_ji(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef i32 @llvm.aie2.vextract.elem32.I512(<16 x i32> [[V:%.*]], i32 5, i32 [[SIGN:%.*]])
// CHECK-NEXT:    ret i32 [[TMP0]]
//
unsigned int test_extract_elem_idx5(v16uint32 v, int sign) {
  return ext_elem(v, 5, sign);
}

// CHECK-LABEL: @_Z13test_ext_elemDv64_ai(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i32 @llvm.aie2.vextract.elem8.I512(<64 x i8> [[V:%.*]], i32 [[IDX:%.*]], i32 1)
// CHECK-NEXT:    [[CONV_I_I:%.*]] = trunc i32 [[TMP0]] to i8
// CHECK-NEXT:    ret i8 [[CONV_I_I]]
//
char test_ext_elem(v64int8 v, int idx) {
  return ext_elem(v, idx);
}

// CHECK-LABEL: @_Z15test_ext_v2int8Dv64_ai(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <64 x i8> [[V:%.*]] to <32 x i16>
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i32 @llvm.aie2.vextract.elem16.I512(<32 x i16> [[TMP0]], i32 [[IDX:%.*]], i32 1)
// CHECK-NEXT:    [[CONV_I:%.*]] = trunc i32 [[TMP1]] to i16
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast i16 [[CONV_I]] to <2 x i8>
// CHECK-NEXT:    ret <2 x i8> [[TMP2]]
//
v2int8 test_ext_v2int8(v64int8 v, int idx) {
  return ext_v2int8(v, idx);
}

// CHECK-LABEL: @_Z20test_ext_v2int8_idx2Dv64_a(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <64 x i8> [[V:%.*]] to <32 x i16>
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i32 @llvm.aie2.vextract.elem16.I512(<32 x i16> [[TMP0]], i32 2, i32 1)
// CHECK-NEXT:    [[CONV_I:%.*]] = trunc i32 [[TMP1]] to i16
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast i16 [[CONV_I]] to <2 x i8>
// CHECK-NEXT:    ret <2 x i8> [[TMP2]]
//
v2int8 test_ext_v2int8_idx2(v64int8 v) {
  return ext_v2int8(v, 2);
}

// CHECK-LABEL: @_Z17test_ext_v2uint32Dv16_ji(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef <2 x i32> @llvm.aie2.vextract.elem64.I512(<16 x i32> [[V:%.*]], i32 [[IDX:%.*]], i32 0)
// CHECK-NEXT:    ret <2 x i32> [[TMP0]]
//
v2uint32 test_ext_v2uint32(v16uint32 v, int idx) {
  return ext_v2uint32(v, idx);
}

// CHECK-LABEL: @_Z20test_extract_v8uint4Dv64_DU8_i(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <64 x i8> [[V:%.*]] to <16 x i32>
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i32 @llvm.aie2.vextract.elem32.I512(<16 x i32> [[TMP0]], i32 [[IDX:%.*]], i32 0)
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast i32 [[TMP1]] to <4 x i8>
// CHECK-NEXT:    ret <4 x i8> [[TMP2]]
//
v8uint4 test_extract_v8uint4(v128uint4 v, int idx) {
  return extract_v8uint4(v, idx);
}

// CHECK-LABEL: @_Z25test_extract_v8uint4_idx1Dv64_DU8_(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <64 x i8> [[V:%.*]] to <16 x i32>
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i32 @llvm.aie2.vextract.elem32.I512(<16 x i32> [[TMP0]], i32 1, i32 0)
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast i32 [[TMP1]] to <4 x i8>
// CHECK-NEXT:    ret <4 x i8> [[TMP2]]
//
v8uint4 test_extract_v8uint4_idx1(v128uint4 v) {
  return extract_v8uint4(v, 1);
}

// CHECK-LABEL: @_Z17test_ext_address1Dv64_ai(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i32 @llvm.aie2.vextract.elem8.I512(<64 x i8> [[V:%.*]], i32 [[IDX:%.*]], i32 0)
// CHECK-NEXT:    [[CONV_I:%.*]] = trunc i32 [[TMP0]] to i20
// CHECK-NEXT:    [[TMP1:%.*]] = inttoptr i20 [[CONV_I]] to ptr
// CHECK-NEXT:    ret ptr [[TMP1]]
//
void * test_ext_address1(v64int8 v, int idx) {
  return ext_address(v, idx);
}

// CHECK-LABEL: @_Z17test_ext_address2Dv32_si(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i32 @llvm.aie2.vextract.elem16.I512(<32 x i16> [[V:%.*]], i32 [[IDX:%.*]], i32 0)
// CHECK-NEXT:    [[CONV_I:%.*]] = trunc i32 [[TMP0]] to i20
// CHECK-NEXT:    [[TMP1:%.*]] = inttoptr i20 [[CONV_I]] to ptr
// CHECK-NEXT:    ret ptr [[TMP1]]
//
void * test_ext_address2(v32int16 v, int idx) {
  return ext_address(v, idx);
}

// CHECK-LABEL: @_Z17test_ext_address3Dv16_ii(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call i32 @llvm.aie2.vextract.elem32.I512(<16 x i32> [[V:%.*]], i32 [[IDX:%.*]], i32 0)
// CHECK-NEXT:    [[CONV_I:%.*]] = trunc i32 [[TMP0]] to i20
// CHECK-NEXT:    [[TMP1:%.*]] = inttoptr i20 [[CONV_I]] to ptr
// CHECK-NEXT:    ret ptr [[TMP1]]
//
void * test_ext_address3(v16int32 v, int idx) {
  return ext_address(v, idx);
}

// CHECK-LABEL: @_Z12test_ext_u64Dv64_DU8_ii(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <64 x i8> [[V:%.*]] to <16 x i32>
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <2 x i32> @llvm.aie2.vextract.elem64.I512(<16 x i32> [[TMP0]], i32 [[IDX:%.*]], i32 [[SIGN:%.*]])
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <2 x i32> [[TMP1]] to i64
// CHECK-NEXT:    ret i64 [[TMP2]]
//
unsigned long long test_ext_u64(v128uint4 v, int idx, int sign) {
  return (unsigned long long)(ext_u64(v, idx, sign));
}

// CHECK-LABEL: @_Z12test_ext_u64Dv64_hii(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <64 x i8> [[V:%.*]] to <16 x i32>
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <2 x i32> @llvm.aie2.vextract.elem64.I512(<16 x i32> [[TMP0]], i32 [[IDX:%.*]], i32 [[SIGN:%.*]])
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <2 x i32> [[TMP1]] to i64
// CHECK-NEXT:    ret i64 [[TMP2]]
//
unsigned long long test_ext_u64(v64uint8 v, int idx, int sign) {
  return (unsigned long long)(ext_u64(v, idx, sign));
}

// CHECK-LABEL: @_Z12test_ext_u64Dv32_sii(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <32 x i16> [[V:%.*]] to <16 x i32>
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <2 x i32> @llvm.aie2.vextract.elem64.I512(<16 x i32> [[TMP0]], i32 [[IDX:%.*]], i32 [[SIGN:%.*]])
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <2 x i32> [[TMP1]] to i64
// CHECK-NEXT:    ret i64 [[TMP2]]
//
unsigned long long test_ext_u64(v32int16 v, int idx, int sign) {
  return (unsigned long long)(ext_u64(v, idx, sign));
}

//
// CHECK-LABEL: @_Z12test_ext_u64Dv16_iii(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <2 x i32> @llvm.aie2.vextract.elem64.I512(<16 x i32> [[V:%.*]], i32 [[IDX:%.*]], i32 [[SIGN:%.*]])
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <2 x i32> [[TMP0]] to i64
// CHECK-NEXT:    ret i64 [[TMP1]]
//
unsigned long long test_ext_u64(v16int32 v, int idx, int sign) {
  return (unsigned long long)(ext_u64(v, idx, sign));
}

/* Test Intrinsic using ACCFLOAT type */

// CHECK-LABEL: @_Z11test_shiftxDv16_u10__accfloatS_ii(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <8 x i64> [[A:%.*]] to <16 x i32>
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <8 x i64> [[B:%.*]] to <16 x i32>
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <16 x i32> @llvm.aie2.vshift.I512.I512(<16 x i32> [[TMP0]], <16 x i32> [[TMP1]], i32 [[STEP:%.*]], i32 [[SHIFT:%.*]])
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <16 x i32> [[TMP2]] to <8 x i64>
// CHECK-NEXT:    ret <8 x i64> [[TMP3]]
//
v16accfloat test_shiftx(v16accfloat a, v16accfloat b, int step, int shift) {
  return shiftx(a,b,step,shift);
}

// CHECK-LABEL: @_Z16test_shift_bytesDv16_u10__accfloatS_i(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <8 x i64> [[A:%.*]] to <16 x i32>
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <8 x i64> [[B:%.*]] to <16 x i32>
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <16 x i32> @llvm.aie2.vshift.I512.I512(<16 x i32> [[TMP0]], <16 x i32> [[TMP1]], i32 0, i32 [[SHIFT:%.*]])
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <16 x i32> [[TMP2]] to <8 x i64>
// CHECK-NEXT:    ret <8 x i64> [[TMP3]]
//
v16accfloat test_shift_bytes(v16accfloat a, v16accfloat b, int shift) {
  return shift_bytes(a,b,shift);
}

// CHECK-LABEL: @_Z10test_shiftDv16_u10__accfloatS_i(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[MUL_I:%.*]] = shl nsw i32 [[SHIFT_BY:%.*]], 2
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <8 x i64> [[A:%.*]] to <16 x i32>
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <8 x i64> [[B:%.*]] to <16 x i32>
// CHECK-NEXT:    [[TMP2:%.*]] = tail call <16 x i32> @llvm.aie2.vshift.I512.I512(<16 x i32> [[TMP0]], <16 x i32> [[TMP1]], i32 0, i32 [[MUL_I]])
// CHECK-NEXT:    [[TMP3:%.*]] = bitcast <16 x i32> [[TMP2]] to <8 x i64>
// CHECK-NEXT:    ret <8 x i64> [[TMP3]]
//
v16accfloat test_shift(v16accfloat a, v16accfloat b, int shift_by) {
  return shift(a, b, shift_by);
}

// CHECK-LABEL: @_Z29test_broadcast_to_v16accfloatf(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <16 x float> @llvm.aie2.vbroadcastfloat.I512(float [[B:%.*]])
// CHECK-NEXT:    [[TMP1:%.*]] = bitcast <16 x float> [[TMP0]] to <8 x i64>
// CHECK-NEXT:    ret <8 x i64> [[TMP1]]
//
v16accfloat test_broadcast_to_v16accfloat (float b) {
   return  broadcast_to_v16accfloat(b);
}

// CHECK-LABEL: @_Z11test_shiftxDv32_u6__bf16S_ii(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef <32 x bfloat> @llvm.aie2.vshift.bf512.bf512(<32 x bfloat> [[A:%.*]], <32 x bfloat> [[B:%.*]], i32 [[STEP:%.*]], i32 [[SHIFT:%.*]])
// CHECK-NEXT:    ret <32 x bfloat> [[TMP0]]
//
v32bfloat16 test_shiftx(v32bfloat16 a, v32bfloat16 b, int step, int shift) {
    return shiftx(a, b, step, shift);
}

// CHECK-LABEL: @_Z11test_insertDv32_u6__bf16i8bfloat16(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[B_COERCE_FCA_0_EXTRACT_I:%.*]] = extractvalue [[CLASS_BFLOAT16:%.*]] [[B_COERCE:%.*]], 0
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef <32 x bfloat> @llvm.aie2.vinsert16.bf512(<32 x bfloat> [[V:%.*]], i32 [[IDX:%.*]], bfloat [[B_COERCE_FCA_0_EXTRACT_I]])
// CHECK-NEXT:    ret <32 x bfloat> [[TMP0]]
//
v32bfloat16 test_insert(v32bfloat16 v, int idx, bfloat16 b) {
  return insert(v, idx, b);
}

// CHECK-LABEL: @_Z11test_insertDv32_u6__bf16iDv2_u6__bf16(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef <32 x bfloat> @llvm.aie2.vinsert32.bf512(<32 x bfloat> [[V:%.*]], i32 [[IDX:%.*]], <2 x bfloat> [[B:%.*]])
// CHECK-NEXT:    ret <32 x bfloat> [[TMP0]]
//
v32bfloat16 test_insert(v32bfloat16 v, int idx, v2bfloat16 b) {
  return insert(v, idx, b);
}

// CHECK-LABEL: @_Z11test_insertDv32_u6__bf16iDv4_u6__bf16(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef <32 x bfloat> @llvm.aie2.vinsert64.bf512(<32 x bfloat> [[V:%.*]], i32 [[IDX:%.*]], <4 x bfloat> [[B:%.*]])
// CHECK-NEXT:    ret <32 x bfloat> [[TMP0]]
//
v32bfloat16 test_insert(v32bfloat16 v, int idx, v4bfloat16 b) {
  return insert(v, idx, b);
}

// CHECK-LABEL: @_Z11test_insertDv32_u6__bf16iy(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast i64 [[B:%.*]] to <4 x bfloat>
// CHECK-NEXT:    [[TMP1:%.*]] = tail call noundef <32 x bfloat> @llvm.aie2.vinsert64.bf512(<32 x bfloat> [[V:%.*]], i32 [[IDX:%.*]], <4 x bfloat> [[TMP0]])
// CHECK-NEXT:    ret <32 x bfloat> [[TMP1]]
//
v32bfloat16 test_insert(v32bfloat16 v, int idx, unsigned long long b) {
  return insert(v, idx,(v4bfloat16)b);
}

// CHECK-LABEL: @_Z29test_broadcast_to_v32bfloat168bfloat16(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[B_COERCE_FCA_0_EXTRACT_I:%.*]] = extractvalue [[CLASS_BFLOAT16:%.*]] [[B_COERCE:%.*]], 0
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef <32 x bfloat> @llvm.aie2.vbroadcast16.bf512(bfloat [[B_COERCE_FCA_0_EXTRACT_I]])
// CHECK-NEXT:    ret <32 x bfloat> [[TMP0]]
//
v32bfloat16 test_broadcast_to_v32bfloat16 (bfloat16 b) { return broadcast_to_v32bfloat16(b); }

// CHECK-LABEL: @_Z29test_broadcast_to_v32bfloat16Dv2_u6__bf16(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef <32 x bfloat> @llvm.aie2.vbroadcast32.bf512(<2 x bfloat> [[B:%.*]])
// CHECK-NEXT:    ret <32 x bfloat> [[TMP0]]
//
v32bfloat16 test_broadcast_to_v32bfloat16 (v2bfloat16 b) { return broadcast_to_v32bfloat16(b); }

// CHECK-LABEL: @_Z29test_broadcast_to_v32bfloat16Dv4_u6__bf16(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef <32 x bfloat> @llvm.aie2.vbroadcast64.bf512(<4 x bfloat> [[B:%.*]])
// CHECK-NEXT:    ret <32 x bfloat> [[TMP0]]
//
v32bfloat16 test_broadcast_to_v32bfloat16 (v4bfloat16 b) { return broadcast_to_v32bfloat16(b); }

// CHECK-LABEL: @_Z21test_shuffle_bfloat168bfloat16j(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[B_COERCE_FCA_0_EXTRACT_I:%.*]] = extractvalue [[CLASS_BFLOAT16:%.*]] [[B_COERCE:%.*]], 0
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef <32 x bfloat> @llvm.aie2.vbcst.shuffle.bf16(bfloat [[B_COERCE_FCA_0_EXTRACT_I]], i32 [[M:%.*]])
// CHECK-NEXT:    ret <32 x bfloat> [[TMP0]]
//
v32bfloat16 test_shuffle_bfloat16(bfloat16 b, unsigned int m) {  return shuffle_bfloat16(b,m) ;}


// CHECK-LABEL: @_Z13test_ext_elemDv32_u6__bf16ii(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <32 x bfloat> [[V:%.*]] to <32 x i16>
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i32 @llvm.aie2.vextract.elem16.I512(<32 x i16> [[TMP0]], i32 [[IDX:%.*]], i32 [[SIGN:%.*]])
// CHECK-NEXT:    [[ELEM_0_EXTRACT_TRUNC_I:%.*]] = trunc i32 [[TMP1]] to i16
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast i16 [[ELEM_0_EXTRACT_TRUNC_I]] to bfloat
// CHECK-NEXT:    [[DOTFCA_0_INSERT_I:%.*]] = insertvalue [[CLASS_BFLOAT16:%.*]] poison, bfloat [[TMP2]], 0
// CHECK-NEXT:    ret [[CLASS_BFLOAT16]] [[DOTFCA_0_INSERT_I]]
//
bfloat16 test_ext_elem(v32bfloat16 v, int idx, int sign) {
  return ext_elem(v, idx, sign);
}

// CHECK-LABEL: @_Z19test_broadcast_elemDv32_u6__bf16i(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef <32 x bfloat> @llvm.aie2.vextract.broadcast32.bf512(<32 x bfloat> [[V:%.*]], i32 [[IDX:%.*]])
// CHECK-NEXT:    ret <32 x bfloat> [[TMP0]]
//
v32bfloat16 test_broadcast_elem (v32bfloat16 v, int idx) {
  return broadcast_elem(v, idx);
}

// CHECK-LABEL: @_Z12test_shuffleDv32_u6__bf16S_j(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call noundef <32 x bfloat> @llvm.aie2.vshuffle.bf16(<32 x bfloat> [[A:%.*]], <32 x bfloat> [[B:%.*]], i32 [[MODE:%.*]])
// CHECK-NEXT:    ret <32 x bfloat> [[TMP0]]
//
v32bfloat16 test_shuffle(v32bfloat16 a, v32bfloat16 b, unsigned int mode) {
  return shuffle(a, b, mode);
}

// CHECK-LABEL: @_Z23test_extract_v2bfloat16Dv32_u6__bf16ii(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <32 x bfloat> [[V:%.*]] to <16 x i32>
// CHECK-NEXT:    [[TMP1:%.*]] = tail call i32 @llvm.aie2.vextract.elem32.I512(<16 x i32> [[TMP0]], i32 [[IDX:%.*]], i32 [[SIGN:%.*]])
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast i32 [[TMP1]] to <2 x bfloat>
// CHECK-NEXT:    ret <2 x bfloat> [[TMP2]]
//
v2bfloat16 test_extract_v2bfloat16(v32bfloat16 v, int idx, int sign) {
   return extract_v2bfloat16(v, idx, sign);
}

// CHECK-LABEL: @_Z12test_ext_u64Dv32_u6__bf16ii(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <32 x bfloat> [[V:%.*]] to <16 x i32>
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <2 x i32> @llvm.aie2.vextract.elem64.I512(<16 x i32> [[TMP0]], i32 [[IDX:%.*]], i32 [[SIGN:%.*]])
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <2 x i32> [[TMP1]] to i64
// CHECK-NEXT:    ret i64 [[TMP2]]
//
unsigned long long test_ext_u64(v32bfloat16 v, int idx, int sign) {
  return ext_u64(v, idx, sign);
}

// CHECK-LABEL: @_Z23test_extract_v4bfloat16Dv32_u6__bf16ii(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = bitcast <32 x bfloat> [[V:%.*]] to <16 x i32>
// CHECK-NEXT:    [[TMP1:%.*]] = tail call <2 x i32> @llvm.aie2.vextract.elem64.I512(<16 x i32> [[TMP0]], i32 [[IDX:%.*]], i32 [[SIGN:%.*]])
// CHECK-NEXT:    [[TMP2:%.*]] = bitcast <2 x i32> [[TMP1]] to <4 x bfloat>
// CHECK-NEXT:    ret <4 x bfloat> [[TMP2]]
//
v4bfloat16 test_extract_v4bfloat16(v32bfloat16 v, int idx, int sign) {
   return extract_v4bfloat16(v, idx, sign);
}

// CHECK-LABEL: @_Z21test_vinsert_accfloatDv16_u10__accfloatf(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[TMP0:%.*]] = tail call <8 x i64> @llvm.aie2.vinsert32.accfloat(<8 x i64> [[X:%.*]], i32 0, float [[A0:%.*]])
// CHECK-NEXT:    ret <8 x i64> [[TMP0]]
//
v16accfloat test_vinsert_accfloat(v16accfloat x, float a0) {
  return __builtin_aiev2_vinsert32_accfloat(x, 0, a0);
}
